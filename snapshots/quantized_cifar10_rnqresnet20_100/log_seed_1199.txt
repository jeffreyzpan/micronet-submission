save path : ./snapshots/quantized_cifar10_rnqresnet20_100
{'arch': 'rnqresnet20', 'batch_size': 128, 'data_path': '../../datasets/CIFAR10/cifar', 'dataset': 'cifar10', 'decay': 0.0001, 'epochs': 100, 'evaluate': False, 'gammas': [0.1, 0.1, 0.1], 'learning_rate': 0.01, 'manualSeed': 1199, 'momentum': 0.9, 'ngpu': 4, 'pretrained': '', 'print_freq': 200, 'resume': 'snapshots/finetune_cifar10_resnet20_300/model_best.pth.tar', 'save_path': './snapshots/quantized_cifar10_rnqresnet20_100', 'schedule': [30, 60, 90], 'start_epoch': 0, 'use_cuda': True, 'workers': 16}
Random Seed: 1199
python version : 3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]
torch  version : 1.1.0
cudnn  version : 7501
=> creating model 'rnqresnet20'
=> network :
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): QuantLinear(in_features=64, out_features=10, bias=True, method=KMeans, nbits=4)
)
=> loading checkpoint 'snapshots/finetune_cifar10_resnet20_300/model_best.pth.tar'
=> loaded checkpoint 'snapshots/finetune_cifar10_resnet20_300/model_best.pth.tar' (epoch 291)

==>>[2019-08-30 19:01:35] [Epoch=000/100] [Need: 00:00:00] [learning_rate=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/391]   Time 13.167 (13.167)   Data 0.481 (0.481)   Loss 0.2368 (0.2368)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2019-08-30 19:01:48]
  Epoch: [000][200/391]   Time 0.138 (0.239)   Data 0.000 (0.003)   Loss 0.2516 (0.2992)   Prec@1 91.406 (89.432)   Prec@5 98.438 (99.720)   [2019-08-30 19:02:23]
  **Test** Prec@1 83.310 Prec@5 99.060 Error@1 16.690

==>>[2019-08-30 19:03:04] [Epoch=001/100] [Need: 02:26:07] [learning_rate=0.0100] [Best : Accuracy=83.31, Error=16.69]
  Epoch: [001][000/391]   Time 1.902 (1.902)   Data 1.628 (1.628)   Loss 0.2234 (0.2234)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2019-08-30 19:03:06]
  Epoch: [001][200/391]   Time 0.168 (0.181)   Data 0.000 (0.008)   Loss 0.3640 (0.3153)   Prec@1 84.375 (88.985)   Prec@5 100.000 (99.681)   [2019-08-30 19:03:40]
  **Test** Prec@1 82.610 Prec@5 99.180 Error@1 17.390

==>>[2019-08-30 19:04:21] [Epoch=002/100] [Need: 02:15:12] [learning_rate=0.0100] [Best : Accuracy=83.31, Error=16.69]
  Epoch: [002][000/391]   Time 1.857 (1.857)   Data 1.649 (1.649)   Loss 0.3175 (0.3175)   Prec@1 87.500 (87.500)   Prec@5 99.219 (99.219)   [2019-08-30 19:04:23]
  Epoch: [002][200/391]   Time 0.169 (0.180)   Data 0.000 (0.008)   Loss 0.3080 (0.3329)   Prec@1 85.938 (88.479)   Prec@5 100.000 (99.600)   [2019-08-30 19:04:57]
  **Test** Prec@1 84.310 Prec@5 99.200 Error@1 15.690

==>>[2019-08-30 19:05:37] [Epoch=003/100] [Need: 02:10:20] [learning_rate=0.0100] [Best : Accuracy=84.31, Error=15.69]
  Epoch: [003][000/391]   Time 1.777 (1.777)   Data 1.569 (1.569)   Loss 0.4135 (0.4135)   Prec@1 88.281 (88.281)   Prec@5 98.438 (98.438)   [2019-08-30 19:05:39]
  Epoch: [003][200/391]   Time 0.172 (0.176)   Data 0.000 (0.008)   Loss 0.3391 (0.3008)   Prec@1 88.281 (89.315)   Prec@5 99.219 (99.724)   [2019-08-30 19:06:12]
  **Test** Prec@1 84.400 Prec@5 99.400 Error@1 15.600

==>>[2019-08-30 19:06:52] [Epoch=004/100] [Need: 02:06:55] [learning_rate=0.0100] [Best : Accuracy=84.40, Error=15.60]
  Epoch: [004][000/391]   Time 1.843 (1.843)   Data 1.630 (1.630)   Loss 0.3050 (0.3050)   Prec@1 90.625 (90.625)   Prec@5 99.219 (99.219)   [2019-08-30 19:06:54]
  Epoch: [004][200/391]   Time 0.201 (0.181)   Data 0.000 (0.008)   Loss 0.3125 (0.2978)   Prec@1 89.062 (89.599)   Prec@5 99.219 (99.716)   [2019-08-30 19:07:29]
  **Test** Prec@1 84.150 Prec@5 99.400 Error@1 15.850

==>>[2019-08-30 19:08:09] [Epoch=005/100] [Need: 02:04:40] [learning_rate=0.0100] [Best : Accuracy=84.40, Error=15.60]
  Epoch: [005][000/391]   Time 1.785 (1.785)   Data 1.573 (1.573)   Loss 0.3620 (0.3620)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2019-08-30 19:08:11]
  Epoch: [005][200/391]   Time 0.155 (0.181)   Data 0.000 (0.008)   Loss 0.3632 (0.2948)   Prec@1 89.062 (89.661)   Prec@5 98.438 (99.716)   [2019-08-30 19:08:45]
  **Test** Prec@1 85.420 Prec@5 99.510 Error@1 14.580

==>>[2019-08-30 19:09:26] [Epoch=006/100] [Need: 02:02:49] [learning_rate=0.0100] [Best : Accuracy=85.42, Error=14.58]
  Epoch: [006][000/391]   Time 1.833 (1.833)   Data 1.603 (1.603)   Loss 0.3286 (0.3286)   Prec@1 85.938 (85.938)   Prec@5 100.000 (100.000)   [2019-08-30 19:09:27]
  Epoch: [006][200/391]   Time 0.215 (0.180)   Data 0.000 (0.008)   Loss 0.3625 (0.2844)   Prec@1 84.375 (89.925)   Prec@5 99.219 (99.740)   [2019-08-30 19:10:02]
  **Test** Prec@1 83.860 Prec@5 99.120 Error@1 16.140

==>>[2019-08-30 19:10:42] [Epoch=007/100] [Need: 02:01:09] [learning_rate=0.0100] [Best : Accuracy=85.42, Error=14.58]
  Epoch: [007][000/391]   Time 1.802 (1.802)   Data 1.582 (1.582)   Loss 0.3234 (0.3234)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2019-08-30 19:10:44]
  Epoch: [007][200/391]   Time 0.140 (0.177)   Data 0.000 (0.008)   Loss 0.3174 (0.2753)   Prec@1 92.188 (90.330)   Prec@5 99.219 (99.771)   [2019-08-30 19:11:18]
  **Test** Prec@1 86.240 Prec@5 99.490 Error@1 13.760

==>>[2019-08-30 19:11:59] [Epoch=008/100] [Need: 01:59:30] [learning_rate=0.0100] [Best : Accuracy=86.24, Error=13.76]
  Epoch: [008][000/391]   Time 1.837 (1.837)   Data 1.626 (1.626)   Loss 0.2906 (0.2906)   Prec@1 90.625 (90.625)   Prec@5 99.219 (99.219)   [2019-08-30 19:12:00]
  Epoch: [008][200/391]   Time 0.163 (0.179)   Data 0.000 (0.008)   Loss 0.3166 (0.2775)   Prec@1 89.062 (90.151)   Prec@5 100.000 (99.743)   [2019-08-30 19:12:35]
  **Test** Prec@1 83.530 Prec@5 99.370 Error@1 16.470

==>>[2019-08-30 19:13:15] [Epoch=009/100] [Need: 01:58:00] [learning_rate=0.0100] [Best : Accuracy=86.24, Error=13.76]
  Epoch: [009][000/391]   Time 1.930 (1.930)   Data 1.728 (1.728)   Loss 0.2766 (0.2766)   Prec@1 89.844 (89.844)   Prec@5 99.219 (99.219)   [2019-08-30 19:13:17]
  Epoch: [009][200/391]   Time 0.170 (0.178)   Data 0.000 (0.009)   Loss 0.3569 (0.2736)   Prec@1 84.375 (90.400)   Prec@5 99.219 (99.778)   [2019-08-30 19:13:51]
  **Test** Prec@1 85.630 Prec@5 99.350 Error@1 14.370

==>>[2019-08-30 19:14:31] [Epoch=010/100] [Need: 01:56:21] [learning_rate=0.0100] [Best : Accuracy=86.24, Error=13.76]
  Epoch: [010][000/391]   Time 1.843 (1.843)   Data 1.634 (1.634)   Loss 0.1729 (0.1729)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 19:14:33]
  Epoch: [010][200/391]   Time 0.155 (0.179)   Data 0.000 (0.008)   Loss 0.2917 (0.2670)   Prec@1 85.938 (90.749)   Prec@5 100.000 (99.767)   [2019-08-30 19:15:07]
  **Test** Prec@1 85.500 Prec@5 99.190 Error@1 14.500

==>>[2019-08-30 19:15:47] [Epoch=011/100] [Need: 01:54:51] [learning_rate=0.0100] [Best : Accuracy=86.24, Error=13.76]
  Epoch: [011][000/391]   Time 1.942 (1.942)   Data 1.725 (1.725)   Loss 0.3080 (0.3080)   Prec@1 89.062 (89.062)   Prec@5 100.000 (100.000)   [2019-08-30 19:15:49]
  Epoch: [011][200/391]   Time 0.169 (0.187)   Data 0.000 (0.009)   Loss 0.4040 (0.2686)   Prec@1 85.938 (90.660)   Prec@5 100.000 (99.771)   [2019-08-30 19:16:25]
  **Test** Prec@1 85.830 Prec@5 99.470 Error@1 14.170

==>>[2019-08-30 19:17:04] [Epoch=012/100] [Need: 01:53:32] [learning_rate=0.0100] [Best : Accuracy=86.24, Error=13.76]
  Epoch: [012][000/391]   Time 1.878 (1.878)   Data 1.663 (1.663)   Loss 0.2603 (0.2603)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 19:17:06]
  Epoch: [012][200/391]   Time 0.203 (0.173)   Data 0.000 (0.009)   Loss 0.2721 (0.2662)   Prec@1 91.406 (90.590)   Prec@5 100.000 (99.786)   [2019-08-30 19:17:39]
  **Test** Prec@1 86.450 Prec@5 99.410 Error@1 13.550

==>>[2019-08-30 19:18:29] [Epoch=013/100] [Need: 01:53:01] [learning_rate=0.0100] [Best : Accuracy=86.45, Error=13.55]
  Epoch: [013][000/391]   Time 2.014 (2.014)   Data 1.768 (1.768)   Loss 0.2908 (0.2908)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 19:18:31]
  Epoch: [013][200/391]   Time 0.150 (0.198)   Data 0.000 (0.009)   Loss 0.2082 (0.2540)   Prec@1 92.969 (91.107)   Prec@5 100.000 (99.802)   [2019-08-30 19:19:08]
  **Test** Prec@1 85.300 Prec@5 99.380 Error@1 14.700

==>>[2019-08-30 19:19:48] [Epoch=014/100] [Need: 01:51:53] [learning_rate=0.0100] [Best : Accuracy=86.45, Error=13.55]
  Epoch: [014][000/391]   Time 1.830 (1.830)   Data 1.620 (1.620)   Loss 0.2413 (0.2413)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 19:19:50]
  Epoch: [014][200/391]   Time 0.226 (0.178)   Data 0.000 (0.008)   Loss 0.2892 (0.2624)   Prec@1 89.844 (91.037)   Prec@5 100.000 (99.833)   [2019-08-30 19:20:24]
  **Test** Prec@1 83.230 Prec@5 99.180 Error@1 16.770

==>>[2019-08-30 19:21:04] [Epoch=015/100] [Need: 01:50:22] [learning_rate=0.0100] [Best : Accuracy=86.45, Error=13.55]
  Epoch: [015][000/391]   Time 1.873 (1.873)   Data 1.631 (1.631)   Loss 0.2405 (0.2405)   Prec@1 90.625 (90.625)   Prec@5 99.219 (99.219)   [2019-08-30 19:21:06]
  Epoch: [015][200/391]   Time 0.173 (0.182)   Data 0.000 (0.008)   Loss 0.1924 (0.2625)   Prec@1 92.969 (90.738)   Prec@5 100.000 (99.786)   [2019-08-30 19:21:40]
  **Test** Prec@1 84.510 Prec@5 99.460 Error@1 15.490

==>>[2019-08-30 19:22:21] [Epoch=016/100] [Need: 01:49:00] [learning_rate=0.0100] [Best : Accuracy=86.45, Error=13.55]
  Epoch: [016][000/391]   Time 1.888 (1.888)   Data 1.643 (1.643)   Loss 0.2671 (0.2671)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2019-08-30 19:22:23]
  Epoch: [016][200/391]   Time 0.164 (0.178)   Data 0.000 (0.008)   Loss 0.4956 (0.2628)   Prec@1 83.594 (90.773)   Prec@5 98.438 (99.790)   [2019-08-30 19:22:57]
  **Test** Prec@1 86.590 Prec@5 99.510 Error@1 13.410

==>>[2019-08-30 19:23:37] [Epoch=017/100] [Need: 01:47:32] [learning_rate=0.0100] [Best : Accuracy=86.59, Error=13.41]
  Epoch: [017][000/391]   Time 1.886 (1.886)   Data 1.652 (1.652)   Loss 0.3116 (0.3116)   Prec@1 91.406 (91.406)   Prec@5 99.219 (99.219)   [2019-08-30 19:23:39]
  Epoch: [017][200/391]   Time 0.189 (0.178)   Data 0.000 (0.008)   Loss 0.3282 (0.2557)   Prec@1 90.625 (90.940)   Prec@5 99.219 (99.829)   [2019-08-30 19:24:12]
  **Test** Prec@1 86.570 Prec@5 99.570 Error@1 13.430

==>>[2019-08-30 19:24:52] [Epoch=018/100] [Need: 01:46:03] [learning_rate=0.0100] [Best : Accuracy=86.59, Error=13.41]
  Epoch: [018][000/391]   Time 1.787 (1.787)   Data 1.574 (1.574)   Loss 0.1657 (0.1657)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 19:24:54]
  Epoch: [018][200/391]   Time 0.159 (0.179)   Data 0.000 (0.008)   Loss 0.2031 (0.2562)   Prec@1 92.969 (90.784)   Prec@5 99.219 (99.833)   [2019-08-30 19:25:28]
  **Test** Prec@1 82.130 Prec@5 99.110 Error@1 17.870

==>>[2019-08-30 19:26:08] [Epoch=019/100] [Need: 01:44:37] [learning_rate=0.0100] [Best : Accuracy=86.59, Error=13.41]
  Epoch: [019][000/391]   Time 1.816 (1.816)   Data 1.593 (1.593)   Loss 0.2032 (0.2032)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 19:26:09]
  Epoch: [019][200/391]   Time 0.176 (0.180)   Data 0.000 (0.008)   Loss 0.2007 (0.2590)   Prec@1 93.750 (90.909)   Prec@5 100.000 (99.813)   [2019-08-30 19:26:44]
  **Test** Prec@1 86.470 Prec@5 99.380 Error@1 13.530

==>>[2019-08-30 19:27:24] [Epoch=020/100] [Need: 01:43:14] [learning_rate=0.0100] [Best : Accuracy=86.59, Error=13.41]
  Epoch: [020][000/391]   Time 1.784 (1.784)   Data 1.564 (1.564)   Loss 0.2189 (0.2189)   Prec@1 89.844 (89.844)   Prec@5 100.000 (100.000)   [2019-08-30 19:27:26]
  Epoch: [020][200/391]   Time 0.174 (0.180)   Data 0.000 (0.008)   Loss 0.3036 (0.2505)   Prec@1 92.969 (91.150)   Prec@5 100.000 (99.817)   [2019-08-30 19:28:00]
  **Test** Prec@1 83.670 Prec@5 99.030 Error@1 16.330

==>>[2019-08-30 19:28:40] [Epoch=021/100] [Need: 01:41:52] [learning_rate=0.0100] [Best : Accuracy=86.59, Error=13.41]
  Epoch: [021][000/391]   Time 1.776 (1.776)   Data 1.551 (1.551)   Loss 0.1596 (0.1596)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 19:28:42]
  Epoch: [021][200/391]   Time 0.173 (0.180)   Data 0.000 (0.008)   Loss 0.2028 (0.2523)   Prec@1 91.406 (91.305)   Prec@5 100.000 (99.775)   [2019-08-30 19:29:16]
  **Test** Prec@1 85.130 Prec@5 99.500 Error@1 14.870

==>>[2019-08-30 19:29:56] [Epoch=022/100] [Need: 01:40:31] [learning_rate=0.0100] [Best : Accuracy=86.59, Error=13.41]
  Epoch: [022][000/391]   Time 1.796 (1.796)   Data 1.573 (1.573)   Loss 0.2094 (0.2094)   Prec@1 93.750 (93.750)   Prec@5 99.219 (99.219)   [2019-08-30 19:29:58]
  Epoch: [022][200/391]   Time 0.167 (0.180)   Data 0.000 (0.008)   Loss 0.2373 (0.2548)   Prec@1 92.969 (91.196)   Prec@5 100.000 (99.759)   [2019-08-30 19:30:32]
  **Test** Prec@1 86.860 Prec@5 99.470 Error@1 13.140

==>>[2019-08-30 19:31:13] [Epoch=023/100] [Need: 01:39:10] [learning_rate=0.0100] [Best : Accuracy=86.86, Error=13.14]
  Epoch: [023][000/391]   Time 1.808 (1.808)   Data 1.613 (1.613)   Loss 0.2201 (0.2201)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 19:31:14]
  Epoch: [023][200/391]   Time 0.169 (0.182)   Data 0.000 (0.008)   Loss 0.3098 (0.2525)   Prec@1 90.625 (91.119)   Prec@5 100.000 (99.848)   [2019-08-30 19:31:49]
  **Test** Prec@1 86.630 Prec@5 99.510 Error@1 13.370

==>>[2019-08-30 19:32:29] [Epoch=024/100] [Need: 01:37:51] [learning_rate=0.0100] [Best : Accuracy=86.86, Error=13.14]
  Epoch: [024][000/391]   Time 1.749 (1.749)   Data 1.555 (1.555)   Loss 0.1590 (0.1590)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 19:32:31]
  Epoch: [024][200/391]   Time 0.153 (0.179)   Data 0.000 (0.008)   Loss 0.3100 (0.2512)   Prec@1 90.625 (91.216)   Prec@5 100.000 (99.798)   [2019-08-30 19:33:05]
  **Test** Prec@1 86.190 Prec@5 99.540 Error@1 13.810

==>>[2019-08-30 19:33:45] [Epoch=025/100] [Need: 01:36:28] [learning_rate=0.0100] [Best : Accuracy=86.86, Error=13.14]
  Epoch: [025][000/391]   Time 1.875 (1.875)   Data 1.653 (1.653)   Loss 0.2955 (0.2955)   Prec@1 86.719 (86.719)   Prec@5 100.000 (100.000)   [2019-08-30 19:33:46]
  Epoch: [025][200/391]   Time 0.168 (0.181)   Data 0.000 (0.008)   Loss 0.1496 (0.2507)   Prec@1 94.531 (91.270)   Prec@5 100.000 (99.806)   [2019-08-30 19:34:21]
  **Test** Prec@1 81.580 Prec@5 99.220 Error@1 18.420

==>>[2019-08-30 19:35:01] [Epoch=026/100] [Need: 01:35:08] [learning_rate=0.0100] [Best : Accuracy=86.86, Error=13.14]
  Epoch: [026][000/391]   Time 1.897 (1.897)   Data 1.700 (1.700)   Loss 0.1820 (0.1820)   Prec@1 95.312 (95.312)   Prec@5 99.219 (99.219)   [2019-08-30 19:35:03]
  Epoch: [026][200/391]   Time 0.159 (0.177)   Data 0.000 (0.009)   Loss 0.2466 (0.2574)   Prec@1 91.406 (91.014)   Prec@5 100.000 (99.821)   [2019-08-30 19:35:37]
  **Test** Prec@1 84.980 Prec@5 99.170 Error@1 15.020

==>>[2019-08-30 19:36:16] [Epoch=027/100] [Need: 01:33:46] [learning_rate=0.0100] [Best : Accuracy=86.86, Error=13.14]
  Epoch: [027][000/391]   Time 1.822 (1.822)   Data 1.638 (1.638)   Loss 0.2632 (0.2632)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2019-08-30 19:36:18]
  Epoch: [027][200/391]   Time 0.173 (0.182)   Data 0.000 (0.008)   Loss 0.3025 (0.2444)   Prec@1 89.844 (91.317)   Prec@5 100.000 (99.833)   [2019-08-30 19:36:53]
  **Test** Prec@1 84.900 Prec@5 99.240 Error@1 15.100

==>>[2019-08-30 19:37:32] [Epoch=028/100] [Need: 01:32:27] [learning_rate=0.0100] [Best : Accuracy=86.86, Error=13.14]
  Epoch: [028][000/391]   Time 1.884 (1.884)   Data 1.660 (1.660)   Loss 0.2243 (0.2243)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 19:37:34]
  Epoch: [028][200/391]   Time 0.167 (0.178)   Data 0.000 (0.008)   Loss 0.3591 (0.2533)   Prec@1 88.281 (91.161)   Prec@5 99.219 (99.778)   [2019-08-30 19:38:08]
  **Test** Prec@1 84.710 Prec@5 99.340 Error@1 15.290

==>>[2019-08-30 19:38:48] [Epoch=029/100] [Need: 01:31:06] [learning_rate=0.0100] [Best : Accuracy=86.86, Error=13.14]
  Epoch: [029][000/391]   Time 1.825 (1.825)   Data 1.618 (1.618)   Loss 0.2776 (0.2776)   Prec@1 89.844 (89.844)   Prec@5 100.000 (100.000)   [2019-08-30 19:38:50]
  Epoch: [029][200/391]   Time 0.177 (0.177)   Data 0.000 (0.008)   Loss 0.2353 (0.2530)   Prec@1 91.406 (91.123)   Prec@5 99.219 (99.860)   [2019-08-30 19:39:24]
  **Test** Prec@1 85.320 Prec@5 99.350 Error@1 14.680

==>>[2019-08-30 19:40:04] [Epoch=030/100] [Need: 01:29:46] [learning_rate=0.0010] [Best : Accuracy=86.86, Error=13.14]
  Epoch: [030][000/391]   Time 1.880 (1.880)   Data 1.663 (1.663)   Loss 0.1765 (0.1765)   Prec@1 95.312 (95.312)   Prec@5 99.219 (99.219)   [2019-08-30 19:40:05]
  Epoch: [030][200/391]   Time 0.173 (0.177)   Data 0.000 (0.008)   Loss 0.1332 (0.2181)   Prec@1 95.312 (92.537)   Prec@5 100.000 (99.872)   [2019-08-30 19:40:39]
  **Test** Prec@1 88.640 Prec@5 99.580 Error@1 11.360

==>>[2019-08-30 19:41:19] [Epoch=031/100] [Need: 01:28:27] [learning_rate=0.0010] [Best : Accuracy=88.64, Error=11.36]
  Epoch: [031][000/391]   Time 1.853 (1.853)   Data 1.635 (1.635)   Loss 0.1689 (0.1689)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 19:41:21]
  Epoch: [031][200/391]   Time 0.177 (0.178)   Data 0.000 (0.008)   Loss 0.2124 (0.2027)   Prec@1 91.406 (92.992)   Prec@5 100.000 (99.907)   [2019-08-30 19:41:55]
  **Test** Prec@1 88.260 Prec@5 99.590 Error@1 11.740

==>>[2019-08-30 19:42:35] [Epoch=032/100] [Need: 01:27:06] [learning_rate=0.0010] [Best : Accuracy=88.64, Error=11.36]
  Epoch: [032][000/391]   Time 1.858 (1.858)   Data 1.636 (1.636)   Loss 0.2278 (0.2278)   Prec@1 91.406 (91.406)   Prec@5 99.219 (99.219)   [2019-08-30 19:42:37]
  Epoch: [032][200/391]   Time 0.167 (0.180)   Data 0.000 (0.008)   Loss 0.1759 (0.1938)   Prec@1 91.406 (93.194)   Prec@5 100.000 (99.930)   [2019-08-30 19:43:11]
  **Test** Prec@1 88.840 Prec@5 99.610 Error@1 11.160

==>>[2019-08-30 19:43:51] [Epoch=033/100] [Need: 01:25:48] [learning_rate=0.0010] [Best : Accuracy=88.84, Error=11.16]
  Epoch: [033][000/391]   Time 1.868 (1.868)   Data 1.631 (1.631)   Loss 0.2459 (0.2459)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2019-08-30 19:43:53]
  Epoch: [033][200/391]   Time 0.175 (0.178)   Data 0.000 (0.008)   Loss 0.2615 (0.1844)   Prec@1 90.625 (93.766)   Prec@5 100.000 (99.918)   [2019-08-30 19:44:27]
  **Test** Prec@1 88.470 Prec@5 99.580 Error@1 11.530

==>>[2019-08-30 19:45:06] [Epoch=034/100] [Need: 01:24:28] [learning_rate=0.0010] [Best : Accuracy=88.84, Error=11.16]
  Epoch: [034][000/391]   Time 1.854 (1.854)   Data 1.652 (1.652)   Loss 0.1280 (0.1280)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 19:45:08]
  Epoch: [034][200/391]   Time 0.189 (0.180)   Data 0.000 (0.008)   Loss 0.1407 (0.1926)   Prec@1 94.531 (93.369)   Prec@5 100.000 (99.883)   [2019-08-30 19:45:42]
  **Test** Prec@1 88.280 Prec@5 99.550 Error@1 11.720

==>>[2019-08-30 19:46:22] [Epoch=035/100] [Need: 01:23:10] [learning_rate=0.0010] [Best : Accuracy=88.84, Error=11.16]
  Epoch: [035][000/391]   Time 1.901 (1.901)   Data 1.674 (1.674)   Loss 0.1488 (0.1488)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 19:46:24]
  Epoch: [035][200/391]   Time 0.156 (0.179)   Data 0.000 (0.008)   Loss 0.2105 (0.1922)   Prec@1 92.969 (93.319)   Prec@5 100.000 (99.911)   [2019-08-30 19:46:58]
  **Test** Prec@1 87.740 Prec@5 99.560 Error@1 12.260

==>>[2019-08-30 19:47:38] [Epoch=036/100] [Need: 01:21:51] [learning_rate=0.0010] [Best : Accuracy=88.84, Error=11.16]
  Epoch: [036][000/391]   Time 1.943 (1.943)   Data 1.714 (1.714)   Loss 0.2199 (0.2199)   Prec@1 89.844 (89.844)   Prec@5 100.000 (100.000)   [2019-08-30 19:47:40]
  Epoch: [036][200/391]   Time 0.162 (0.181)   Data 0.000 (0.009)   Loss 0.1896 (0.1911)   Prec@1 92.969 (93.260)   Prec@5 100.000 (99.918)   [2019-08-30 19:48:15]
  **Test** Prec@1 88.600 Prec@5 99.660 Error@1 11.400

==>>[2019-08-30 19:48:55] [Epoch=037/100] [Need: 01:20:34] [learning_rate=0.0010] [Best : Accuracy=88.84, Error=11.16]
  Epoch: [037][000/391]   Time 1.851 (1.851)   Data 1.645 (1.645)   Loss 0.2102 (0.2102)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 19:48:57]
  Epoch: [037][200/391]   Time 0.163 (0.181)   Data 0.000 (0.008)   Loss 0.2108 (0.1884)   Prec@1 89.844 (93.264)   Prec@5 100.000 (99.903)   [2019-08-30 19:49:31]
  **Test** Prec@1 88.720 Prec@5 99.620 Error@1 11.280

==>>[2019-08-30 19:50:12] [Epoch=038/100] [Need: 01:19:18] [learning_rate=0.0010] [Best : Accuracy=88.84, Error=11.16]
  Epoch: [038][000/391]   Time 1.818 (1.818)   Data 1.610 (1.610)   Loss 0.1305 (0.1305)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 19:50:13]
  Epoch: [038][200/391]   Time 0.149 (0.181)   Data 0.000 (0.008)   Loss 0.2603 (0.1884)   Prec@1 91.406 (93.334)   Prec@5 100.000 (99.918)   [2019-08-30 19:50:48]
  **Test** Prec@1 88.510 Prec@5 99.710 Error@1 11.490

==>>[2019-08-30 19:51:28] [Epoch=039/100] [Need: 01:18:01] [learning_rate=0.0010] [Best : Accuracy=88.84, Error=11.16]
  Epoch: [039][000/391]   Time 1.780 (1.780)   Data 1.580 (1.580)   Loss 0.0753 (0.0753)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 19:51:30]
  Epoch: [039][200/391]   Time 0.199 (0.180)   Data 0.000 (0.008)   Loss 0.2063 (0.1885)   Prec@1 92.188 (93.229)   Prec@5 100.000 (99.914)   [2019-08-30 19:52:04]
  **Test** Prec@1 88.990 Prec@5 99.660 Error@1 11.010

==>>[2019-08-30 19:52:45] [Epoch=040/100] [Need: 01:16:44] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [040][000/391]   Time 1.891 (1.891)   Data 1.630 (1.630)   Loss 0.2463 (0.2463)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 19:52:47]
  Epoch: [040][200/391]   Time 0.164 (0.182)   Data 0.000 (0.008)   Loss 0.1738 (0.1815)   Prec@1 93.750 (93.688)   Prec@5 100.000 (99.930)   [2019-08-30 19:53:21]
  **Test** Prec@1 87.580 Prec@5 99.600 Error@1 12.420

==>>[2019-08-30 19:54:02] [Epoch=041/100] [Need: 01:15:27] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [041][000/391]   Time 1.930 (1.930)   Data 1.693 (1.693)   Loss 0.1791 (0.1791)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 19:54:03]
  Epoch: [041][200/391]   Time 0.180 (0.180)   Data 0.000 (0.009)   Loss 0.1897 (0.1809)   Prec@1 92.969 (93.703)   Prec@5 100.000 (99.934)   [2019-08-30 19:54:38]
  **Test** Prec@1 88.600 Prec@5 99.590 Error@1 11.400

==>>[2019-08-30 19:55:18] [Epoch=042/100] [Need: 01:14:10] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [042][000/391]   Time 1.860 (1.860)   Data 1.649 (1.649)   Loss 0.2504 (0.2504)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2019-08-30 19:55:20]
  Epoch: [042][200/391]   Time 0.195 (0.179)   Data 0.000 (0.008)   Loss 0.1681 (0.1866)   Prec@1 94.531 (93.365)   Prec@5 100.000 (99.914)   [2019-08-30 19:55:54]
  **Test** Prec@1 87.100 Prec@5 99.500 Error@1 12.900

==>>[2019-08-30 19:56:34] [Epoch=043/100] [Need: 01:12:53] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [043][000/391]   Time 1.904 (1.904)   Data 1.690 (1.690)   Loss 0.2345 (0.2345)   Prec@1 92.188 (92.188)   Prec@5 99.219 (99.219)   [2019-08-30 19:56:36]
  Epoch: [043][200/391]   Time 0.172 (0.177)   Data 0.000 (0.009)   Loss 0.1340 (0.1904)   Prec@1 95.312 (93.299)   Prec@5 100.000 (99.907)   [2019-08-30 19:57:10]
  **Test** Prec@1 88.190 Prec@5 99.570 Error@1 11.810

==>>[2019-08-30 19:57:50] [Epoch=044/100] [Need: 01:11:34] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [044][000/391]   Time 1.879 (1.879)   Data 1.648 (1.648)   Loss 0.1953 (0.1953)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 19:57:52]
  Epoch: [044][200/391]   Time 0.191 (0.179)   Data 0.000 (0.008)   Loss 0.1210 (0.1813)   Prec@1 94.531 (93.758)   Prec@5 100.000 (99.914)   [2019-08-30 19:58:26]
  **Test** Prec@1 88.630 Prec@5 99.600 Error@1 11.370

==>>[2019-08-30 19:59:06] [Epoch=045/100] [Need: 01:10:17] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [045][000/391]   Time 1.886 (1.886)   Data 1.648 (1.648)   Loss 0.1602 (0.1602)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 19:59:08]
  Epoch: [045][200/391]   Time 0.162 (0.180)   Data 0.000 (0.008)   Loss 0.1829 (0.1825)   Prec@1 95.312 (93.571)   Prec@5 100.000 (99.918)   [2019-08-30 19:59:42]
  **Test** Prec@1 87.870 Prec@5 99.530 Error@1 12.130

==>>[2019-08-30 20:00:22] [Epoch=046/100] [Need: 01:08:59] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [046][000/391]   Time 1.863 (1.863)   Data 1.631 (1.631)   Loss 0.1262 (0.1262)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 20:00:24]
  Epoch: [046][200/391]   Time 0.200 (0.181)   Data 0.000 (0.008)   Loss 0.2128 (0.1813)   Prec@1 91.406 (93.832)   Prec@5 100.000 (99.899)   [2019-08-30 20:00:58]
  **Test** Prec@1 88.730 Prec@5 99.580 Error@1 11.270

==>>[2019-08-30 20:01:38] [Epoch=047/100] [Need: 01:07:42] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [047][000/391]   Time 1.866 (1.866)   Data 1.640 (1.640)   Loss 0.2070 (0.2070)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 20:01:40]
  Epoch: [047][200/391]   Time 0.176 (0.179)   Data 0.000 (0.008)   Loss 0.2326 (0.1859)   Prec@1 92.969 (93.458)   Prec@5 100.000 (99.922)   [2019-08-30 20:02:14]
  **Test** Prec@1 88.240 Prec@5 99.530 Error@1 11.760

==>>[2019-08-30 20:02:54] [Epoch=048/100] [Need: 01:06:25] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [048][000/391]   Time 1.829 (1.829)   Data 1.595 (1.595)   Loss 0.1676 (0.1676)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 20:02:56]
  Epoch: [048][200/391]   Time 0.176 (0.177)   Data 0.000 (0.008)   Loss 0.1408 (0.1858)   Prec@1 96.094 (93.451)   Prec@5 100.000 (99.911)   [2019-08-30 20:03:30]
  **Test** Prec@1 88.290 Prec@5 99.520 Error@1 11.710

==>>[2019-08-30 20:04:09] [Epoch=049/100] [Need: 01:05:07] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [049][000/391]   Time 1.861 (1.861)   Data 1.632 (1.632)   Loss 0.1280 (0.1280)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 20:04:11]
  Epoch: [049][200/391]   Time 0.161 (0.179)   Data 0.000 (0.008)   Loss 0.1490 (0.1784)   Prec@1 95.312 (93.754)   Prec@5 100.000 (99.895)   [2019-08-30 20:04:45]
  **Test** Prec@1 88.480 Prec@5 99.590 Error@1 11.520

==>>[2019-08-30 20:05:26] [Epoch=050/100] [Need: 01:03:50] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [050][000/391]   Time 1.908 (1.908)   Data 1.679 (1.679)   Loss 0.2131 (0.2131)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 20:05:28]
  Epoch: [050][200/391]   Time 0.166 (0.180)   Data 0.000 (0.009)   Loss 0.2881 (0.1859)   Prec@1 89.062 (93.563)   Prec@5 100.000 (99.887)   [2019-08-30 20:06:02]
  **Test** Prec@1 88.310 Prec@5 99.520 Error@1 11.690

==>>[2019-08-30 20:06:42] [Epoch=051/100] [Need: 01:02:33] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [051][000/391]   Time 1.872 (1.872)   Data 1.639 (1.639)   Loss 0.1995 (0.1995)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 20:06:44]
  Epoch: [051][200/391]   Time 0.158 (0.178)   Data 0.000 (0.008)   Loss 0.2546 (0.1833)   Prec@1 90.625 (93.630)   Prec@5 100.000 (99.903)   [2019-08-30 20:07:18]
  **Test** Prec@1 87.310 Prec@5 99.540 Error@1 12.690

==>>[2019-08-30 20:07:57] [Epoch=052/100] [Need: 01:01:15] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [052][000/391]   Time 1.882 (1.882)   Data 1.668 (1.668)   Loss 0.1730 (0.1730)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 20:07:59]
  Epoch: [052][200/391]   Time 0.196 (0.185)   Data 0.000 (0.008)   Loss 0.1536 (0.1873)   Prec@1 96.094 (93.303)   Prec@5 100.000 (99.918)   [2019-08-30 20:08:35]
  **Test** Prec@1 88.700 Prec@5 99.590 Error@1 11.300

==>>[2019-08-30 20:09:14] [Epoch=053/100] [Need: 00:59:59] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [053][000/391]   Time 1.864 (1.864)   Data 1.621 (1.621)   Loss 0.0740 (0.0740)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 20:09:16]
  Epoch: [053][200/391]   Time 0.148 (0.181)   Data 0.000 (0.008)   Loss 0.1653 (0.1809)   Prec@1 92.188 (93.497)   Prec@5 100.000 (99.895)   [2019-08-30 20:09:51]
  **Test** Prec@1 88.460 Prec@5 99.620 Error@1 11.540

==>>[2019-08-30 20:10:31] [Epoch=054/100] [Need: 00:58:43] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [054][000/391]   Time 1.830 (1.830)   Data 1.617 (1.617)   Loss 0.1624 (0.1624)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 20:10:33]
  Epoch: [054][200/391]   Time 0.175 (0.177)   Data 0.000 (0.008)   Loss 0.1431 (0.1857)   Prec@1 95.312 (93.509)   Prec@5 100.000 (99.907)   [2019-08-30 20:11:07]
  **Test** Prec@1 88.750 Prec@5 99.580 Error@1 11.250

==>>[2019-08-30 20:11:47] [Epoch=055/100] [Need: 00:57:26] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [055][000/391]   Time 1.843 (1.843)   Data 1.620 (1.620)   Loss 0.1801 (0.1801)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 20:11:49]
  Epoch: [055][200/391]   Time 0.176 (0.180)   Data 0.000 (0.008)   Loss 0.1867 (0.1802)   Prec@1 93.750 (93.723)   Prec@5 100.000 (99.895)   [2019-08-30 20:12:23]
  **Test** Prec@1 88.420 Prec@5 99.640 Error@1 11.580

==>>[2019-08-30 20:13:03] [Epoch=056/100] [Need: 00:56:09] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [056][000/391]   Time 1.887 (1.887)   Data 1.657 (1.657)   Loss 0.1571 (0.1571)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 20:13:05]
  Epoch: [056][200/391]   Time 0.183 (0.179)   Data 0.000 (0.008)   Loss 0.1609 (0.1812)   Prec@1 93.750 (93.645)   Prec@5 100.000 (99.891)   [2019-08-30 20:13:40]
  **Test** Prec@1 87.710 Prec@5 99.600 Error@1 12.290

==>>[2019-08-30 20:14:20] [Epoch=057/100] [Need: 00:54:52] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [057][000/391]   Time 1.870 (1.870)   Data 1.650 (1.650)   Loss 0.1687 (0.1687)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 20:14:22]
  Epoch: [057][200/391]   Time 0.169 (0.180)   Data 0.000 (0.008)   Loss 0.1104 (0.1824)   Prec@1 96.094 (93.536)   Prec@5 100.000 (99.914)   [2019-08-30 20:14:56]
  **Test** Prec@1 88.330 Prec@5 99.620 Error@1 11.670

==>>[2019-08-30 20:15:36] [Epoch=058/100] [Need: 00:53:36] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [058][000/391]   Time 1.923 (1.923)   Data 1.686 (1.686)   Loss 0.2252 (0.2252)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 20:15:38]
  Epoch: [058][200/391]   Time 0.175 (0.179)   Data 0.000 (0.009)   Loss 0.2218 (0.1818)   Prec@1 90.625 (93.668)   Prec@5 100.000 (99.926)   [2019-08-30 20:16:12]
  **Test** Prec@1 88.960 Prec@5 99.630 Error@1 11.040

==>>[2019-08-30 20:16:52] [Epoch=059/100] [Need: 00:52:18] [learning_rate=0.0010] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [059][000/391]   Time 1.910 (1.910)   Data 1.659 (1.659)   Loss 0.2041 (0.2041)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 20:16:54]
  Epoch: [059][200/391]   Time 0.155 (0.177)   Data 0.000 (0.008)   Loss 0.2039 (0.1841)   Prec@1 88.281 (93.447)   Prec@5 100.000 (99.911)   [2019-08-30 20:17:28]
  **Test** Prec@1 88.270 Prec@5 99.660 Error@1 11.730

==>>[2019-08-30 20:18:07] [Epoch=060/100] [Need: 00:51:01] [learning_rate=0.0001] [Best : Accuracy=88.99, Error=11.01]
  Epoch: [060][000/391]   Time 1.958 (1.958)   Data 1.698 (1.698)   Loss 0.2181 (0.2181)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 20:18:09]
  Epoch: [060][200/391]   Time 0.167 (0.179)   Data 0.000 (0.009)   Loss 0.1612 (0.1749)   Prec@1 93.750 (93.987)   Prec@5 100.000 (99.946)   [2019-08-30 20:18:43]
  **Test** Prec@1 89.170 Prec@5 99.620 Error@1 10.830

==>>[2019-08-30 20:19:23] [Epoch=061/100] [Need: 00:49:44] [learning_rate=0.0001] [Best : Accuracy=89.17, Error=10.83]
  Epoch: [061][000/391]   Time 1.890 (1.890)   Data 1.676 (1.676)   Loss 0.1568 (0.1568)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 20:19:25]
  Epoch: [061][200/391]   Time 0.206 (0.180)   Data 0.000 (0.009)   Loss 0.1909 (0.1701)   Prec@1 90.625 (94.053)   Prec@5 100.000 (99.918)   [2019-08-30 20:19:59]
  **Test** Prec@1 89.070 Prec@5 99.680 Error@1 10.930

==>>[2019-08-30 20:20:39] [Epoch=062/100] [Need: 00:48:27] [learning_rate=0.0001] [Best : Accuracy=89.17, Error=10.83]
  Epoch: [062][000/391]   Time 1.867 (1.867)   Data 1.647 (1.647)   Loss 0.1671 (0.1671)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 20:20:41]
  Epoch: [062][200/391]   Time 0.178 (0.177)   Data 0.000 (0.008)   Loss 0.0967 (0.1629)   Prec@1 96.875 (94.329)   Prec@5 100.000 (99.922)   [2019-08-30 20:21:14]
  **Test** Prec@1 89.340 Prec@5 99.630 Error@1 10.660

==>>[2019-08-30 20:21:55] [Epoch=063/100] [Need: 00:47:10] [learning_rate=0.0001] [Best : Accuracy=89.34, Error=10.66]
  Epoch: [063][000/391]   Time 1.843 (1.843)   Data 1.652 (1.652)   Loss 0.1940 (0.1940)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 20:21:57]
  Epoch: [063][200/391]   Time 0.139 (0.180)   Data 0.000 (0.008)   Loss 0.2224 (0.1633)   Prec@1 92.188 (94.364)   Prec@5 100.000 (99.946)   [2019-08-30 20:22:31]
  **Test** Prec@1 89.190 Prec@5 99.580 Error@1 10.810

==>>[2019-08-30 20:23:11] [Epoch=064/100] [Need: 00:45:53] [learning_rate=0.0001] [Best : Accuracy=89.34, Error=10.66]
  Epoch: [064][000/391]   Time 1.874 (1.874)   Data 1.667 (1.667)   Loss 0.1240 (0.1240)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 20:23:13]
  Epoch: [064][200/391]   Time 0.163 (0.176)   Data 0.000 (0.008)   Loss 0.2036 (0.1622)   Prec@1 90.625 (94.224)   Prec@5 100.000 (99.938)   [2019-08-30 20:23:46]
  **Test** Prec@1 88.690 Prec@5 99.600 Error@1 11.310

==>>[2019-08-30 20:24:27] [Epoch=065/100] [Need: 00:44:36] [learning_rate=0.0001] [Best : Accuracy=89.34, Error=10.66]
  Epoch: [065][000/391]   Time 1.788 (1.788)   Data 1.588 (1.588)   Loss 0.0923 (0.0923)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 20:24:28]
  Epoch: [065][200/391]   Time 0.212 (0.180)   Data 0.000 (0.008)   Loss 0.1271 (0.1684)   Prec@1 94.531 (94.127)   Prec@5 100.000 (99.934)   [2019-08-30 20:25:03]
  **Test** Prec@1 89.540 Prec@5 99.710 Error@1 10.460

==>>[2019-08-30 20:25:44] [Epoch=066/100] [Need: 00:43:20] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [066][000/391]   Time 1.875 (1.875)   Data 1.652 (1.652)   Loss 0.1674 (0.1674)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 20:25:45]
  Epoch: [066][200/391]   Time 0.170 (0.178)   Data 0.000 (0.008)   Loss 0.1308 (0.1633)   Prec@1 94.531 (94.454)   Prec@5 100.000 (99.934)   [2019-08-30 20:26:19]
  **Test** Prec@1 89.080 Prec@5 99.660 Error@1 10.920

==>>[2019-08-30 20:26:59] [Epoch=067/100] [Need: 00:42:03] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [067][000/391]   Time 1.914 (1.914)   Data 1.688 (1.688)   Loss 0.1175 (0.1175)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 20:27:01]
  Epoch: [067][200/391]   Time 0.155 (0.178)   Data 0.000 (0.009)   Loss 0.0995 (0.1633)   Prec@1 96.094 (94.228)   Prec@5 100.000 (99.942)   [2019-08-30 20:27:35]
  **Test** Prec@1 88.630 Prec@5 99.600 Error@1 11.370

==>>[2019-08-30 20:28:15] [Epoch=068/100] [Need: 00:40:46] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [068][000/391]   Time 1.856 (1.856)   Data 1.638 (1.638)   Loss 0.1044 (0.1044)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 20:28:16]
  Epoch: [068][200/391]   Time 0.179 (0.181)   Data 0.000 (0.008)   Loss 0.1324 (0.1622)   Prec@1 92.188 (94.248)   Prec@5 100.000 (99.930)   [2019-08-30 20:28:51]
  **Test** Prec@1 88.590 Prec@5 99.580 Error@1 11.410

==>>[2019-08-30 20:29:31] [Epoch=069/100] [Need: 00:39:30] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [069][000/391]   Time 1.861 (1.861)   Data 1.639 (1.639)   Loss 0.1372 (0.1372)   Prec@1 95.312 (95.312)   Prec@5 99.219 (99.219)   [2019-08-30 20:29:33]
  Epoch: [069][200/391]   Time 0.151 (0.179)   Data 0.000 (0.008)   Loss 0.1911 (0.1662)   Prec@1 94.531 (94.135)   Prec@5 100.000 (99.930)   [2019-08-30 20:30:07]
  **Test** Prec@1 88.760 Prec@5 99.650 Error@1 11.240

==>>[2019-08-30 20:30:49] [Epoch=070/100] [Need: 00:38:14] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [070][000/391]   Time 2.132 (2.132)   Data 1.844 (1.844)   Loss 0.1283 (0.1283)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 20:30:51]
  Epoch: [070][200/391]   Time 0.221 (0.218)   Data 0.000 (0.009)   Loss 0.1362 (0.1632)   Prec@1 94.531 (94.391)   Prec@5 100.000 (99.911)   [2019-08-30 20:31:33]
  **Test** Prec@1 88.680 Prec@5 99.670 Error@1 11.320

==>>[2019-08-30 20:32:22] [Epoch=071/100] [Need: 00:37:04] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [071][000/391]   Time 2.185 (2.185)   Data 1.890 (1.890)   Loss 0.1180 (0.1180)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 20:32:24]
  Epoch: [071][200/391]   Time 0.145 (0.207)   Data 0.000 (0.010)   Loss 0.1184 (0.1613)   Prec@1 96.094 (94.419)   Prec@5 100.000 (99.953)   [2019-08-30 20:33:03]
  **Test** Prec@1 89.040 Prec@5 99.640 Error@1 10.960

==>>[2019-08-30 20:33:53] [Epoch=072/100] [Need: 00:35:53] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [072][000/391]   Time 2.029 (2.029)   Data 1.725 (1.725)   Loss 0.1400 (0.1400)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 20:33:55]
  Epoch: [072][200/391]   Time 0.238 (0.201)   Data 0.000 (0.009)   Loss 0.1463 (0.1620)   Prec@1 96.094 (94.376)   Prec@5 100.000 (99.961)   [2019-08-30 20:34:34]
  **Test** Prec@1 88.860 Prec@5 99.650 Error@1 11.140

==>>[2019-08-30 20:35:22] [Epoch=073/100] [Need: 00:34:41] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [073][000/391]   Time 2.035 (2.035)   Data 1.764 (1.764)   Loss 0.2305 (0.2305)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 20:35:24]
  Epoch: [073][200/391]   Time 0.238 (0.220)   Data 0.000 (0.009)   Loss 0.2166 (0.1626)   Prec@1 91.406 (94.197)   Prec@5 100.000 (99.957)   [2019-08-30 20:36:06]
  **Test** Prec@1 88.940 Prec@5 99.630 Error@1 11.060

==>>[2019-08-30 20:37:08] [Epoch=074/100] [Need: 00:33:34] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [074][000/391]   Time 2.626 (2.626)   Data 2.293 (2.293)   Loss 0.1024 (0.1024)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 20:37:10]
  Epoch: [074][200/391]   Time 0.269 (0.292)   Data 0.000 (0.012)   Loss 0.1681 (0.1651)   Prec@1 94.531 (94.236)   Prec@5 100.000 (99.930)   [2019-08-30 20:38:06]
  **Test** Prec@1 89.120 Prec@5 99.670 Error@1 10.880

==>>[2019-08-30 20:39:34] [Epoch=075/100] [Need: 00:32:39] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [075][000/391]   Time 3.230 (3.230)   Data 2.633 (2.633)   Loss 0.1995 (0.1995)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 20:39:38]
  Epoch: [075][200/391]   Time 0.252 (0.391)   Data 0.000 (0.013)   Loss 0.1388 (0.1641)   Prec@1 96.094 (94.415)   Prec@5 100.000 (99.946)   [2019-08-30 20:40:53]
  **Test** Prec@1 89.390 Prec@5 99.610 Error@1 10.610

==>>[2019-08-30 20:42:31] [Epoch=076/100] [Need: 00:31:52] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [076][000/391]   Time 5.632 (5.632)   Data 5.263 (5.263)   Loss 0.3087 (0.3087)   Prec@1 88.281 (88.281)   Prec@5 100.000 (100.000)   [2019-08-30 20:42:36]
  Epoch: [076][200/391]   Time 0.346 (0.447)   Data 0.000 (0.026)   Loss 0.1737 (0.1657)   Prec@1 93.750 (94.248)   Prec@5 100.000 (99.922)   [2019-08-30 20:44:00]
  **Test** Prec@1 88.560 Prec@5 99.630 Error@1 11.440

==>>[2019-08-30 20:45:45] [Epoch=077/100] [Need: 00:31:06] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [077][000/391]   Time 5.977 (5.977)   Data 5.560 (5.560)   Loss 0.0860 (0.0860)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 20:45:51]
  Epoch: [077][200/391]   Time 0.354 (0.431)   Data 0.000 (0.028)   Loss 0.1855 (0.1639)   Prec@1 93.750 (94.314)   Prec@5 100.000 (99.946)   [2019-08-30 20:47:12]
  **Test** Prec@1 88.670 Prec@5 99.630 Error@1 11.330

==>>[2019-08-30 20:48:54] [Epoch=078/100] [Need: 00:30:15] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [078][000/391]   Time 5.748 (5.748)   Data 5.219 (5.219)   Loss 0.1650 (0.1650)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 20:49:00]
  Epoch: [078][200/391]   Time 0.340 (0.421)   Data 0.000 (0.026)   Loss 0.1759 (0.1639)   Prec@1 95.312 (94.232)   Prec@5 100.000 (99.934)   [2019-08-30 20:50:19]
  **Test** Prec@1 88.770 Prec@5 99.650 Error@1 11.230

==>>[2019-08-30 20:52:08] [Epoch=079/100] [Need: 00:29:23] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [079][000/391]   Time 5.596 (5.596)   Data 5.009 (5.009)   Loss 0.1810 (0.1810)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 20:52:14]
  Epoch: [079][200/391]   Time 0.331 (0.468)   Data 0.000 (0.025)   Loss 0.2079 (0.1669)   Prec@1 92.188 (94.084)   Prec@5 100.000 (99.938)   [2019-08-30 20:53:42]
  **Test** Prec@1 88.510 Prec@5 99.640 Error@1 11.490

==>>[2019-08-30 20:55:32] [Epoch=080/100] [Need: 00:28:29] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [080][000/391]   Time 6.322 (6.322)   Data 5.866 (5.866)   Loss 0.1428 (0.1428)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 20:55:39]
  Epoch: [080][200/391]   Time 0.303 (0.477)   Data 0.000 (0.029)   Loss 0.1221 (0.1623)   Prec@1 96.875 (94.325)   Prec@5 100.000 (99.938)   [2019-08-30 20:57:08]
  **Test** Prec@1 88.890 Prec@5 99.640 Error@1 11.110

==>>[2019-08-30 20:59:00] [Epoch=081/100] [Need: 00:27:32] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [081][000/391]   Time 5.400 (5.400)   Data 4.795 (4.795)   Loss 0.1222 (0.1222)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 20:59:05]
  Epoch: [081][200/391]   Time 0.394 (0.477)   Data 0.000 (0.024)   Loss 0.2159 (0.1606)   Prec@1 92.188 (94.263)   Prec@5 100.000 (99.922)   [2019-08-30 21:00:36]
  **Test** Prec@1 88.460 Prec@5 99.630 Error@1 11.540

==>>[2019-08-30 21:02:27] [Epoch=082/100] [Need: 00:26:31] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [082][000/391]   Time 5.604 (5.604)   Data 5.245 (5.245)   Loss 0.1972 (0.1972)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 21:02:33]
  Epoch: [082][200/391]   Time 0.455 (0.457)   Data 0.000 (0.026)   Loss 0.1758 (0.1610)   Prec@1 93.750 (94.329)   Prec@5 100.000 (99.957)   [2019-08-30 21:03:59]
  **Test** Prec@1 87.630 Prec@5 99.450 Error@1 12.370

==>>[2019-08-30 21:05:53] [Epoch=083/100] [Need: 00:25:27] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [083][000/391]   Time 7.018 (7.018)   Data 6.415 (6.415)   Loss 0.1300 (0.1300)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:06:00]
  Epoch: [083][200/391]   Time 0.369 (0.476)   Data 0.000 (0.032)   Loss 0.1916 (0.1695)   Prec@1 93.750 (93.987)   Prec@5 100.000 (99.918)   [2019-08-30 21:07:29]
  **Test** Prec@1 88.470 Prec@5 99.600 Error@1 11.530

==>>[2019-08-30 21:09:18] [Epoch=084/100] [Need: 00:24:19] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [084][000/391]   Time 4.705 (4.705)   Data 4.117 (4.117)   Loss 0.1519 (0.1519)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 21:09:22]
  Epoch: [084][200/391]   Time 0.455 (0.474)   Data 0.000 (0.021)   Loss 0.1059 (0.1677)   Prec@1 96.094 (94.162)   Prec@5 100.000 (99.926)   [2019-08-30 21:10:53]
  **Test** Prec@1 88.880 Prec@5 99.570 Error@1 11.120

==>>[2019-08-30 21:12:45] [Epoch=085/100] [Need: 00:23:08] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [085][000/391]   Time 7.088 (7.088)   Data 6.507 (6.507)   Loss 0.1341 (0.1341)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 21:12:52]
  Epoch: [085][200/391]   Time 0.458 (0.472)   Data 0.000 (0.033)   Loss 0.2206 (0.1660)   Prec@1 92.188 (94.286)   Prec@5 100.000 (99.903)   [2019-08-30 21:14:20]
  **Test** Prec@1 88.760 Prec@5 99.620 Error@1 11.240

==>>[2019-08-30 21:16:12] [Epoch=086/100] [Need: 00:21:54] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [086][000/391]   Time 7.147 (7.147)   Data 6.344 (6.344)   Loss 0.1334 (0.1334)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:16:20]
  Epoch: [086][200/391]   Time 0.498 (0.473)   Data 0.000 (0.032)   Loss 0.1601 (0.1666)   Prec@1 92.969 (94.065)   Prec@5 100.000 (99.914)   [2019-08-30 21:17:47]
  **Test** Prec@1 88.740 Prec@5 99.560 Error@1 11.260

==>>[2019-08-30 21:19:39] [Epoch=087/100] [Need: 00:20:37] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [087][000/391]   Time 5.568 (5.568)   Data 4.870 (4.870)   Loss 0.1959 (0.1959)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2019-08-30 21:19:45]
  Epoch: [087][200/391]   Time 0.504 (0.455)   Data 0.000 (0.025)   Loss 0.1825 (0.1621)   Prec@1 93.750 (94.220)   Prec@5 99.219 (99.961)   [2019-08-30 21:21:11]
  **Test** Prec@1 88.910 Prec@5 99.580 Error@1 11.090

==>>[2019-08-30 21:23:05] [Epoch=088/100] [Need: 00:19:17] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [088][000/391]   Time 7.267 (7.267)   Data 6.645 (6.645)   Loss 0.1374 (0.1374)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:23:12]
  Epoch: [088][200/391]   Time 0.615 (0.476)   Data 0.000 (0.033)   Loss 0.1309 (0.1628)   Prec@1 96.094 (94.384)   Prec@5 100.000 (99.907)   [2019-08-30 21:24:40]
  **Test** Prec@1 87.640 Prec@5 99.550 Error@1 12.360

==>>[2019-08-30 21:26:30] [Epoch=089/100] [Need: 00:17:54] [learning_rate=0.0001] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [089][000/391]   Time 7.198 (7.198)   Data 6.619 (6.619)   Loss 0.1899 (0.1899)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 21:26:37]
  Epoch: [089][200/391]   Time 0.410 (0.484)   Data 0.000 (0.033)   Loss 0.2111 (0.1608)   Prec@1 92.969 (94.508)   Prec@5 100.000 (99.899)   [2019-08-30 21:28:08]
  **Test** Prec@1 88.670 Prec@5 99.660 Error@1 11.330

==>>[2019-08-30 21:29:58] [Epoch=090/100] [Need: 00:16:29] [learning_rate=0.0000] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [090][000/391]   Time 5.786 (5.786)   Data 5.098 (5.098)   Loss 0.1952 (0.1952)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 21:30:04]
  Epoch: [090][200/391]   Time 0.493 (0.460)   Data 0.000 (0.026)   Loss 0.1831 (0.1566)   Prec@1 94.531 (94.566)   Prec@5 100.000 (99.930)   [2019-08-30 21:31:31]
  **Test** Prec@1 89.480 Prec@5 99.640 Error@1 10.520

==>>[2019-08-30 21:33:26] [Epoch=091/100] [Need: 00:15:00] [learning_rate=0.0000] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [091][000/391]   Time 5.509 (5.509)   Data 4.990 (4.990)   Loss 0.0915 (0.0915)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 21:33:31]
  Epoch: [091][200/391]   Time 0.416 (0.457)   Data 0.000 (0.025)   Loss 0.1522 (0.1528)   Prec@1 96.094 (94.761)   Prec@5 100.000 (99.946)   [2019-08-30 21:34:57]
  **Test** Prec@1 89.430 Prec@5 99.630 Error@1 10.570

==>>[2019-08-30 21:36:51] [Epoch=092/100] [Need: 00:13:30] [learning_rate=0.0000] [Best : Accuracy=89.54, Error=10.46]
  Epoch: [092][000/391]   Time 5.480 (5.480)   Data 5.059 (5.059)   Loss 0.1497 (0.1497)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 21:36:56]
  Epoch: [092][200/391]   Time 0.511 (0.455)   Data 0.000 (0.025)   Loss 0.1967 (0.1522)   Prec@1 95.312 (94.753)   Prec@5 100.000 (99.961)   [2019-08-30 21:38:22]
  **Test** Prec@1 89.730 Prec@5 99.720 Error@1 10.270

==>>[2019-08-30 21:40:14] [Epoch=093/100] [Need: 00:11:56] [learning_rate=0.0000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [093][000/391]   Time 5.829 (5.829)   Data 5.238 (5.238)   Loss 0.1643 (0.1643)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 21:40:20]
  Epoch: [093][200/391]   Time 0.382 (0.471)   Data 0.000 (0.027)   Loss 0.2158 (0.1588)   Prec@1 92.188 (94.504)   Prec@5 100.000 (99.918)   [2019-08-30 21:41:48]
  **Test** Prec@1 89.070 Prec@5 99.640 Error@1 10.930

==>>[2019-08-30 21:43:40] [Epoch=094/100] [Need: 00:10:20] [learning_rate=0.0000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [094][000/391]   Time 6.806 (6.806)   Data 6.337 (6.337)   Loss 0.1793 (0.1793)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 21:43:47]
  Epoch: [094][200/391]   Time 0.404 (0.469)   Data 0.000 (0.032)   Loss 0.1694 (0.1535)   Prec@1 94.531 (94.694)   Prec@5 100.000 (99.953)   [2019-08-30 21:45:14]
  **Test** Prec@1 89.270 Prec@5 99.650 Error@1 10.730

==>>[2019-08-30 21:47:05] [Epoch=095/100] [Need: 00:08:42] [learning_rate=0.0000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [095][000/391]   Time 7.608 (7.608)   Data 7.134 (7.134)   Loss 0.1423 (0.1423)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 21:47:13]
  Epoch: [095][200/391]   Time 0.427 (0.488)   Data 0.000 (0.037)   Loss 0.0988 (0.1582)   Prec@1 96.094 (94.640)   Prec@5 100.000 (99.942)   [2019-08-30 21:48:43]
  **Test** Prec@1 89.310 Prec@5 99.660 Error@1 10.690

==>>[2019-08-30 21:50:39] [Epoch=096/100] [Need: 00:07:02] [learning_rate=0.0000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [096][000/391]   Time 6.522 (6.522)   Data 5.873 (5.873)   Loss 0.1829 (0.1829)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 21:50:45]
  Epoch: [096][200/391]   Time 0.301 (0.456)   Data 0.000 (0.029)   Loss 0.2107 (0.1613)   Prec@1 93.750 (94.341)   Prec@5 100.000 (99.926)   [2019-08-30 21:52:10]
  **Test** Prec@1 89.220 Prec@5 99.670 Error@1 10.780

==>>[2019-08-30 21:54:05] [Epoch=097/100] [Need: 00:05:20] [learning_rate=0.0000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [097][000/391]   Time 4.911 (4.911)   Data 4.431 (4.431)   Loss 0.1230 (0.1230)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:54:10]
  Epoch: [097][200/391]   Time 0.436 (0.456)   Data 0.000 (0.022)   Loss 0.1335 (0.1536)   Prec@1 94.531 (94.551)   Prec@5 100.000 (99.942)   [2019-08-30 21:55:37]
  **Test** Prec@1 89.330 Prec@5 99.700 Error@1 10.670

==>>[2019-08-30 21:57:31] [Epoch=098/100] [Need: 00:03:35] [learning_rate=0.0000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [098][000/391]   Time 5.858 (5.858)   Data 5.269 (5.269)   Loss 0.1319 (0.1319)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:57:37]
  Epoch: [098][200/391]   Time 0.308 (0.464)   Data 0.000 (0.027)   Loss 0.1795 (0.1599)   Prec@1 92.969 (94.422)   Prec@5 100.000 (99.930)   [2019-08-30 21:59:05]
  **Test** Prec@1 88.970 Prec@5 99.550 Error@1 11.030

==>>[2019-08-30 22:00:56] [Epoch=099/100] [Need: 00:01:48] [learning_rate=0.0000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [099][000/391]   Time 6.330 (6.330)   Data 5.754 (5.754)   Loss 0.1793 (0.1793)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 22:01:02]
  Epoch: [099][200/391]   Time 0.471 (0.491)   Data 0.000 (0.029)   Loss 0.1719 (0.1558)   Prec@1 93.750 (94.586)   Prec@5 100.000 (99.891)   [2019-08-30 22:02:35]
  **Test** Prec@1 89.180 Prec@5 99.600 Error@1 10.820
