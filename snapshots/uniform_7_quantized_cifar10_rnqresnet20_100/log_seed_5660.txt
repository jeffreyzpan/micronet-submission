save path : ./snapshots/uniform_7_quantized_cifar10_rnqresnet20_100
{'aq_bits': 7, 'aq_type': 'uniform', 'arch': 'rnqresnet20', 'batch_size': 128, 'data_path': '../../datasets/CIFAR10/cifar', 'dataset': 'cifar10', 'decay': 0.0001, 'epochs': 100, 'evaluate': False, 'gammas': [0.1, 0.1, 0.1], 'learning_rate': 0.01, 'manualSeed': 5660, 'momentum': 0.9, 'ngpu': 4, 'pretrained': '', 'print_freq': 200, 'resume': 'snapshots/finetune_cifar10_resnet20_300/model_best.pth.tar', 'save_path': './snapshots/uniform_7_quantized_cifar10_rnqresnet20_100', 'schedule': [30, 60, 90], 'start_epoch': 0, 'use_cuda': True, 'workers': 16}
Random Seed: 5660
python version : 3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]
torch  version : 1.1.0
cudnn  version : 7501
=> creating model 'rnqresnet20'
=> network :
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): QuantLinear(in_features=64, out_features=10, bias=True, method=KMeans, nbits=4)
)
=> loading checkpoint 'snapshots/finetune_cifar10_resnet20_300/model_best.pth.tar'
=> loaded checkpoint 'snapshots/finetune_cifar10_resnet20_300/model_best.pth.tar' (epoch 291)

==>>[2019-08-30 20:41:43] [Epoch=000/100] [Need: 00:00:00] [learning_rate=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/391]   Time 33.101 (33.101)   Data 1.268 (1.268)   Loss 0.0473 (0.0473)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 20:42:16]
  Epoch: [000][200/391]   Time 0.434 (0.507)   Data 0.000 (0.007)   Loss 0.2145 (0.1712)   Prec@1 91.406 (93.960)   Prec@5 100.000 (99.942)   [2019-08-30 20:43:25]
  **Test** Prec@1 86.990 Prec@5 99.570 Error@1 13.010

==>>[2019-08-30 20:44:49] [Epoch=001/100] [Need: 05:06:17] [learning_rate=0.0100] [Best : Accuracy=86.99, Error=13.01]
  Epoch: [001][000/391]   Time 3.254 (3.254)   Data 2.718 (2.718)   Loss 0.2028 (0.2028)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 20:44:53]
  Epoch: [001][200/391]   Time 0.334 (0.352)   Data 0.000 (0.014)   Loss 0.2837 (0.1550)   Prec@1 91.406 (94.516)   Prec@5 100.000 (99.949)   [2019-08-30 20:46:00]
  **Test** Prec@1 86.420 Prec@5 99.390 Error@1 13.580

==>>[2019-08-30 20:47:24] [Epoch=002/100] [Need: 04:37:22] [learning_rate=0.0100] [Best : Accuracy=86.99, Error=13.01]
  Epoch: [002][000/391]   Time 3.566 (3.566)   Data 3.080 (3.080)   Loss 0.1188 (0.1188)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 20:47:27]
  Epoch: [002][200/391]   Time 0.330 (0.375)   Data 0.000 (0.016)   Loss 0.1388 (0.1528)   Prec@1 96.875 (94.558)   Prec@5 100.000 (99.926)   [2019-08-30 20:48:39]
  **Test** Prec@1 87.520 Prec@5 99.390 Error@1 12.480

==>>[2019-08-30 20:50:04] [Epoch=003/100] [Need: 04:29:24] [learning_rate=0.0100] [Best : Accuracy=87.52, Error=12.48]
  Epoch: [003][000/391]   Time 3.518 (3.518)   Data 3.135 (3.135)   Loss 0.1077 (0.1077)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 20:50:07]
  Epoch: [003][200/391]   Time 0.427 (0.368)   Data 0.000 (0.016)   Loss 0.1077 (0.1552)   Prec@1 96.094 (94.469)   Prec@5 100.000 (99.922)   [2019-08-30 20:51:18]
  **Test** Prec@1 88.470 Prec@5 99.430 Error@1 11.530

==>>[2019-08-30 20:52:47] [Epoch=004/100] [Need: 04:25:11] [learning_rate=0.0100] [Best : Accuracy=88.47, Error=11.53]
  Epoch: [004][000/391]   Time 4.263 (4.263)   Data 3.669 (3.669)   Loss 0.1323 (0.1323)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 20:52:51]
  Epoch: [004][200/391]   Time 0.392 (0.394)   Data 0.000 (0.019)   Loss 0.1055 (0.1503)   Prec@1 97.656 (94.733)   Prec@5 100.000 (99.934)   [2019-08-30 20:54:06]
  **Test** Prec@1 87.540 Prec@5 99.390 Error@1 12.460

==>>[2019-08-30 20:55:39] [Epoch=005/100] [Need: 04:24:17] [learning_rate=0.0100] [Best : Accuracy=88.47, Error=11.53]
  Epoch: [005][000/391]   Time 3.621 (3.621)   Data 3.135 (3.135)   Loss 0.1932 (0.1932)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 20:55:42]
  Epoch: [005][200/391]   Time 0.509 (0.388)   Data 0.000 (0.016)   Loss 0.1031 (0.1530)   Prec@1 95.312 (94.558)   Prec@5 100.000 (99.930)   [2019-08-30 20:56:57]
  **Test** Prec@1 88.610 Prec@5 99.620 Error@1 11.390

==>>[2019-08-30 20:58:29] [Epoch=006/100] [Need: 04:22:26] [learning_rate=0.0100] [Best : Accuracy=88.61, Error=11.39]
  Epoch: [006][000/391]   Time 3.853 (3.853)   Data 3.413 (3.413)   Loss 0.0887 (0.0887)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 20:58:33]
  Epoch: [006][200/391]   Time 0.302 (0.397)   Data 0.000 (0.017)   Loss 0.1491 (0.1555)   Prec@1 96.094 (94.527)   Prec@5 100.000 (99.961)   [2019-08-30 20:59:49]
  **Test** Prec@1 87.670 Prec@5 99.390 Error@1 12.330

==>>[2019-08-30 21:01:22] [Epoch=007/100] [Need: 04:20:38] [learning_rate=0.0100] [Best : Accuracy=88.61, Error=11.39]
  Epoch: [007][000/391]   Time 3.257 (3.257)   Data 2.722 (2.722)   Loss 0.1927 (0.1927)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 21:01:25]
  Epoch: [007][200/391]   Time 0.407 (0.388)   Data 0.000 (0.014)   Loss 0.1605 (0.1508)   Prec@1 93.750 (94.547)   Prec@5 100.000 (99.965)   [2019-08-30 21:02:39]
  **Test** Prec@1 88.280 Prec@5 99.620 Error@1 11.720

==>>[2019-08-30 21:04:09] [Epoch=008/100] [Need: 04:17:54] [learning_rate=0.0100] [Best : Accuracy=88.61, Error=11.39]
  Epoch: [008][000/391]   Time 4.582 (4.582)   Data 3.965 (3.965)   Loss 0.1177 (0.1177)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 21:04:14]
  Epoch: [008][200/391]   Time 0.446 (0.393)   Data 0.000 (0.020)   Loss 0.1477 (0.1487)   Prec@1 91.406 (94.753)   Prec@5 100.000 (99.969)   [2019-08-30 21:05:29]
  **Test** Prec@1 88.410 Prec@5 99.530 Error@1 11.590

==>>[2019-08-30 21:06:59] [Epoch=009/100] [Need: 04:15:18] [learning_rate=0.0100] [Best : Accuracy=88.61, Error=11.39]
  Epoch: [009][000/391]   Time 3.499 (3.499)   Data 3.025 (3.025)   Loss 0.1161 (0.1161)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:07:03]
  Epoch: [009][200/391]   Time 0.397 (0.383)   Data 0.000 (0.015)   Loss 0.1600 (0.1531)   Prec@1 95.312 (94.593)   Prec@5 100.000 (99.926)   [2019-08-30 21:08:16]
  **Test** Prec@1 87.230 Prec@5 99.330 Error@1 12.770

==>>[2019-08-30 21:09:49] [Epoch=010/100] [Need: 04:12:47] [learning_rate=0.0100] [Best : Accuracy=88.61, Error=11.39]
  Epoch: [010][000/391]   Time 4.143 (4.143)   Data 3.750 (3.750)   Loss 0.1697 (0.1697)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 21:09:54]
  Epoch: [010][200/391]   Time 0.409 (0.396)   Data 0.000 (0.019)   Loss 0.1650 (0.1494)   Prec@1 93.750 (94.457)   Prec@5 100.000 (99.949)   [2019-08-30 21:11:09]
  **Test** Prec@1 88.210 Prec@5 99.460 Error@1 11.790

==>>[2019-08-30 21:12:41] [Epoch=011/100] [Need: 04:10:21] [learning_rate=0.0100] [Best : Accuracy=88.61, Error=11.39]
  Epoch: [011][000/391]   Time 4.084 (4.084)   Data 3.369 (3.369)   Loss 0.1225 (0.1225)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 21:12:45]
  Epoch: [011][200/391]   Time 0.442 (0.383)   Data 0.000 (0.017)   Loss 0.1271 (0.1524)   Prec@1 96.094 (94.796)   Prec@5 100.000 (99.930)   [2019-08-30 21:13:58]
  **Test** Prec@1 89.310 Prec@5 99.660 Error@1 10.690

==>>[2019-08-30 21:15:31] [Epoch=012/100] [Need: 04:07:42] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [012][000/391]   Time 4.795 (4.795)   Data 4.257 (4.257)   Loss 0.1540 (0.1540)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 21:15:36]
  Epoch: [012][200/391]   Time 0.561 (0.396)   Data 0.000 (0.021)   Loss 0.1220 (0.1539)   Prec@1 96.094 (94.586)   Prec@5 100.000 (99.965)   [2019-08-30 21:16:51]
  **Test** Prec@1 89.090 Prec@5 99.590 Error@1 10.910

==>>[2019-08-30 21:18:23] [Epoch=013/100] [Need: 04:05:14] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [013][000/391]   Time 4.277 (4.277)   Data 3.760 (3.760)   Loss 0.1430 (0.1430)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:18:27]
  Epoch: [013][200/391]   Time 0.541 (0.389)   Data 0.000 (0.019)   Loss 0.1052 (0.1496)   Prec@1 96.875 (94.691)   Prec@5 100.000 (99.965)   [2019-08-30 21:19:41]
  **Test** Prec@1 88.630 Prec@5 99.630 Error@1 11.370

==>>[2019-08-30 21:21:14] [Epoch=014/100] [Need: 04:02:35] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [014][000/391]   Time 3.784 (3.784)   Data 3.197 (3.197)   Loss 0.1602 (0.1602)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 21:21:18]
  Epoch: [014][200/391]   Time 0.316 (0.393)   Data 0.000 (0.016)   Loss 0.1489 (0.1476)   Prec@1 93.750 (94.776)   Prec@5 100.000 (99.938)   [2019-08-30 21:22:33]
  **Test** Prec@1 87.440 Prec@5 99.570 Error@1 12.560

==>>[2019-08-30 21:24:06] [Epoch=015/100] [Need: 04:00:00] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [015][000/391]   Time 4.684 (4.684)   Data 4.225 (4.225)   Loss 0.1611 (0.1611)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:24:10]
  Epoch: [015][200/391]   Time 0.345 (0.405)   Data 0.000 (0.021)   Loss 0.1617 (0.1545)   Prec@1 95.312 (94.667)   Prec@5 100.000 (99.942)   [2019-08-30 21:25:27]
  **Test** Prec@1 88.910 Prec@5 99.520 Error@1 11.090

==>>[2019-08-30 21:26:57] [Epoch=016/100] [Need: 03:57:22] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [016][000/391]   Time 4.588 (4.588)   Data 4.250 (4.250)   Loss 0.1990 (0.1990)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 21:27:02]
  Epoch: [016][200/391]   Time 0.435 (0.395)   Data 0.000 (0.021)   Loss 0.1851 (0.1561)   Prec@1 92.188 (94.434)   Prec@5 100.000 (99.930)   [2019-08-30 21:28:17]
  **Test** Prec@1 88.230 Prec@5 99.490 Error@1 11.770

==>>[2019-08-30 21:29:46] [Epoch=017/100] [Need: 03:54:32] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [017][000/391]   Time 4.144 (4.144)   Data 3.894 (3.894)   Loss 0.1351 (0.1351)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:29:50]
  Epoch: [017][200/391]   Time 0.380 (0.395)   Data 0.000 (0.020)   Loss 0.1451 (0.1522)   Prec@1 95.312 (94.694)   Prec@5 100.000 (99.961)   [2019-08-30 21:31:05]
  **Test** Prec@1 85.130 Prec@5 99.210 Error@1 14.870

==>>[2019-08-30 21:32:38] [Epoch=018/100] [Need: 03:51:50] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [018][000/391]   Time 4.459 (4.459)   Data 3.887 (3.887)   Loss 0.1753 (0.1753)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 21:32:42]
  Epoch: [018][200/391]   Time 0.268 (0.396)   Data 0.000 (0.020)   Loss 0.1783 (0.1524)   Prec@1 95.312 (94.648)   Prec@5 100.000 (99.957)   [2019-08-30 21:33:58]
  **Test** Prec@1 87.110 Prec@5 99.350 Error@1 12.890

==>>[2019-08-30 21:35:30] [Epoch=019/100] [Need: 03:49:15] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [019][000/391]   Time 5.378 (5.378)   Data 4.784 (4.784)   Loss 0.1290 (0.1290)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:35:36]
  Epoch: [019][200/391]   Time 0.291 (0.399)   Data 0.000 (0.024)   Loss 0.2182 (0.1460)   Prec@1 93.750 (94.897)   Prec@5 100.000 (99.930)   [2019-08-30 21:36:51]
  **Test** Prec@1 88.790 Prec@5 99.630 Error@1 11.210

==>>[2019-08-30 21:38:22] [Epoch=020/100] [Need: 03:46:29] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [020][000/391]   Time 5.028 (5.028)   Data 4.574 (4.574)   Loss 0.1046 (0.1046)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:38:27]
  Epoch: [020][200/391]   Time 0.409 (0.403)   Data 0.000 (0.023)   Loss 0.1351 (0.1488)   Prec@1 92.969 (94.796)   Prec@5 100.000 (99.946)   [2019-08-30 21:39:43]
  **Test** Prec@1 88.510 Prec@5 99.580 Error@1 11.490

==>>[2019-08-30 21:41:14] [Epoch=021/100] [Need: 03:43:50] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [021][000/391]   Time 4.308 (4.308)   Data 3.756 (3.756)   Loss 0.0965 (0.0965)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 21:41:19]
  Epoch: [021][200/391]   Time 0.286 (0.402)   Data 0.000 (0.019)   Loss 0.1241 (0.1633)   Prec@1 95.312 (94.193)   Prec@5 100.000 (99.895)   [2019-08-30 21:42:35]
  **Test** Prec@1 88.250 Prec@5 99.520 Error@1 11.750

==>>[2019-08-30 21:44:01] [Epoch=022/100] [Need: 03:40:48] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [022][000/391]   Time 3.944 (3.944)   Data 3.479 (3.479)   Loss 0.0941 (0.0941)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:44:04]
  Epoch: [022][200/391]   Time 0.557 (0.406)   Data 0.000 (0.018)   Loss 0.1976 (0.1566)   Prec@1 92.188 (94.395)   Prec@5 100.000 (99.942)   [2019-08-30 21:45:22]
  **Test** Prec@1 88.190 Prec@5 99.630 Error@1 11.810

==>>[2019-08-30 21:46:52] [Epoch=023/100] [Need: 03:38:01] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [023][000/391]   Time 5.195 (5.195)   Data 4.646 (4.646)   Loss 0.1093 (0.1093)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:46:57]
  Epoch: [023][200/391]   Time 0.205 (0.406)   Data 0.000 (0.023)   Loss 0.1641 (0.1573)   Prec@1 93.750 (94.512)   Prec@5 100.000 (99.930)   [2019-08-30 21:48:14]
  **Test** Prec@1 87.100 Prec@5 99.530 Error@1 12.900

==>>[2019-08-30 21:49:44] [Epoch=024/100] [Need: 03:35:20] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [024][000/391]   Time 5.191 (5.191)   Data 4.522 (4.522)   Loss 0.1329 (0.1329)   Prec@1 96.875 (96.875)   Prec@5 99.219 (99.219)   [2019-08-30 21:49:50]
  Epoch: [024][200/391]   Time 0.331 (0.402)   Data 0.000 (0.023)   Loss 0.1376 (0.1528)   Prec@1 95.312 (94.663)   Prec@5 100.000 (99.949)   [2019-08-30 21:51:05]
  **Test** Prec@1 88.230 Prec@5 99.650 Error@1 11.770

==>>[2019-08-30 21:52:34] [Epoch=025/100] [Need: 03:32:29] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [025][000/391]   Time 5.101 (5.101)   Data 4.434 (4.434)   Loss 0.1368 (0.1368)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:52:39]
  Epoch: [025][200/391]   Time 0.554 (0.414)   Data 0.000 (0.022)   Loss 0.1797 (0.1572)   Prec@1 92.969 (94.384)   Prec@5 100.000 (99.934)   [2019-08-30 21:53:57]
  **Test** Prec@1 88.040 Prec@5 99.560 Error@1 11.960

==>>[2019-08-30 21:55:27] [Epoch=026/100] [Need: 03:29:48] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [026][000/391]   Time 3.593 (3.593)   Data 3.101 (3.101)   Loss 0.2027 (0.2027)   Prec@1 92.969 (92.969)   Prec@5 99.219 (99.219)   [2019-08-30 21:55:30]
  Epoch: [026][200/391]   Time 0.232 (0.396)   Data 0.000 (0.016)   Loss 0.1187 (0.1612)   Prec@1 95.312 (94.317)   Prec@5 100.000 (99.911)   [2019-08-30 21:56:46]
  **Test** Prec@1 86.470 Prec@5 99.470 Error@1 13.530

==>>[2019-08-30 21:58:16] [Epoch=027/100] [Need: 03:26:54] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [027][000/391]   Time 5.031 (5.031)   Data 4.494 (4.494)   Loss 0.0894 (0.0894)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 21:58:21]
  Epoch: [027][200/391]   Time 0.503 (0.403)   Data 0.000 (0.023)   Loss 0.1793 (0.1546)   Prec@1 94.531 (94.566)   Prec@5 100.000 (99.938)   [2019-08-30 21:59:37]
  **Test** Prec@1 87.580 Prec@5 99.500 Error@1 12.420

==>>[2019-08-30 22:01:06] [Epoch=028/100] [Need: 03:24:05] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [028][000/391]   Time 3.810 (3.810)   Data 3.397 (3.397)   Loss 0.1617 (0.1617)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 22:01:10]
  Epoch: [028][200/391]   Time 0.442 (0.386)   Data 0.000 (0.019)   Loss 0.1083 (0.1522)   Prec@1 94.531 (94.663)   Prec@5 100.000 (99.946)   [2019-08-30 22:02:24]
  **Test** Prec@1 88.430 Prec@5 99.540 Error@1 11.570

==>>[2019-08-30 22:03:54] [Epoch=029/100] [Need: 03:21:10] [learning_rate=0.0100] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [029][000/391]   Time 4.197 (4.197)   Data 3.893 (3.893)   Loss 0.1411 (0.1411)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 22:03:58]
  Epoch: [029][200/391]   Time 0.364 (0.391)   Data 0.000 (0.020)   Loss 0.1003 (0.1553)   Prec@1 97.656 (94.574)   Prec@5 100.000 (99.922)   [2019-08-30 22:05:13]
  **Test** Prec@1 88.150 Prec@5 99.550 Error@1 11.850

==>>[2019-08-30 22:06:34] [Epoch=030/100] [Need: 03:17:57] [learning_rate=0.0010] [Best : Accuracy=89.31, Error=10.69]
  Epoch: [030][000/391]   Time 3.366 (3.366)   Data 2.934 (2.934)   Loss 0.1383 (0.1383)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 22:06:38]
  Epoch: [030][200/391]   Time 0.351 (0.375)   Data 0.000 (0.015)   Loss 0.1190 (0.1242)   Prec@1 96.875 (95.861)   Prec@5 100.000 (99.957)   [2019-08-30 22:07:50]
  **Test** Prec@1 90.730 Prec@5 99.730 Error@1 9.270

==>>[2019-08-30 22:09:15] [Epoch=031/100] [Need: 03:14:46] [learning_rate=0.0010] [Best : Accuracy=90.73, Error=9.27]
  Epoch: [031][000/391]   Time 2.937 (2.937)   Data 2.587 (2.587)   Loss 0.0624 (0.0624)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 22:09:18]
  Epoch: [031][200/391]   Time 0.454 (0.373)   Data 0.000 (0.014)   Loss 0.0866 (0.1069)   Prec@1 97.656 (96.444)   Prec@5 100.000 (99.969)   [2019-08-30 22:10:30]
  **Test** Prec@1 90.570 Prec@5 99.700 Error@1 9.430

==>>[2019-08-30 22:11:52] [Epoch=032/100] [Need: 03:11:31] [learning_rate=0.0010] [Best : Accuracy=90.73, Error=9.27]
  Epoch: [032][000/391]   Time 3.767 (3.767)   Data 3.378 (3.378)   Loss 0.1575 (0.1575)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 22:11:55]
  Epoch: [032][200/391]   Time 0.256 (0.372)   Data 0.000 (0.017)   Loss 0.0773 (0.1008)   Prec@1 99.219 (96.498)   Prec@5 100.000 (99.977)   [2019-08-30 22:13:06]
  **Test** Prec@1 90.630 Prec@5 99.680 Error@1 9.370

==>>[2019-08-30 22:14:30] [Epoch=033/100] [Need: 03:08:19] [learning_rate=0.0010] [Best : Accuracy=90.73, Error=9.27]
  Epoch: [033][000/391]   Time 3.786 (3.786)   Data 3.283 (3.283)   Loss 0.0740 (0.0740)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 22:14:34]
  Epoch: [033][200/391]   Time 0.514 (0.389)   Data 0.000 (0.017)   Loss 0.0547 (0.1004)   Prec@1 98.438 (96.595)   Prec@5 100.000 (99.977)   [2019-08-30 22:15:48]
  **Test** Prec@1 90.920 Prec@5 99.740 Error@1 9.080

==>>[2019-08-30 22:17:20] [Epoch=034/100] [Need: 03:05:34] [learning_rate=0.0010] [Best : Accuracy=90.92, Error=9.08]
  Epoch: [034][000/391]   Time 3.747 (3.747)   Data 3.442 (3.442)   Loss 0.1131 (0.1131)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 22:17:24]
  Epoch: [034][200/391]   Time 0.273 (0.400)   Data 0.000 (0.017)   Loss 0.1471 (0.0964)   Prec@1 94.531 (96.708)   Prec@5 100.000 (99.977)   [2019-08-30 22:18:41]
  **Test** Prec@1 90.880 Prec@5 99.700 Error@1 9.120

==>>[2019-08-30 22:20:13] [Epoch=035/100] [Need: 03:02:52] [learning_rate=0.0010] [Best : Accuracy=90.92, Error=9.08]
  Epoch: [035][000/391]   Time 4.768 (4.768)   Data 4.268 (4.268)   Loss 0.0786 (0.0786)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 22:20:17]
  Epoch: [035][200/391]   Time 0.515 (0.401)   Data 0.000 (0.021)   Loss 0.1132 (0.0934)   Prec@1 95.312 (96.770)   Prec@5 100.000 (99.961)   [2019-08-30 22:21:33]
  **Test** Prec@1 90.900 Prec@5 99.640 Error@1 9.100

==>>[2019-08-30 22:23:05] [Epoch=036/100] [Need: 03:00:10] [learning_rate=0.0010] [Best : Accuracy=90.92, Error=9.08]
  Epoch: [036][000/391]   Time 5.584 (5.584)   Data 5.071 (5.071)   Loss 0.1523 (0.1523)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 22:23:10]
  Epoch: [036][200/391]   Time 0.403 (0.409)   Data 0.000 (0.025)   Loss 0.0661 (0.0902)   Prec@1 97.656 (97.030)   Prec@5 100.000 (99.988)   [2019-08-30 22:24:27]
  **Test** Prec@1 90.500 Prec@5 99.720 Error@1 9.500

==>>[2019-08-30 22:25:57] [Epoch=037/100] [Need: 02:57:26] [learning_rate=0.0010] [Best : Accuracy=90.92, Error=9.08]
  Epoch: [037][000/391]   Time 3.984 (3.984)   Data 3.541 (3.541)   Loss 0.0558 (0.0558)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 22:26:01]
  Epoch: [037][200/391]   Time 0.533 (0.396)   Data 0.000 (0.018)   Loss 0.1268 (0.0918)   Prec@1 94.531 (96.871)   Prec@5 100.000 (99.981)   [2019-08-30 22:27:17]
  **Test** Prec@1 91.110 Prec@5 99.740 Error@1 8.890

==>>[2019-08-30 22:28:46] [Epoch=038/100] [Need: 02:54:37] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [038][000/391]   Time 5.345 (5.345)   Data 4.992 (4.992)   Loss 0.0585 (0.0585)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 22:28:51]
  Epoch: [038][200/391]   Time 0.486 (0.404)   Data 0.000 (0.025)   Loss 0.1672 (0.0886)   Prec@1 92.188 (97.065)   Prec@5 100.000 (99.988)   [2019-08-30 22:30:07]
  **Test** Prec@1 90.550 Prec@5 99.610 Error@1 9.450

==>>[2019-08-30 22:31:40] [Epoch=039/100] [Need: 02:51:56] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [039][000/391]   Time 3.669 (3.669)   Data 3.314 (3.314)   Loss 0.0386 (0.0386)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 22:31:43]
  Epoch: [039][200/391]   Time 0.476 (0.388)   Data 0.000 (0.017)   Loss 0.0594 (0.0914)   Prec@1 96.875 (96.871)   Prec@5 100.000 (99.992)   [2019-08-30 22:32:58]
  **Test** Prec@1 90.740 Prec@5 99.730 Error@1 9.260

==>>[2019-08-30 22:34:27] [Epoch=040/100] [Need: 02:49:02] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [040][000/391]   Time 4.073 (4.073)   Data 3.664 (3.664)   Loss 0.1227 (0.1227)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 22:34:31]
  Epoch: [040][200/391]   Time 0.487 (0.397)   Data 0.000 (0.018)   Loss 0.0652 (0.0892)   Prec@1 97.656 (96.976)   Prec@5 100.000 (99.977)   [2019-08-30 22:35:46]
  **Test** Prec@1 90.810 Prec@5 99.610 Error@1 9.190

==>>[2019-08-30 22:37:17] [Epoch=041/100] [Need: 02:46:16] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [041][000/391]   Time 3.743 (3.743)   Data 3.298 (3.298)   Loss 0.0645 (0.0645)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 22:37:21]
  Epoch: [041][200/391]   Time 0.363 (0.404)   Data 0.000 (0.017)   Loss 0.0903 (0.0883)   Prec@1 98.438 (97.027)   Prec@5 100.000 (99.988)   [2019-08-30 22:38:38]
  **Test** Prec@1 90.330 Prec@5 99.640 Error@1 9.670

==>>[2019-08-30 22:40:10] [Epoch=042/100] [Need: 02:43:32] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [042][000/391]   Time 4.078 (4.078)   Data 3.614 (3.614)   Loss 0.1565 (0.1565)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 22:40:14]
  Epoch: [042][200/391]   Time 0.414 (0.397)   Data 0.001 (0.018)   Loss 0.1067 (0.0888)   Prec@1 96.094 (97.015)   Prec@5 100.000 (99.984)   [2019-08-30 22:41:30]
  **Test** Prec@1 90.770 Prec@5 99.720 Error@1 9.230

==>>[2019-08-30 22:42:58] [Epoch=043/100] [Need: 02:40:42] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [043][000/391]   Time 3.488 (3.488)   Data 3.076 (3.076)   Loss 0.1234 (0.1234)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 22:43:02]
  Epoch: [043][200/391]   Time 0.271 (0.398)   Data 0.000 (0.016)   Loss 0.0545 (0.0883)   Prec@1 97.656 (96.941)   Prec@5 100.000 (99.984)   [2019-08-30 22:44:18]
  **Test** Prec@1 90.760 Prec@5 99.730 Error@1 9.240

==>>[2019-08-30 22:45:50] [Epoch=044/100] [Need: 02:37:57] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [044][000/391]   Time 4.174 (4.174)   Data 3.795 (3.795)   Loss 0.1069 (0.1069)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 22:45:55]
  Epoch: [044][200/391]   Time 0.345 (0.404)   Data 0.000 (0.019)   Loss 0.0486 (0.0877)   Prec@1 98.438 (97.019)   Prec@5 100.000 (99.988)   [2019-08-30 22:47:12]
  **Test** Prec@1 90.610 Prec@5 99.700 Error@1 9.390

==>>[2019-08-30 22:48:42] [Epoch=045/100] [Need: 02:35:09] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [045][000/391]   Time 5.323 (5.323)   Data 4.839 (4.839)   Loss 0.0722 (0.0722)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 22:48:47]
  Epoch: [045][200/391]   Time 0.346 (0.397)   Data 0.000 (0.024)   Loss 0.0937 (0.0900)   Prec@1 96.875 (96.910)   Prec@5 100.000 (99.988)   [2019-08-30 22:50:01]
  **Test** Prec@1 90.570 Prec@5 99.650 Error@1 9.430

==>>[2019-08-30 22:51:34] [Epoch=046/100] [Need: 02:32:23] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [046][000/391]   Time 4.840 (4.840)   Data 4.533 (4.533)   Loss 0.0985 (0.0985)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 22:51:39]
  Epoch: [046][200/391]   Time 0.256 (0.406)   Data 0.000 (0.023)   Loss 0.1190 (0.0898)   Prec@1 95.312 (96.828)   Prec@5 100.000 (99.984)   [2019-08-30 22:52:56]
  **Test** Prec@1 90.550 Prec@5 99.660 Error@1 9.450

==>>[2019-08-30 22:54:27] [Epoch=047/100] [Need: 02:29:39] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [047][000/391]   Time 4.079 (4.079)   Data 3.618 (3.618)   Loss 0.0794 (0.0794)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 22:54:31]
  Epoch: [047][200/391]   Time 0.292 (0.408)   Data 0.000 (0.018)   Loss 0.0893 (0.0881)   Prec@1 97.656 (97.073)   Prec@5 99.219 (99.996)   [2019-08-30 22:55:49]
  **Test** Prec@1 90.620 Prec@5 99.560 Error@1 9.380

==>>[2019-08-30 22:57:21] [Epoch=048/100] [Need: 02:26:55] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [048][000/391]   Time 5.094 (5.094)   Data 4.515 (4.515)   Loss 0.0893 (0.0893)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 22:57:26]
  Epoch: [048][200/391]   Time 0.397 (0.403)   Data 0.000 (0.023)   Loss 0.0630 (0.0852)   Prec@1 97.656 (97.062)   Prec@5 100.000 (99.973)   [2019-08-30 22:58:42]
  **Test** Prec@1 90.400 Prec@5 99.620 Error@1 9.600

==>>[2019-08-30 23:00:13] [Epoch=049/100] [Need: 02:24:07] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [049][000/391]   Time 4.069 (4.069)   Data 3.461 (3.461)   Loss 0.0693 (0.0693)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 23:00:17]
  Epoch: [049][200/391]   Time 0.488 (0.402)   Data 0.000 (0.018)   Loss 0.1726 (0.0873)   Prec@1 95.312 (97.038)   Prec@5 100.000 (99.988)   [2019-08-30 23:01:34]
  **Test** Prec@1 90.520 Prec@5 99.650 Error@1 9.480

==>>[2019-08-30 23:03:05] [Epoch=050/100] [Need: 02:21:20] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [050][000/391]   Time 5.356 (5.356)   Data 4.768 (4.768)   Loss 0.0793 (0.0793)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 23:03:10]
  Epoch: [050][200/391]   Time 0.370 (0.409)   Data 0.000 (0.024)   Loss 0.0942 (0.0858)   Prec@1 97.656 (97.108)   Prec@5 100.000 (99.988)   [2019-08-30 23:04:27]
  **Test** Prec@1 90.560 Prec@5 99.650 Error@1 9.440

==>>[2019-08-30 23:05:58] [Epoch=051/100] [Need: 02:18:34] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [051][000/391]   Time 2.835 (2.835)   Data 2.413 (2.413)   Loss 0.0889 (0.0889)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:06:01]
  Epoch: [051][200/391]   Time 0.509 (0.388)   Data 0.000 (0.012)   Loss 0.0921 (0.0831)   Prec@1 96.875 (97.287)   Prec@5 100.000 (99.992)   [2019-08-30 23:07:16]
  **Test** Prec@1 90.150 Prec@5 99.700 Error@1 9.850

==>>[2019-08-30 23:08:47] [Epoch=052/100] [Need: 02:15:43] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [052][000/391]   Time 4.338 (4.338)   Data 3.918 (3.918)   Loss 0.0745 (0.0745)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:08:51]
  Epoch: [052][200/391]   Time 0.305 (0.398)   Data 0.000 (0.020)   Loss 0.1388 (0.0873)   Prec@1 94.531 (96.999)   Prec@5 100.000 (99.988)   [2019-08-30 23:10:07]
  **Test** Prec@1 90.370 Prec@5 99.650 Error@1 9.630

==>>[2019-08-30 23:11:40] [Epoch=053/100] [Need: 02:12:57] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [053][000/391]   Time 2.894 (2.894)   Data 2.533 (2.533)   Loss 0.0906 (0.0906)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 23:11:43]
  Epoch: [053][200/391]   Time 0.376 (0.396)   Data 0.000 (0.013)   Loss 0.0454 (0.0843)   Prec@1 98.438 (97.163)   Prec@5 100.000 (99.984)   [2019-08-30 23:13:00]
  **Test** Prec@1 90.090 Prec@5 99.650 Error@1 9.910

==>>[2019-08-30 23:14:31] [Epoch=054/100] [Need: 02:10:09] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [054][000/391]   Time 4.121 (4.121)   Data 3.756 (3.756)   Loss 0.0728 (0.0728)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 23:14:36]
  Epoch: [054][200/391]   Time 0.512 (0.413)   Data 0.000 (0.019)   Loss 0.0658 (0.0851)   Prec@1 97.656 (97.151)   Prec@5 100.000 (99.996)   [2019-08-30 23:15:54]
  **Test** Prec@1 89.870 Prec@5 99.630 Error@1 10.130

==>>[2019-08-30 23:17:26] [Epoch=055/100] [Need: 02:07:23] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [055][000/391]   Time 3.956 (3.956)   Data 3.591 (3.591)   Loss 0.1452 (0.1452)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 23:17:30]
  Epoch: [055][200/391]   Time 0.325 (0.402)   Data 0.000 (0.018)   Loss 0.0445 (0.0884)   Prec@1 98.438 (97.042)   Prec@5 100.000 (99.977)   [2019-08-30 23:18:47]
  **Test** Prec@1 89.900 Prec@5 99.650 Error@1 10.100

==>>[2019-08-30 23:20:21] [Epoch=056/100] [Need: 02:04:37] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [056][000/391]   Time 3.518 (3.518)   Data 3.065 (3.065)   Loss 0.0808 (0.0808)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:20:25]
  Epoch: [056][200/391]   Time 0.536 (0.411)   Data 0.002 (0.016)   Loss 0.1559 (0.0820)   Prec@1 96.094 (97.209)   Prec@5 100.000 (99.984)   [2019-08-30 23:21:44]
  **Test** Prec@1 90.560 Prec@5 99.730 Error@1 9.440

==>>[2019-08-30 23:23:16] [Epoch=057/100] [Need: 02:01:51] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [057][000/391]   Time 4.250 (4.250)   Data 3.817 (3.817)   Loss 0.0819 (0.0819)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 23:23:21]
  Epoch: [057][200/391]   Time 0.292 (0.394)   Data 0.000 (0.019)   Loss 0.0679 (0.0843)   Prec@1 98.438 (97.135)   Prec@5 100.000 (99.977)   [2019-08-30 23:24:35]
  **Test** Prec@1 90.650 Prec@5 99.710 Error@1 9.350

==>>[2019-08-30 23:26:05] [Epoch=058/100] [Need: 01:59:00] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [058][000/391]   Time 4.245 (4.245)   Data 3.679 (3.679)   Loss 0.0621 (0.0621)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 23:26:09]
  Epoch: [058][200/391]   Time 0.440 (0.394)   Data 0.000 (0.019)   Loss 0.1055 (0.0853)   Prec@1 96.094 (97.100)   Prec@5 100.000 (99.996)   [2019-08-30 23:27:24]
  **Test** Prec@1 89.710 Prec@5 99.650 Error@1 10.290

==>>[2019-08-30 23:28:57] [Epoch=059/100] [Need: 01:56:11] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [059][000/391]   Time 4.229 (4.229)   Data 3.895 (3.895)   Loss 0.1115 (0.1115)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 23:29:01]
  Epoch: [059][200/391]   Time 0.202 (0.412)   Data 0.000 (0.020)   Loss 0.1153 (0.0869)   Prec@1 94.531 (97.058)   Prec@5 100.000 (99.981)   [2019-08-30 23:30:20]
  **Test** Prec@1 90.290 Prec@5 99.760 Error@1 9.710

==>>[2019-08-30 23:31:52] [Epoch=060/100] [Need: 01:53:25] [learning_rate=0.0001] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [060][000/391]   Time 3.910 (3.910)   Data 3.489 (3.489)   Loss 0.0579 (0.0579)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:31:56]
  Epoch: [060][200/391]   Time 0.411 (0.398)   Data 0.000 (0.018)   Loss 0.0535 (0.0825)   Prec@1 99.219 (97.221)   Prec@5 100.000 (99.984)   [2019-08-30 23:33:12]
  **Test** Prec@1 91.160 Prec@5 99.700 Error@1 8.840

==>>[2019-08-30 23:34:42] [Epoch=061/100] [Need: 01:50:35] [learning_rate=0.0001] [Best : Accuracy=91.16, Error=8.84]
  Epoch: [061][000/391]   Time 4.560 (4.560)   Data 4.163 (4.163)   Loss 0.0342 (0.0342)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 23:34:47]
  Epoch: [061][200/391]   Time 0.321 (0.394)   Data 0.000 (0.021)   Loss 0.0789 (0.0759)   Prec@1 98.438 (97.509)   Prec@5 100.000 (99.996)   [2019-08-30 23:36:02]
  **Test** Prec@1 91.210 Prec@5 99.710 Error@1 8.790

==>>[2019-08-30 23:37:34] [Epoch=062/100] [Need: 01:47:45] [learning_rate=0.0001] [Best : Accuracy=91.21, Error=8.79]
  Epoch: [062][000/391]   Time 4.891 (4.891)   Data 4.426 (4.426)   Loss 0.0878 (0.0878)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 23:37:39]
  Epoch: [062][200/391]   Time 0.487 (0.405)   Data 0.000 (0.022)   Loss 0.0368 (0.0732)   Prec@1 100.000 (97.641)   Prec@5 100.000 (99.992)   [2019-08-30 23:38:55]
  **Test** Prec@1 90.930 Prec@5 99.690 Error@1 9.070

==>>[2019-08-30 23:40:28] [Epoch=063/100] [Need: 01:44:57] [learning_rate=0.0001] [Best : Accuracy=91.21, Error=8.79]
  Epoch: [063][000/391]   Time 4.077 (4.077)   Data 3.522 (3.522)   Loss 0.0485 (0.0485)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 23:40:32]
  Epoch: [063][200/391]   Time 0.509 (0.395)   Data 0.000 (0.018)   Loss 0.0679 (0.0711)   Prec@1 97.656 (97.648)   Prec@5 100.000 (99.988)   [2019-08-30 23:41:47]
  **Test** Prec@1 91.060 Prec@5 99.730 Error@1 8.940

==>>[2019-08-30 23:43:18] [Epoch=064/100] [Need: 01:42:07] [learning_rate=0.0001] [Best : Accuracy=91.21, Error=8.79]
  Epoch: [064][000/391]   Time 4.601 (4.601)   Data 4.247 (4.247)   Loss 0.0608 (0.0608)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 23:43:23]
  Epoch: [064][200/391]   Time 0.471 (0.400)   Data 0.000 (0.021)   Loss 0.1044 (0.0709)   Prec@1 96.875 (97.648)   Prec@5 100.000 (99.988)   [2019-08-30 23:44:39]
  **Test** Prec@1 91.170 Prec@5 99.730 Error@1 8.830

==>>[2019-08-30 23:46:11] [Epoch=065/100] [Need: 01:39:19] [learning_rate=0.0001] [Best : Accuracy=91.21, Error=8.79]
  Epoch: [065][000/391]   Time 3.866 (3.866)   Data 3.349 (3.349)   Loss 0.0328 (0.0328)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2019-08-30 23:46:15]
  Epoch: [065][200/391]   Time 0.199 (0.402)   Data 0.000 (0.017)   Loss 0.0604 (0.0702)   Prec@1 96.875 (97.676)   Prec@5 100.000 (99.984)   [2019-08-30 23:47:32]
  **Test** Prec@1 91.030 Prec@5 99.680 Error@1 8.970

==>>[2019-08-30 23:49:03] [Epoch=066/100] [Need: 01:36:29] [learning_rate=0.0001] [Best : Accuracy=91.21, Error=8.79]
  Epoch: [066][000/391]   Time 4.862 (4.862)   Data 4.194 (4.194)   Loss 0.0709 (0.0709)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 23:49:08]
  Epoch: [066][200/391]   Time 0.356 (0.400)   Data 0.000 (0.021)   Loss 0.1264 (0.0732)   Prec@1 95.312 (97.489)   Prec@5 100.000 (99.988)   [2019-08-30 23:50:24]
  **Test** Prec@1 91.160 Prec@5 99.700 Error@1 8.840

==>>[2019-08-30 23:51:53] [Epoch=067/100] [Need: 01:33:38] [learning_rate=0.0001] [Best : Accuracy=91.21, Error=8.79]
  Epoch: [067][000/391]   Time 5.120 (5.120)   Data 4.635 (4.635)   Loss 0.0376 (0.0376)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 23:51:58]
  Epoch: [067][200/391]   Time 0.254 (0.411)   Data 0.000 (0.023)   Loss 0.1010 (0.0722)   Prec@1 96.094 (97.613)   Prec@5 100.000 (99.992)   [2019-08-30 23:53:15]
  **Test** Prec@1 91.060 Prec@5 99.720 Error@1 8.940

==>>[2019-08-30 23:54:48] [Epoch=068/100] [Need: 01:30:51] [learning_rate=0.0001] [Best : Accuracy=91.21, Error=8.79]
  Epoch: [068][000/391]   Time 5.183 (5.183)   Data 4.807 (4.807)   Loss 0.0795 (0.0795)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 23:54:54]
  Epoch: [068][200/391]   Time 0.604 (0.396)   Data 0.000 (0.024)   Loss 0.0695 (0.0711)   Prec@1 97.656 (97.699)   Prec@5 100.000 (99.988)   [2019-08-30 23:56:08]
  **Test** Prec@1 90.650 Prec@5 99.760 Error@1 9.350

==>>[2019-08-30 23:57:43] [Epoch=069/100] [Need: 01:28:02] [learning_rate=0.0001] [Best : Accuracy=91.21, Error=8.79]
  Epoch: [069][000/391]   Time 4.976 (4.976)   Data 4.471 (4.471)   Loss 0.0886 (0.0886)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:57:48]
  Epoch: [069][200/391]   Time 0.359 (0.383)   Data 0.000 (0.023)   Loss 0.0567 (0.0694)   Prec@1 99.219 (97.827)   Prec@5 100.000 (99.988)   [2019-08-30 23:59:00]
  **Test** Prec@1 91.240 Prec@5 99.740 Error@1 8.760

==>>[2019-08-31 00:00:33] [Epoch=070/100] [Need: 01:25:12] [learning_rate=0.0001] [Best : Accuracy=91.24, Error=8.76]
  Epoch: [070][000/391]   Time 3.977 (3.977)   Data 3.414 (3.414)   Loss 0.1073 (0.1073)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 00:00:37]
  Epoch: [070][200/391]   Time 0.304 (0.387)   Data 0.000 (0.017)   Loss 0.1043 (0.0729)   Prec@1 96.875 (97.656)   Prec@5 100.000 (99.988)   [2019-08-31 00:01:51]
  **Test** Prec@1 90.980 Prec@5 99.710 Error@1 9.020

==>>[2019-08-31 00:03:21] [Epoch=071/100] [Need: 01:22:20] [learning_rate=0.0001] [Best : Accuracy=91.24, Error=8.76]
  Epoch: [071][000/391]   Time 4.642 (4.642)   Data 4.250 (4.250)   Loss 0.0224 (0.0224)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2019-08-31 00:03:25]
  Epoch: [071][200/391]   Time 0.420 (0.387)   Data 0.000 (0.021)   Loss 0.1246 (0.0713)   Prec@1 96.094 (97.680)   Prec@5 100.000 (99.992)   [2019-08-31 00:04:39]
  **Test** Prec@1 91.320 Prec@5 99.740 Error@1 8.680

==>>[2019-08-31 00:06:12] [Epoch=072/100] [Need: 01:19:30] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [072][000/391]   Time 3.854 (3.854)   Data 3.437 (3.437)   Loss 0.0528 (0.0528)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:06:16]
  Epoch: [072][200/391]   Time 0.418 (0.402)   Data 0.000 (0.018)   Loss 0.0792 (0.0699)   Prec@1 97.656 (97.777)   Prec@5 100.000 (99.992)   [2019-08-31 00:07:33]
  **Test** Prec@1 91.020 Prec@5 99.740 Error@1 8.980

==>>[2019-08-31 00:09:06] [Epoch=073/100] [Need: 01:16:41] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [073][000/391]   Time 4.876 (4.876)   Data 4.395 (4.395)   Loss 0.0522 (0.0522)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:09:11]
  Epoch: [073][200/391]   Time 0.323 (0.402)   Data 0.000 (0.022)   Loss 0.0533 (0.0706)   Prec@1 98.438 (97.699)   Prec@5 100.000 (99.988)   [2019-08-31 00:10:27]
  **Test** Prec@1 90.960 Prec@5 99.730 Error@1 9.040

==>>[2019-08-31 00:11:58] [Epoch=074/100] [Need: 01:13:52] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [074][000/391]   Time 4.667 (4.667)   Data 4.087 (4.087)   Loss 0.0620 (0.0620)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 00:12:03]
  Epoch: [074][200/391]   Time 0.445 (0.396)   Data 0.000 (0.021)   Loss 0.1200 (0.0654)   Prec@1 96.875 (97.851)   Prec@5 100.000 (99.988)   [2019-08-31 00:13:18]
  **Test** Prec@1 90.780 Prec@5 99.730 Error@1 9.220

==>>[2019-08-31 00:14:51] [Epoch=075/100] [Need: 01:11:02] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [075][000/391]   Time 3.941 (3.941)   Data 3.396 (3.396)   Loss 0.0617 (0.0617)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 00:14:55]
  Epoch: [075][200/391]   Time 0.278 (0.375)   Data 0.000 (0.017)   Loss 0.0332 (0.0690)   Prec@1 99.219 (97.773)   Prec@5 100.000 (99.988)   [2019-08-31 00:16:06]
  **Test** Prec@1 90.850 Prec@5 99.660 Error@1 9.150

==>>[2019-08-31 00:17:37] [Epoch=076/100] [Need: 01:08:10] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [076][000/391]   Time 4.292 (4.292)   Data 3.904 (3.904)   Loss 0.1084 (0.1084)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 00:17:42]
  Epoch: [076][200/391]   Time 0.443 (0.407)   Data 0.000 (0.020)   Loss 0.0512 (0.0680)   Prec@1 98.438 (97.781)   Prec@5 100.000 (99.992)   [2019-08-31 00:18:59]
  **Test** Prec@1 91.070 Prec@5 99.780 Error@1 8.930

==>>[2019-08-31 00:20:30] [Epoch=077/100] [Need: 01:05:20] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [077][000/391]   Time 4.697 (4.697)   Data 4.249 (4.249)   Loss 0.0421 (0.0421)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:20:35]
  Epoch: [077][200/391]   Time 0.403 (0.394)   Data 0.000 (0.021)   Loss 0.0835 (0.0689)   Prec@1 96.875 (97.703)   Prec@5 100.000 (99.996)   [2019-08-31 00:21:49]
  **Test** Prec@1 91.190 Prec@5 99.670 Error@1 8.810

==>>[2019-08-31 00:23:25] [Epoch=078/100] [Need: 01:02:31] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [078][000/391]   Time 4.309 (4.309)   Data 3.978 (3.978)   Loss 0.0592 (0.0592)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:23:29]
  Epoch: [078][200/391]   Time 0.501 (0.398)   Data 0.000 (0.020)   Loss 0.0420 (0.0704)   Prec@1 97.656 (97.645)   Prec@5 100.000 (99.992)   [2019-08-31 00:24:45]
  **Test** Prec@1 90.910 Prec@5 99.680 Error@1 9.090

==>>[2019-08-31 00:26:14] [Epoch=079/100] [Need: 00:59:40] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [079][000/391]   Time 4.480 (4.480)   Data 3.987 (3.987)   Loss 0.0775 (0.0775)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 00:26:19]
  Epoch: [079][200/391]   Time 0.523 (0.387)   Data 0.000 (0.020)   Loss 0.0900 (0.0691)   Prec@1 98.438 (97.742)   Prec@5 100.000 (99.996)   [2019-08-31 00:27:32]
  **Test** Prec@1 91.030 Prec@5 99.710 Error@1 8.970

==>>[2019-08-31 00:29:08] [Epoch=080/100] [Need: 00:56:50] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [080][000/391]   Time 4.348 (4.348)   Data 3.881 (3.881)   Loss 0.0434 (0.0434)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2019-08-31 00:29:12]
  Epoch: [080][200/391]   Time 0.291 (0.390)   Data 0.000 (0.020)   Loss 0.0603 (0.0695)   Prec@1 97.656 (97.738)   Prec@5 100.000 (99.996)   [2019-08-31 00:30:26]
  **Test** Prec@1 91.220 Prec@5 99.720 Error@1 8.780

==>>[2019-08-31 00:32:00] [Epoch=081/100] [Need: 00:54:00] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [081][000/391]   Time 4.862 (4.862)   Data 4.361 (4.361)   Loss 0.0494 (0.0494)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 00:32:04]
  Epoch: [081][200/391]   Time 0.315 (0.389)   Data 0.000 (0.023)   Loss 0.0777 (0.0729)   Prec@1 96.875 (97.606)   Prec@5 100.000 (99.981)   [2019-08-31 00:33:18]
  **Test** Prec@1 90.660 Prec@5 99.660 Error@1 9.340

==>>[2019-08-31 00:34:54] [Epoch=082/100] [Need: 00:51:10] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [082][000/391]   Time 3.434 (3.434)   Data 3.009 (3.009)   Loss 0.0314 (0.0314)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:34:57]
  Epoch: [082][200/391]   Time 0.353 (0.390)   Data 0.000 (0.015)   Loss 0.0693 (0.0700)   Prec@1 97.656 (97.613)   Prec@5 100.000 (100.000)   [2019-08-31 00:36:12]
  **Test** Prec@1 90.960 Prec@5 99.650 Error@1 9.040

==>>[2019-08-31 00:37:43] [Epoch=083/100] [Need: 00:48:19] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [083][000/391]   Time 5.058 (5.058)   Data 4.514 (4.514)   Loss 0.0738 (0.0738)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 00:37:48]
  Epoch: [083][200/391]   Time 0.512 (0.410)   Data 0.000 (0.023)   Loss 0.0588 (0.0653)   Prec@1 98.438 (97.913)   Prec@5 100.000 (99.996)   [2019-08-31 00:39:05]
  **Test** Prec@1 90.800 Prec@5 99.710 Error@1 9.200

==>>[2019-08-31 00:40:35] [Epoch=084/100] [Need: 00:45:29] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [084][000/391]   Time 4.994 (4.994)   Data 4.500 (4.500)   Loss 0.1077 (0.1077)   Prec@1 96.875 (96.875)   Prec@5 99.219 (99.219)   [2019-08-31 00:40:40]
  Epoch: [084][200/391]   Time 0.465 (0.398)   Data 0.000 (0.023)   Loss 0.0374 (0.0716)   Prec@1 100.000 (97.544)   Prec@5 100.000 (99.996)   [2019-08-31 00:41:55]
  **Test** Prec@1 91.070 Prec@5 99.730 Error@1 8.930

==>>[2019-08-31 00:43:27] [Epoch=085/100] [Need: 00:42:39] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [085][000/391]   Time 5.092 (5.092)   Data 4.563 (4.563)   Loss 0.0382 (0.0382)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:43:32]
  Epoch: [085][200/391]   Time 0.358 (0.400)   Data 0.000 (0.023)   Loss 0.0595 (0.0702)   Prec@1 97.656 (97.757)   Prec@5 100.000 (99.984)   [2019-08-31 00:44:47]
  **Test** Prec@1 90.470 Prec@5 99.680 Error@1 9.530

==>>[2019-08-31 00:46:16] [Epoch=086/100] [Need: 00:39:48] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [086][000/391]   Time 4.213 (4.213)   Data 3.643 (3.643)   Loss 0.0618 (0.0618)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 00:46:20]
  Epoch: [086][200/391]   Time 0.439 (0.391)   Data 0.000 (0.018)   Loss 0.0582 (0.0706)   Prec@1 98.438 (97.691)   Prec@5 100.000 (99.988)   [2019-08-31 00:47:35]
  **Test** Prec@1 90.530 Prec@5 99.700 Error@1 9.470

==>>[2019-08-31 00:49:10] [Epoch=087/100] [Need: 00:36:58] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [087][000/391]   Time 4.987 (4.987)   Data 4.446 (4.446)   Loss 0.0677 (0.0677)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:49:15]
  Epoch: [087][200/391]   Time 0.382 (0.396)   Data 0.000 (0.022)   Loss 0.0675 (0.0701)   Prec@1 96.875 (97.769)   Prec@5 100.000 (99.996)   [2019-08-31 00:50:30]
  **Test** Prec@1 91.140 Prec@5 99.650 Error@1 8.860

==>>[2019-08-31 00:52:02] [Epoch=088/100] [Need: 00:34:07] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [088][000/391]   Time 4.327 (4.327)   Data 3.854 (3.854)   Loss 0.0387 (0.0387)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:52:06]
  Epoch: [088][200/391]   Time 0.710 (0.384)   Data 0.000 (0.019)   Loss 0.0580 (0.0706)   Prec@1 97.656 (97.777)   Prec@5 100.000 (99.977)   [2019-08-31 00:53:19]
  **Test** Prec@1 90.920 Prec@5 99.730 Error@1 9.080

==>>[2019-08-31 00:54:52] [Epoch=089/100] [Need: 00:31:17] [learning_rate=0.0001] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [089][000/391]   Time 3.817 (3.817)   Data 3.217 (3.217)   Loss 0.0359 (0.0359)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2019-08-31 00:54:55]
  Epoch: [089][200/391]   Time 0.342 (0.387)   Data 0.000 (0.016)   Loss 0.1030 (0.0683)   Prec@1 93.750 (97.711)   Prec@5 100.000 (99.988)   [2019-08-31 00:56:10]
  **Test** Prec@1 90.870 Prec@5 99.720 Error@1 9.130

==>>[2019-08-31 00:57:39] [Epoch=090/100] [Need: 00:28:26] [learning_rate=0.0000] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [090][000/391]   Time 4.927 (4.927)   Data 4.518 (4.518)   Loss 0.0429 (0.0429)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2019-08-31 00:57:44]
  Epoch: [090][200/391]   Time 0.378 (0.399)   Data 0.000 (0.023)   Loss 0.0495 (0.0668)   Prec@1 97.656 (97.816)   Prec@5 100.000 (99.992)   [2019-08-31 00:59:00]
  **Test** Prec@1 91.130 Prec@5 99.720 Error@1 8.870

==>>[2019-08-31 01:00:32] [Epoch=091/100] [Need: 00:25:35] [learning_rate=0.0000] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [091][000/391]   Time 4.751 (4.751)   Data 4.309 (4.309)   Loss 0.0940 (0.0940)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 01:00:37]
  Epoch: [091][200/391]   Time 0.511 (0.395)   Data 0.000 (0.022)   Loss 0.0496 (0.0651)   Prec@1 97.656 (97.905)   Prec@5 100.000 (100.000)   [2019-08-31 01:01:51]
  **Test** Prec@1 91.200 Prec@5 99.710 Error@1 8.800

==>>[2019-08-31 01:03:23] [Epoch=092/100] [Need: 00:22:45] [learning_rate=0.0000] [Best : Accuracy=91.32, Error=8.68]
  Epoch: [092][000/391]   Time 4.487 (4.487)   Data 4.010 (4.010)   Loss 0.0239 (0.0239)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 01:03:27]
  Epoch: [092][200/391]   Time 0.280 (0.396)   Data 0.000 (0.021)   Loss 0.1078 (0.0627)   Prec@1 96.094 (97.983)   Prec@5 100.000 (99.992)   [2019-08-31 01:04:42]
  **Test** Prec@1 91.330 Prec@5 99.730 Error@1 8.670

==>>[2019-08-31 01:06:07] [Epoch=093/100] [Need: 00:19:53] [learning_rate=0.0000] [Best : Accuracy=91.33, Error=8.67]
  Epoch: [093][000/391]   Time 4.971 (4.971)   Data 4.460 (4.460)   Loss 0.0802 (0.0802)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 01:06:12]
  Epoch: [093][200/391]   Time 0.359 (0.372)   Data 0.000 (0.023)   Loss 0.0458 (0.0676)   Prec@1 98.438 (97.785)   Prec@5 100.000 (99.992)   [2019-08-31 01:07:22]
  **Test** Prec@1 91.430 Prec@5 99.740 Error@1 8.570

==>>[2019-08-31 01:08:45] [Epoch=094/100] [Need: 00:17:02] [learning_rate=0.0000] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [094][000/391]   Time 4.615 (4.615)   Data 4.194 (4.194)   Loss 0.0247 (0.0247)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2019-08-31 01:08:50]
  Epoch: [094][200/391]   Time 0.305 (0.375)   Data 0.000 (0.021)   Loss 0.0735 (0.0645)   Prec@1 98.438 (97.963)   Prec@5 100.000 (99.992)   [2019-08-31 01:10:01]
  **Test** Prec@1 91.200 Prec@5 99.720 Error@1 8.800

==>>[2019-08-31 01:11:24] [Epoch=095/100] [Need: 00:14:11] [learning_rate=0.0000] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [095][000/391]   Time 4.243 (4.243)   Data 3.705 (3.705)   Loss 0.0870 (0.0870)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 01:11:28]
  Epoch: [095][200/391]   Time 0.208 (0.375)   Data 0.000 (0.019)   Loss 0.0323 (0.0652)   Prec@1 98.438 (97.854)   Prec@5 100.000 (99.988)   [2019-08-31 01:12:40]
  **Test** Prec@1 91.120 Prec@5 99.740 Error@1 8.880

==>>[2019-08-31 01:14:01] [Epoch=096/100] [Need: 00:11:20] [learning_rate=0.0000] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [096][000/391]   Time 4.056 (4.056)   Data 3.536 (3.536)   Loss 0.0627 (0.0627)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:14:05]
  Epoch: [096][200/391]   Time 0.319 (0.377)   Data 0.000 (0.018)   Loss 0.0645 (0.0656)   Prec@1 96.094 (97.917)   Prec@5 100.000 (99.988)   [2019-08-31 01:15:17]
  **Test** Prec@1 91.210 Prec@5 99.720 Error@1 8.790

==>>[2019-08-31 01:16:34] [Epoch=097/100] [Need: 00:08:29] [learning_rate=0.0000] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [097][000/391]   Time 3.794 (3.794)   Data 3.332 (3.332)   Loss 0.0568 (0.0568)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:16:38]
  Epoch: [097][200/391]   Time 0.282 (0.344)   Data 0.000 (0.017)   Loss 0.0497 (0.0644)   Prec@1 99.219 (97.940)   Prec@5 100.000 (99.984)   [2019-08-31 01:17:43]
  **Test** Prec@1 91.020 Prec@5 99.720 Error@1 8.980

==>>[2019-08-31 01:18:53] [Epoch=098/100] [Need: 00:05:39] [learning_rate=0.0000] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [098][000/391]   Time 2.735 (2.735)   Data 2.378 (2.378)   Loss 0.0644 (0.0644)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:18:56]
  Epoch: [098][200/391]   Time 0.160 (0.297)   Data 0.000 (0.012)   Loss 0.0636 (0.0638)   Prec@1 98.438 (97.854)   Prec@5 100.000 (99.996)   [2019-08-31 01:19:53]
  **Test** Prec@1 91.410 Prec@5 99.660 Error@1 8.590

==>>[2019-08-31 01:20:49] [Epoch=099/100] [Need: 00:02:49] [learning_rate=0.0000] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [099][000/391]   Time 2.267 (2.267)   Data 1.879 (1.879)   Loss 0.1068 (0.1068)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-31 01:20:52]
  Epoch: [099][200/391]   Time 0.196 (0.206)   Data 0.000 (0.010)   Loss 0.0424 (0.0664)   Prec@1 99.219 (97.870)   Prec@5 100.000 (99.996)   [2019-08-31 01:21:31]
  **Test** Prec@1 90.800 Prec@5 99.610 Error@1 9.200
