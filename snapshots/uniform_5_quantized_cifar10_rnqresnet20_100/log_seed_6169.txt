save path : ./snapshots/uniform_5_quantized_cifar10_rnqresnet20_100
{'aq_bits': 5, 'aq_type': 'uniform', 'arch': 'rnqresnet20', 'batch_size': 128, 'data_path': '../../datasets/CIFAR10/cifar', 'dataset': 'cifar10', 'decay': 0.0001, 'epochs': 100, 'evaluate': False, 'gammas': [0.1, 0.1, 0.1], 'learning_rate': 0.01, 'manualSeed': 6169, 'momentum': 0.9, 'ngpu': 4, 'pretrained': '', 'print_freq': 200, 'resume': 'snapshots/finetune_cifar10_resnet20_300/model_best.pth.tar', 'save_path': './snapshots/uniform_5_quantized_cifar10_rnqresnet20_100', 'schedule': [30, 60, 90], 'start_epoch': 0, 'use_cuda': True, 'workers': 16}
Random Seed: 6169
python version : 3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]
torch  version : 1.1.0
cudnn  version : 7501
=> creating model 'rnqresnet20'
=> network :
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): QuantLinear(in_features=64, out_features=10, bias=True, method=KMeans, nbits=4)
)
=> loading checkpoint 'snapshots/finetune_cifar10_resnet20_300/model_best.pth.tar'
=> loaded checkpoint 'snapshots/finetune_cifar10_resnet20_300/model_best.pth.tar' (epoch 291)

==>>[2019-08-30 20:37:37] [Epoch=000/100] [Need: 00:00:00] [learning_rate=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/391]   Time 38.924 (38.924)   Data 0.963 (0.963)   Loss 0.0591 (0.0591)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 20:38:16]
  Epoch: [000][200/391]   Time 0.276 (0.483)   Data 0.000 (0.005)   Loss 0.2000 (0.1823)   Prec@1 90.625 (93.381)   Prec@5 100.000 (99.934)   [2019-08-30 20:39:14]
  **Test** Prec@1 86.360 Prec@5 99.540 Error@1 13.640

==>>[2019-08-30 20:40:33] [Epoch=001/100] [Need: 04:47:56] [learning_rate=0.0100] [Best : Accuracy=86.36, Error=13.64]
  Epoch: [001][000/391]   Time 3.266 (3.266)   Data 2.847 (2.847)   Loss 0.1771 (0.1771)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 20:40:36]
  Epoch: [001][200/391]   Time 0.287 (0.338)   Data 0.000 (0.014)   Loss 0.2954 (0.1689)   Prec@1 90.625 (93.948)   Prec@5 100.000 (99.942)   [2019-08-30 20:41:41]
  **Test** Prec@1 88.190 Prec@5 99.490 Error@1 11.810

==>>[2019-08-30 20:43:03] [Epoch=002/100] [Need: 04:25:45] [learning_rate=0.0100] [Best : Accuracy=88.19, Error=11.81]
  Epoch: [002][000/391]   Time 3.960 (3.960)   Data 3.598 (3.598)   Loss 0.1227 (0.1227)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 20:43:07]
  Epoch: [002][200/391]   Time 0.322 (0.365)   Data 0.000 (0.018)   Loss 0.1295 (0.1608)   Prec@1 94.531 (94.244)   Prec@5 100.000 (99.930)   [2019-08-30 20:44:17]
  **Test** Prec@1 87.870 Prec@5 99.380 Error@1 12.130

==>>[2019-08-30 20:45:40] [Epoch=003/100] [Need: 04:20:09] [learning_rate=0.0100] [Best : Accuracy=88.19, Error=11.81]
  Epoch: [003][000/391]   Time 2.874 (2.874)   Data 2.482 (2.482)   Loss 0.1992 (0.1992)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 20:45:43]
  Epoch: [003][200/391]   Time 0.274 (0.374)   Data 0.000 (0.013)   Loss 0.1271 (0.1661)   Prec@1 96.094 (94.100)   Prec@5 100.000 (99.930)   [2019-08-30 20:46:56]
  **Test** Prec@1 83.720 Prec@5 99.110 Error@1 16.280

==>>[2019-08-30 20:48:22] [Epoch=004/100] [Need: 04:17:35] [learning_rate=0.0100] [Best : Accuracy=88.19, Error=11.81]
  Epoch: [004][000/391]   Time 2.731 (2.731)   Data 2.474 (2.474)   Loss 0.1262 (0.1262)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 20:48:25]
  Epoch: [004][200/391]   Time 0.434 (0.362)   Data 0.000 (0.013)   Loss 0.2173 (0.1600)   Prec@1 90.625 (94.314)   Prec@5 100.000 (99.918)   [2019-08-30 20:49:35]
  **Test** Prec@1 88.140 Prec@5 99.520 Error@1 11.860

==>>[2019-08-30 20:51:03] [Epoch=005/100] [Need: 04:14:51] [learning_rate=0.0100] [Best : Accuracy=88.19, Error=11.81]
  Epoch: [005][000/391]   Time 3.562 (3.562)   Data 3.040 (3.040)   Loss 0.1439 (0.1439)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 20:51:06]
  Epoch: [005][200/391]   Time 0.192 (0.364)   Data 0.000 (0.015)   Loss 0.1842 (0.1651)   Prec@1 93.750 (94.146)   Prec@5 99.219 (99.926)   [2019-08-30 20:52:16]
  **Test** Prec@1 87.270 Prec@5 99.390 Error@1 12.730

==>>[2019-08-30 20:53:49] [Epoch=006/100] [Need: 04:13:31] [learning_rate=0.0100] [Best : Accuracy=88.19, Error=11.81]
  Epoch: [006][000/391]   Time 3.024 (3.024)   Data 2.541 (2.541)   Loss 0.1039 (0.1039)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 20:53:52]
  Epoch: [006][200/391]   Time 0.289 (0.384)   Data 0.000 (0.013)   Loss 0.1155 (0.1614)   Prec@1 96.094 (94.325)   Prec@5 100.000 (99.942)   [2019-08-30 20:55:06]
  **Test** Prec@1 87.450 Prec@5 99.500 Error@1 12.550

==>>[2019-08-30 20:56:40] [Epoch=007/100] [Need: 04:12:47] [learning_rate=0.0100] [Best : Accuracy=88.19, Error=11.81]
  Epoch: [007][000/391]   Time 3.894 (3.894)   Data 3.429 (3.429)   Loss 0.1846 (0.1846)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 20:56:44]
  Epoch: [007][200/391]   Time 0.398 (0.388)   Data 0.000 (0.017)   Loss 0.1118 (0.1641)   Prec@1 95.312 (94.119)   Prec@5 100.000 (99.899)   [2019-08-30 20:57:58]
  **Test** Prec@1 86.810 Prec@5 99.360 Error@1 13.190

==>>[2019-08-30 20:59:31] [Epoch=008/100] [Need: 04:11:42] [learning_rate=0.0100] [Best : Accuracy=88.19, Error=11.81]
  Epoch: [008][000/391]   Time 3.461 (3.461)   Data 3.147 (3.147)   Loss 0.1693 (0.1693)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 20:59:35]
  Epoch: [008][200/391]   Time 0.340 (0.402)   Data 0.000 (0.016)   Loss 0.1032 (0.1715)   Prec@1 95.312 (93.975)   Prec@5 100.000 (99.938)   [2019-08-30 21:00:52]
  **Test** Prec@1 81.390 Prec@5 98.890 Error@1 18.610

==>>[2019-08-30 21:02:26] [Epoch=009/100] [Need: 04:10:44] [learning_rate=0.0100] [Best : Accuracy=88.19, Error=11.81]
  Epoch: [009][000/391]   Time 3.220 (3.220)   Data 2.791 (2.791)   Loss 0.0984 (0.0984)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:02:29]
  Epoch: [009][200/391]   Time 0.257 (0.381)   Data 0.000 (0.014)   Loss 0.2008 (0.1636)   Prec@1 94.531 (94.325)   Prec@5 99.219 (99.957)   [2019-08-30 21:03:42]
  **Test** Prec@1 88.490 Prec@5 99.630 Error@1 11.510

==>>[2019-08-30 21:05:18] [Epoch=010/100] [Need: 04:09:01] [learning_rate=0.0100] [Best : Accuracy=88.49, Error=11.51]
  Epoch: [010][000/391]   Time 3.368 (3.368)   Data 2.790 (2.790)   Loss 0.1216 (0.1216)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:05:21]
  Epoch: [010][200/391]   Time 0.386 (0.379)   Data 0.000 (0.014)   Loss 0.0916 (0.1627)   Prec@1 97.656 (94.166)   Prec@5 100.000 (99.911)   [2019-08-30 21:06:34]
  **Test** Prec@1 88.010 Prec@5 99.650 Error@1 11.990

==>>[2019-08-30 21:08:09] [Epoch=011/100] [Need: 04:06:49] [learning_rate=0.0100] [Best : Accuracy=88.49, Error=11.51]
  Epoch: [011][000/391]   Time 3.498 (3.498)   Data 3.172 (3.172)   Loss 0.0742 (0.0742)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 21:08:12]
  Epoch: [011][200/391]   Time 0.288 (0.377)   Data 0.000 (0.016)   Loss 0.1676 (0.1630)   Prec@1 96.094 (94.259)   Prec@5 100.000 (99.942)   [2019-08-30 21:09:24]
  **Test** Prec@1 86.810 Prec@5 99.360 Error@1 13.190

==>>[2019-08-30 21:11:01] [Epoch=012/100] [Need: 04:04:45] [learning_rate=0.0100] [Best : Accuracy=88.49, Error=11.51]
  Epoch: [012][000/391]   Time 3.987 (3.987)   Data 3.608 (3.608)   Loss 0.1905 (0.1905)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 21:11:05]
  Epoch: [012][200/391]   Time 0.387 (0.379)   Data 0.000 (0.018)   Loss 0.1959 (0.1610)   Prec@1 92.969 (94.434)   Prec@5 100.000 (99.899)   [2019-08-30 21:12:17]
  **Test** Prec@1 89.150 Prec@5 99.680 Error@1 10.850

==>>[2019-08-30 21:13:52] [Epoch=013/100] [Need: 04:02:29] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [013][000/391]   Time 3.074 (3.074)   Data 2.757 (2.757)   Loss 0.0967 (0.0967)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 21:13:55]
  Epoch: [013][200/391]   Time 0.275 (0.373)   Data 0.000 (0.014)   Loss 0.2094 (0.1584)   Prec@1 93.750 (94.368)   Prec@5 99.219 (99.922)   [2019-08-30 21:15:07]
  **Test** Prec@1 88.480 Prec@5 99.520 Error@1 11.520

==>>[2019-08-30 21:16:40] [Epoch=014/100] [Need: 03:59:48] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [014][000/391]   Time 4.176 (4.176)   Data 3.830 (3.830)   Loss 0.1538 (0.1538)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:16:44]
  Epoch: [014][200/391]   Time 0.491 (0.387)   Data 0.000 (0.019)   Loss 0.0513 (0.1640)   Prec@1 98.438 (94.263)   Prec@5 100.000 (99.911)   [2019-08-30 21:17:58]
  **Test** Prec@1 88.390 Prec@5 99.610 Error@1 11.610

==>>[2019-08-30 21:19:33] [Epoch=015/100] [Need: 03:57:29] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [015][000/391]   Time 3.172 (3.172)   Data 2.684 (2.684)   Loss 0.1865 (0.1865)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 21:19:36]
  Epoch: [015][200/391]   Time 0.583 (0.393)   Data 0.000 (0.014)   Loss 0.1019 (0.1608)   Prec@1 96.094 (94.376)   Prec@5 100.000 (99.907)   [2019-08-30 21:20:52]
  **Test** Prec@1 86.510 Prec@5 99.300 Error@1 13.490

==>>[2019-08-30 21:22:29] [Epoch=016/100] [Need: 03:55:27] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [016][000/391]   Time 2.952 (2.952)   Data 2.546 (2.546)   Loss 0.1453 (0.1453)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:22:31]
  Epoch: [016][200/391]   Time 0.458 (0.372)   Data 0.000 (0.013)   Loss 0.1386 (0.1649)   Prec@1 95.312 (94.185)   Prec@5 100.000 (99.926)   [2019-08-30 21:23:43]
  **Test** Prec@1 88.770 Prec@5 99.610 Error@1 11.230

==>>[2019-08-30 21:25:18] [Epoch=017/100] [Need: 03:52:44] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [017][000/391]   Time 2.908 (2.908)   Data 2.560 (2.560)   Loss 0.1537 (0.1537)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:25:21]
  Epoch: [017][200/391]   Time 0.288 (0.385)   Data 0.000 (0.013)   Loss 0.1766 (0.1648)   Prec@1 93.750 (94.119)   Prec@5 100.000 (99.926)   [2019-08-30 21:26:36]
  **Test** Prec@1 88.450 Prec@5 99.610 Error@1 11.550

==>>[2019-08-30 21:28:12] [Epoch=018/100] [Need: 03:50:24] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [018][000/391]   Time 3.878 (3.878)   Data 3.173 (3.173)   Loss 0.1663 (0.1663)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 21:28:16]
  Epoch: [018][200/391]   Time 0.239 (0.398)   Data 0.000 (0.016)   Loss 0.1220 (0.1660)   Prec@1 96.094 (94.065)   Prec@5 100.000 (99.895)   [2019-08-30 21:29:32]
  **Test** Prec@1 88.890 Prec@5 99.620 Error@1 11.110

==>>[2019-08-30 21:31:06] [Epoch=019/100] [Need: 03:47:55] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [019][000/391]   Time 4.248 (4.248)   Data 3.729 (3.729)   Loss 0.1177 (0.1177)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:31:10]
  Epoch: [019][200/391]   Time 0.347 (0.397)   Data 0.000 (0.019)   Loss 0.2321 (0.1661)   Prec@1 92.969 (94.224)   Prec@5 99.219 (99.946)   [2019-08-30 21:32:26]
  **Test** Prec@1 86.090 Prec@5 99.430 Error@1 13.910

==>>[2019-08-30 21:34:00] [Epoch=020/100] [Need: 03:45:25] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [020][000/391]   Time 4.772 (4.772)   Data 4.253 (4.253)   Loss 0.1447 (0.1447)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 21:34:05]
  Epoch: [020][200/391]   Time 0.444 (0.399)   Data 0.000 (0.021)   Loss 0.1282 (0.1601)   Prec@1 95.312 (94.360)   Prec@5 100.000 (99.934)   [2019-08-30 21:35:20]
  **Test** Prec@1 86.490 Prec@5 99.480 Error@1 13.510

==>>[2019-08-30 21:36:54] [Epoch=021/100] [Need: 03:42:57] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [021][000/391]   Time 3.934 (3.934)   Data 3.358 (3.358)   Loss 0.2024 (0.2024)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 21:36:58]
  Epoch: [021][200/391]   Time 0.305 (0.383)   Data 0.000 (0.017)   Loss 0.1846 (0.1651)   Prec@1 94.531 (94.069)   Prec@5 100.000 (99.953)   [2019-08-30 21:38:11]
  **Test** Prec@1 87.830 Prec@5 99.610 Error@1 12.170

==>>[2019-08-30 21:39:47] [Epoch=022/100] [Need: 03:40:21] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [022][000/391]   Time 3.720 (3.720)   Data 3.297 (3.297)   Loss 0.1863 (0.1863)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 21:39:51]
  Epoch: [022][200/391]   Time 0.307 (0.385)   Data 0.000 (0.017)   Loss 0.1480 (0.1623)   Prec@1 93.750 (94.329)   Prec@5 100.000 (99.930)   [2019-08-30 21:41:05]
  **Test** Prec@1 88.840 Prec@5 99.490 Error@1 11.160

==>>[2019-08-30 21:42:41] [Epoch=023/100] [Need: 03:37:45] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [023][000/391]   Time 3.383 (3.383)   Data 2.839 (2.839)   Loss 0.1446 (0.1446)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 21:42:44]
  Epoch: [023][200/391]   Time 0.382 (0.379)   Data 0.000 (0.014)   Loss 0.1569 (0.1564)   Prec@1 94.531 (94.551)   Prec@5 100.000 (99.926)   [2019-08-30 21:43:57]
  **Test** Prec@1 89.100 Prec@5 99.610 Error@1 10.900

==>>[2019-08-30 21:45:34] [Epoch=024/100] [Need: 03:35:06] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [024][000/391]   Time 3.608 (3.608)   Data 3.105 (3.105)   Loss 0.1225 (0.1225)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:45:38]
  Epoch: [024][200/391]   Time 0.419 (0.368)   Data 0.000 (0.016)   Loss 0.1093 (0.1633)   Prec@1 96.094 (94.352)   Prec@5 99.219 (99.957)   [2019-08-30 21:46:48]
  **Test** Prec@1 87.460 Prec@5 99.410 Error@1 12.540

==>>[2019-08-30 21:48:24] [Epoch=025/100] [Need: 03:32:16] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [025][000/391]   Time 4.193 (4.193)   Data 3.800 (3.800)   Loss 0.1973 (0.1973)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 21:48:28]
  Epoch: [025][200/391]   Time 0.461 (0.388)   Data 0.000 (0.019)   Loss 0.2744 (0.1607)   Prec@1 90.625 (94.255)   Prec@5 99.219 (99.926)   [2019-08-30 21:49:42]
  **Test** Prec@1 85.310 Prec@5 99.510 Error@1 14.690

==>>[2019-08-30 21:51:17] [Epoch=026/100] [Need: 03:29:36] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [026][000/391]   Time 4.194 (4.194)   Data 3.745 (3.745)   Loss 0.1320 (0.1320)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:51:21]
  Epoch: [026][200/391]   Time 0.362 (0.393)   Data 0.000 (0.019)   Loss 0.1706 (0.1668)   Prec@1 94.531 (94.185)   Prec@5 100.000 (99.891)   [2019-08-30 21:52:36]
  **Test** Prec@1 88.450 Prec@5 99.580 Error@1 11.550

==>>[2019-08-30 21:54:09] [Epoch=027/100] [Need: 03:26:53] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [027][000/391]   Time 3.650 (3.650)   Data 3.149 (3.149)   Loss 0.0846 (0.0846)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:54:13]
  Epoch: [027][200/391]   Time 0.553 (0.381)   Data 0.000 (0.016)   Loss 0.1224 (0.1611)   Prec@1 94.531 (94.302)   Prec@5 100.000 (99.930)   [2019-08-30 21:55:26]
  **Test** Prec@1 88.730 Prec@5 99.530 Error@1 11.270

==>>[2019-08-30 21:56:59] [Epoch=028/100] [Need: 03:24:02] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [028][000/391]   Time 3.372 (3.372)   Data 2.806 (2.806)   Loss 0.1445 (0.1445)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 21:57:02]
  Epoch: [028][200/391]   Time 0.438 (0.373)   Data 0.000 (0.014)   Loss 0.1781 (0.1578)   Prec@1 95.312 (94.523)   Prec@5 99.219 (99.961)   [2019-08-30 21:58:13]
  **Test** Prec@1 87.630 Prec@5 99.570 Error@1 12.370

==>>[2019-08-30 21:59:49] [Epoch=029/100] [Need: 03:21:11] [learning_rate=0.0100] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [029][000/391]   Time 3.505 (3.505)   Data 3.118 (3.118)   Loss 0.1799 (0.1799)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 21:59:52]
  Epoch: [029][200/391]   Time 0.246 (0.358)   Data 0.000 (0.016)   Loss 0.1987 (0.1628)   Prec@1 90.625 (94.232)   Prec@5 100.000 (99.922)   [2019-08-30 22:01:01]
  **Test** Prec@1 87.230 Prec@5 99.580 Error@1 12.770

==>>[2019-08-30 22:02:36] [Epoch=030/100] [Need: 03:18:15] [learning_rate=0.0010] [Best : Accuracy=89.15, Error=10.85]
  Epoch: [030][000/391]   Time 4.468 (4.468)   Data 4.143 (4.143)   Loss 0.1440 (0.1440)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 22:02:41]
  Epoch: [030][200/391]   Time 0.231 (0.381)   Data 0.000 (0.021)   Loss 0.1537 (0.1336)   Prec@1 93.750 (95.472)   Prec@5 100.000 (99.969)   [2019-08-30 22:03:53]
  **Test** Prec@1 90.780 Prec@5 99.660 Error@1 9.220

==>>[2019-08-30 22:05:25] [Epoch=031/100] [Need: 03:15:22] [learning_rate=0.0010] [Best : Accuracy=90.78, Error=9.22]
  Epoch: [031][000/391]   Time 3.285 (3.285)   Data 2.725 (2.725)   Loss 0.1107 (0.1107)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 22:05:28]
  Epoch: [031][200/391]   Time 0.256 (0.349)   Data 0.000 (0.014)   Loss 0.1611 (0.1151)   Prec@1 94.531 (96.016)   Prec@5 100.000 (99.961)   [2019-08-30 22:06:35]
  **Test** Prec@1 90.720 Prec@5 99.720 Error@1 9.280

==>>[2019-08-30 22:08:07] [Epoch=032/100] [Need: 03:12:16] [learning_rate=0.0010] [Best : Accuracy=90.78, Error=9.22]
  Epoch: [032][000/391]   Time 3.956 (3.956)   Data 3.628 (3.628)   Loss 0.1173 (0.1173)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 22:08:11]
  Epoch: [032][200/391]   Time 0.261 (0.352)   Data 0.000 (0.018)   Loss 0.0880 (0.1123)   Prec@1 97.656 (96.234)   Prec@5 100.000 (99.977)   [2019-08-30 22:09:18]
  **Test** Prec@1 91.040 Prec@5 99.710 Error@1 8.960

==>>[2019-08-30 22:10:51] [Epoch=033/100] [Need: 03:09:13] [learning_rate=0.0010] [Best : Accuracy=91.04, Error=8.96]
  Epoch: [033][000/391]   Time 3.680 (3.680)   Data 3.290 (3.290)   Loss 0.1409 (0.1409)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 22:10:54]
  Epoch: [033][200/391]   Time 0.280 (0.355)   Data 0.000 (0.017)   Loss 0.0769 (0.1042)   Prec@1 96.875 (96.451)   Prec@5 100.000 (99.981)   [2019-08-30 22:12:02]
  **Test** Prec@1 90.910 Prec@5 99.690 Error@1 9.090

==>>[2019-08-30 22:13:30] [Epoch=034/100] [Need: 03:06:07] [learning_rate=0.0010] [Best : Accuracy=91.04, Error=8.96]
  Epoch: [034][000/391]   Time 4.250 (4.250)   Data 3.851 (3.851)   Loss 0.1382 (0.1382)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 22:13:35]
  Epoch: [034][200/391]   Time 0.235 (0.351)   Data 0.000 (0.019)   Loss 0.0842 (0.1056)   Prec@1 96.875 (96.467)   Prec@5 100.000 (99.977)   [2019-08-30 22:14:41]
  **Test** Prec@1 91.170 Prec@5 99.650 Error@1 8.830

==>>[2019-08-30 22:16:12] [Epoch=035/100] [Need: 03:03:04] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [035][000/391]   Time 3.268 (3.268)   Data 2.679 (2.679)   Loss 0.1176 (0.1176)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 22:16:15]
  Epoch: [035][200/391]   Time 0.323 (0.372)   Data 0.000 (0.014)   Loss 0.0737 (0.1060)   Prec@1 98.438 (96.292)   Prec@5 100.000 (99.981)   [2019-08-30 22:17:27]
  **Test** Prec@1 90.960 Prec@5 99.750 Error@1 9.040

==>>[2019-08-30 22:19:02] [Epoch=036/100] [Need: 03:00:16] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [036][000/391]   Time 3.992 (3.992)   Data 3.464 (3.464)   Loss 0.1135 (0.1135)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 22:19:06]
  Epoch: [036][200/391]   Time 0.360 (0.371)   Data 0.000 (0.017)   Loss 0.0727 (0.1018)   Prec@1 98.438 (96.498)   Prec@5 100.000 (99.984)   [2019-08-30 22:20:17]
  **Test** Prec@1 89.050 Prec@5 99.660 Error@1 10.950

==>>[2019-08-30 22:21:52] [Epoch=037/100] [Need: 02:57:27] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [037][000/391]   Time 4.703 (4.703)   Data 4.214 (4.214)   Loss 0.1438 (0.1438)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 22:21:56]
  Epoch: [037][200/391]   Time 0.261 (0.371)   Data 0.000 (0.021)   Loss 0.1023 (0.1017)   Prec@1 96.094 (96.506)   Prec@5 100.000 (99.977)   [2019-08-30 22:23:06]
  **Test** Prec@1 90.820 Prec@5 99.690 Error@1 9.180

==>>[2019-08-30 22:24:42] [Epoch=038/100] [Need: 02:54:40] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [038][000/391]   Time 4.068 (4.068)   Data 3.530 (3.530)   Loss 0.0672 (0.0672)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 22:24:46]
  Epoch: [038][200/391]   Time 0.328 (0.372)   Data 0.000 (0.018)   Loss 0.0726 (0.0988)   Prec@1 97.656 (96.603)   Prec@5 100.000 (99.942)   [2019-08-30 22:25:57]
  **Test** Prec@1 89.720 Prec@5 99.660 Error@1 10.280

==>>[2019-08-30 22:27:34] [Epoch=039/100] [Need: 02:51:55] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [039][000/391]   Time 3.200 (3.200)   Data 2.681 (2.681)   Loss 0.0613 (0.0613)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 22:27:37]
  Epoch: [039][200/391]   Time 0.380 (0.368)   Data 0.000 (0.014)   Loss 0.0643 (0.0994)   Prec@1 98.438 (96.583)   Prec@5 100.000 (99.977)   [2019-08-30 22:28:48]
  **Test** Prec@1 90.760 Prec@5 99.700 Error@1 9.240

==>>[2019-08-30 22:30:25] [Epoch=040/100] [Need: 02:49:10] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [040][000/391]   Time 3.587 (3.587)   Data 3.114 (3.114)   Loss 0.0794 (0.0794)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 22:30:29]
  Epoch: [040][200/391]   Time 0.169 (0.376)   Data 0.000 (0.016)   Loss 0.0694 (0.1013)   Prec@1 97.656 (96.525)   Prec@5 100.000 (99.984)   [2019-08-30 22:31:41]
  **Test** Prec@1 90.920 Prec@5 99.710 Error@1 9.080

==>>[2019-08-30 22:33:17] [Epoch=041/100] [Need: 02:46:25] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [041][000/391]   Time 3.202 (3.202)   Data 2.812 (2.812)   Loss 0.1079 (0.1079)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 22:33:20]
  Epoch: [041][200/391]   Time 0.291 (0.387)   Data 0.000 (0.014)   Loss 0.0453 (0.0959)   Prec@1 99.219 (96.747)   Prec@5 100.000 (99.984)   [2019-08-30 22:34:34]
  **Test** Prec@1 90.780 Prec@5 99.630 Error@1 9.220

==>>[2019-08-30 22:36:08] [Epoch=042/100] [Need: 02:43:38] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [042][000/391]   Time 4.152 (4.152)   Data 3.679 (3.679)   Loss 0.0748 (0.0748)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 22:36:12]
  Epoch: [042][200/391]   Time 0.400 (0.386)   Data 0.000 (0.019)   Loss 0.1796 (0.0983)   Prec@1 95.312 (96.700)   Prec@5 100.000 (99.984)   [2019-08-30 22:37:26]
  **Test** Prec@1 90.710 Prec@5 99.580 Error@1 9.290

==>>[2019-08-30 22:39:00] [Epoch=043/100] [Need: 02:40:53] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [043][000/391]   Time 4.050 (4.050)   Data 3.625 (3.625)   Loss 0.0920 (0.0920)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 22:39:04]
  Epoch: [043][200/391]   Time 0.471 (0.377)   Data 0.000 (0.018)   Loss 0.0855 (0.0974)   Prec@1 98.438 (96.599)   Prec@5 100.000 (99.981)   [2019-08-30 22:40:16]
  **Test** Prec@1 90.710 Prec@5 99.780 Error@1 9.290

==>>[2019-08-30 22:41:52] [Epoch=044/100] [Need: 02:38:07] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [044][000/391]   Time 5.173 (5.173)   Data 4.588 (4.588)   Loss 0.0386 (0.0386)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 22:41:58]
  Epoch: [044][200/391]   Time 0.438 (0.376)   Data 0.000 (0.023)   Loss 0.0992 (0.0939)   Prec@1 97.656 (96.793)   Prec@5 100.000 (99.981)   [2019-08-30 22:43:08]
  **Test** Prec@1 90.570 Prec@5 99.670 Error@1 9.430

==>>[2019-08-30 22:44:45] [Epoch=045/100] [Need: 02:35:21] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [045][000/391]   Time 4.062 (4.062)   Data 3.585 (3.585)   Loss 0.0487 (0.0487)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 22:44:49]
  Epoch: [045][200/391]   Time 0.397 (0.384)   Data 0.000 (0.018)   Loss 0.0994 (0.0972)   Prec@1 95.312 (96.677)   Prec@5 100.000 (99.996)   [2019-08-30 22:46:02]
  **Test** Prec@1 90.640 Prec@5 99.670 Error@1 9.360

==>>[2019-08-30 22:47:37] [Epoch=046/100] [Need: 02:32:35] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [046][000/391]   Time 4.142 (4.142)   Data 3.582 (3.582)   Loss 0.1145 (0.1145)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 22:47:41]
  Epoch: [046][200/391]   Time 0.445 (0.378)   Data 0.000 (0.018)   Loss 0.0963 (0.0947)   Prec@1 96.875 (96.657)   Prec@5 100.000 (99.977)   [2019-08-30 22:48:53]
  **Test** Prec@1 90.640 Prec@5 99.620 Error@1 9.360

==>>[2019-08-30 22:50:27] [Epoch=047/100] [Need: 02:29:46] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [047][000/391]   Time 3.031 (3.031)   Data 2.573 (2.573)   Loss 0.1254 (0.1254)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 22:50:30]
  Epoch: [047][200/391]   Time 0.416 (0.368)   Data 0.000 (0.013)   Loss 0.0904 (0.0972)   Prec@1 97.656 (96.743)   Prec@5 100.000 (99.988)   [2019-08-30 22:51:41]
  **Test** Prec@1 90.810 Prec@5 99.660 Error@1 9.190

==>>[2019-08-30 22:53:17] [Epoch=048/100] [Need: 02:26:57] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [048][000/391]   Time 3.802 (3.802)   Data 3.351 (3.351)   Loss 0.1618 (0.1618)   Prec@1 96.094 (96.094)   Prec@5 99.219 (99.219)   [2019-08-30 22:53:21]
  Epoch: [048][200/391]   Time 0.373 (0.379)   Data 0.000 (0.017)   Loss 0.0702 (0.0980)   Prec@1 97.656 (96.716)   Prec@5 100.000 (99.965)   [2019-08-30 22:54:33]
  **Test** Prec@1 90.580 Prec@5 99.730 Error@1 9.420

==>>[2019-08-30 22:56:09] [Epoch=049/100] [Need: 02:24:10] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [049][000/391]   Time 3.579 (3.579)   Data 3.058 (3.058)   Loss 0.0740 (0.0740)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 22:56:12]
  Epoch: [049][200/391]   Time 0.364 (0.382)   Data 0.000 (0.016)   Loss 0.0543 (0.0946)   Prec@1 97.656 (96.708)   Prec@5 100.000 (99.988)   [2019-08-30 22:57:26]
  **Test** Prec@1 90.740 Prec@5 99.720 Error@1 9.260

==>>[2019-08-30 22:59:02] [Epoch=050/100] [Need: 02:21:24] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [050][000/391]   Time 3.624 (3.624)   Data 3.144 (3.144)   Loss 0.0932 (0.0932)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 22:59:06]
  Epoch: [050][200/391]   Time 0.390 (0.364)   Data 0.000 (0.016)   Loss 0.1025 (0.0917)   Prec@1 96.875 (96.832)   Prec@5 100.000 (99.977)   [2019-08-30 23:00:16]
  **Test** Prec@1 90.340 Prec@5 99.630 Error@1 9.660

==>>[2019-08-30 23:01:53] [Epoch=051/100] [Need: 02:18:36] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [051][000/391]   Time 4.175 (4.175)   Data 3.720 (3.720)   Loss 0.1297 (0.1297)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 23:01:58]
  Epoch: [051][200/391]   Time 0.233 (0.370)   Data 0.000 (0.019)   Loss 0.0585 (0.0940)   Prec@1 98.438 (96.727)   Prec@5 100.000 (99.984)   [2019-08-30 23:03:08]
  **Test** Prec@1 90.620 Prec@5 99.650 Error@1 9.380

==>>[2019-08-30 23:04:42] [Epoch=052/100] [Need: 02:15:44] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [052][000/391]   Time 3.507 (3.507)   Data 3.089 (3.089)   Loss 0.0739 (0.0739)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 23:04:46]
  Epoch: [052][200/391]   Time 0.278 (0.375)   Data 0.000 (0.016)   Loss 0.0860 (0.0947)   Prec@1 96.094 (96.685)   Prec@5 100.000 (99.965)   [2019-08-30 23:05:58]
  **Test** Prec@1 90.260 Prec@5 99.650 Error@1 9.740

==>>[2019-08-30 23:07:34] [Epoch=053/100] [Need: 02:12:57] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [053][000/391]   Time 3.090 (3.090)   Data 2.457 (2.457)   Loss 0.1886 (0.1886)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 23:07:37]
  Epoch: [053][200/391]   Time 0.359 (0.388)   Data 0.000 (0.014)   Loss 0.1039 (0.0935)   Prec@1 96.094 (96.766)   Prec@5 100.000 (99.977)   [2019-08-30 23:08:52]
  **Test** Prec@1 90.470 Prec@5 99.670 Error@1 9.530

==>>[2019-08-30 23:10:25] [Epoch=054/100] [Need: 02:10:09] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [054][000/391]   Time 4.625 (4.625)   Data 4.129 (4.129)   Loss 0.0843 (0.0843)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:10:30]
  Epoch: [054][200/391]   Time 0.392 (0.387)   Data 0.000 (0.021)   Loss 0.0587 (0.0981)   Prec@1 98.438 (96.564)   Prec@5 100.000 (99.977)   [2019-08-30 23:11:43]
  **Test** Prec@1 90.590 Prec@5 99.740 Error@1 9.410

==>>[2019-08-30 23:13:17] [Epoch=055/100] [Need: 02:07:21] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [055][000/391]   Time 4.741 (4.741)   Data 4.317 (4.317)   Loss 0.1078 (0.1078)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 23:13:22]
  Epoch: [055][200/391]   Time 0.299 (0.381)   Data 0.000 (0.022)   Loss 0.1513 (0.0938)   Prec@1 95.312 (96.735)   Prec@5 100.000 (99.984)   [2019-08-30 23:14:34]
  **Test** Prec@1 90.450 Prec@5 99.710 Error@1 9.550

==>>[2019-08-30 23:16:10] [Epoch=056/100] [Need: 02:04:33] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [056][000/391]   Time 4.958 (4.958)   Data 4.493 (4.493)   Loss 0.0973 (0.0973)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:16:15]
  Epoch: [056][200/391]   Time 0.400 (0.379)   Data 0.000 (0.023)   Loss 0.0618 (0.0973)   Prec@1 97.656 (96.630)   Prec@5 100.000 (99.984)   [2019-08-30 23:17:26]
  **Test** Prec@1 89.160 Prec@5 99.630 Error@1 10.840

==>>[2019-08-30 23:18:58] [Epoch=057/100] [Need: 02:01:42] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [057][000/391]   Time 5.027 (5.027)   Data 4.432 (4.432)   Loss 0.0702 (0.0702)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 23:19:03]
  Epoch: [057][200/391]   Time 0.585 (0.383)   Data 0.000 (0.022)   Loss 0.1336 (0.0921)   Prec@1 95.312 (96.879)   Prec@5 100.000 (99.977)   [2019-08-30 23:20:15]
  **Test** Prec@1 91.090 Prec@5 99.660 Error@1 8.910

==>>[2019-08-30 23:21:49] [Epoch=058/100] [Need: 01:58:53] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [058][000/391]   Time 3.907 (3.907)   Data 3.450 (3.450)   Loss 0.1198 (0.1198)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 23:21:53]
  Epoch: [058][200/391]   Time 0.413 (0.381)   Data 0.000 (0.018)   Loss 0.1337 (0.0950)   Prec@1 93.750 (96.673)   Prec@5 100.000 (99.988)   [2019-08-30 23:23:06]
  **Test** Prec@1 90.510 Prec@5 99.720 Error@1 9.490

==>>[2019-08-30 23:24:37] [Epoch=059/100] [Need: 01:56:02] [learning_rate=0.0010] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [059][000/391]   Time 3.223 (3.223)   Data 2.728 (2.728)   Loss 0.0926 (0.0926)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 23:24:40]
  Epoch: [059][200/391]   Time 0.509 (0.378)   Data 0.000 (0.014)   Loss 0.0572 (0.0924)   Prec@1 98.438 (96.902)   Prec@5 100.000 (99.973)   [2019-08-30 23:25:53]
  **Test** Prec@1 90.560 Prec@5 99.720 Error@1 9.440

==>>[2019-08-30 23:27:28] [Epoch=060/100] [Need: 01:53:13] [learning_rate=0.0001] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [060][000/391]   Time 4.855 (4.855)   Data 4.430 (4.430)   Loss 0.0646 (0.0646)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 23:27:33]
  Epoch: [060][200/391]   Time 0.443 (0.393)   Data 0.000 (0.022)   Loss 0.0612 (0.0883)   Prec@1 98.438 (96.972)   Prec@5 100.000 (99.973)   [2019-08-30 23:28:47]
  **Test** Prec@1 91.070 Prec@5 99.690 Error@1 8.930

==>>[2019-08-30 23:30:19] [Epoch=061/100] [Need: 01:50:24] [learning_rate=0.0001] [Best : Accuracy=91.17, Error=8.83]
  Epoch: [061][000/391]   Time 5.445 (5.445)   Data 4.938 (4.938)   Loss 0.0804 (0.0804)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:30:25]
  Epoch: [061][200/391]   Time 0.401 (0.385)   Data 0.000 (0.025)   Loss 0.0389 (0.0805)   Prec@1 100.000 (97.345)   Prec@5 100.000 (99.992)   [2019-08-30 23:31:37]
  **Test** Prec@1 91.430 Prec@5 99.690 Error@1 8.570

==>>[2019-08-30 23:33:12] [Epoch=062/100] [Need: 01:47:36] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [062][000/391]   Time 4.856 (4.856)   Data 4.418 (4.418)   Loss 0.0976 (0.0976)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:33:17]
  Epoch: [062][200/391]   Time 0.382 (0.388)   Data 0.000 (0.022)   Loss 0.0774 (0.0825)   Prec@1 96.875 (97.182)   Prec@5 100.000 (99.984)   [2019-08-30 23:34:30]
  **Test** Prec@1 91.270 Prec@5 99.780 Error@1 8.730

==>>[2019-08-30 23:36:06] [Epoch=063/100] [Need: 01:44:48] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [063][000/391]   Time 4.951 (4.951)   Data 4.620 (4.620)   Loss 0.0450 (0.0450)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 23:36:11]
  Epoch: [063][200/391]   Time 0.468 (0.390)   Data 0.000 (0.023)   Loss 0.1214 (0.0812)   Prec@1 95.312 (97.369)   Prec@5 100.000 (99.984)   [2019-08-30 23:37:24]
  **Test** Prec@1 90.820 Prec@5 99.710 Error@1 9.180

==>>[2019-08-30 23:38:57] [Epoch=064/100] [Need: 01:41:59] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [064][000/391]   Time 3.737 (3.737)   Data 3.274 (3.274)   Loss 0.1233 (0.1233)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 23:39:01]
  Epoch: [064][200/391]   Time 0.354 (0.392)   Data 0.000 (0.017)   Loss 0.0881 (0.0836)   Prec@1 96.875 (97.147)   Prec@5 100.000 (99.981)   [2019-08-30 23:40:16]
  **Test** Prec@1 91.170 Prec@5 99.700 Error@1 8.830

==>>[2019-08-30 23:41:50] [Epoch=065/100] [Need: 01:39:11] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [065][000/391]   Time 3.571 (3.571)   Data 3.226 (3.226)   Loss 0.0626 (0.0626)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 23:41:54]
  Epoch: [065][200/391]   Time 0.443 (0.386)   Data 0.000 (0.016)   Loss 0.0485 (0.0800)   Prec@1 98.438 (97.260)   Prec@5 100.000 (99.977)   [2019-08-30 23:43:08]
  **Test** Prec@1 91.040 Prec@5 99.750 Error@1 8.960

==>>[2019-08-30 23:44:43] [Epoch=066/100] [Need: 01:36:22] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [066][000/391]   Time 4.212 (4.212)   Data 3.653 (3.653)   Loss 0.0700 (0.0700)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 23:44:47]
  Epoch: [066][200/391]   Time 0.429 (0.381)   Data 0.000 (0.019)   Loss 0.1150 (0.0777)   Prec@1 96.875 (97.373)   Prec@5 100.000 (99.988)   [2019-08-30 23:45:59]
  **Test** Prec@1 91.250 Prec@5 99.760 Error@1 8.750

==>>[2019-08-30 23:47:35] [Epoch=067/100] [Need: 01:33:33] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [067][000/391]   Time 4.468 (4.468)   Data 4.021 (4.021)   Loss 0.0517 (0.0517)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 23:47:40]
  Epoch: [067][200/391]   Time 0.482 (0.376)   Data 0.000 (0.021)   Loss 0.0531 (0.0780)   Prec@1 98.438 (97.450)   Prec@5 100.000 (99.992)   [2019-08-30 23:48:51]
  **Test** Prec@1 91.130 Prec@5 99.730 Error@1 8.870

==>>[2019-08-30 23:50:26] [Epoch=068/100] [Need: 01:30:43] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [068][000/391]   Time 4.460 (4.460)   Data 3.905 (3.905)   Loss 0.0892 (0.0892)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:50:31]
  Epoch: [068][200/391]   Time 0.404 (0.387)   Data 0.000 (0.020)   Loss 0.1076 (0.0794)   Prec@1 96.094 (97.291)   Prec@5 100.000 (99.981)   [2019-08-30 23:51:44]
  **Test** Prec@1 91.110 Prec@5 99.730 Error@1 8.890

==>>[2019-08-30 23:53:17] [Epoch=069/100] [Need: 01:27:54] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [069][000/391]   Time 4.443 (4.443)   Data 4.040 (4.040)   Loss 0.0939 (0.0939)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 23:53:21]
  Epoch: [069][200/391]   Time 0.500 (0.396)   Data 0.000 (0.020)   Loss 0.0601 (0.0813)   Prec@1 99.219 (97.213)   Prec@5 100.000 (99.984)   [2019-08-30 23:54:37]
  **Test** Prec@1 90.910 Prec@5 99.650 Error@1 9.090

==>>[2019-08-30 23:56:11] [Epoch=070/100] [Need: 01:25:05] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [070][000/391]   Time 3.756 (3.756)   Data 3.248 (3.248)   Loss 0.0694 (0.0694)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 23:56:15]
  Epoch: [070][200/391]   Time 0.268 (0.390)   Data 0.000 (0.016)   Loss 0.0815 (0.0767)   Prec@1 96.094 (97.462)   Prec@5 100.000 (99.988)   [2019-08-30 23:57:29]
  **Test** Prec@1 91.020 Prec@5 99.710 Error@1 8.980

==>>[2019-08-30 23:59:02] [Epoch=071/100] [Need: 01:22:15] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [071][000/391]   Time 4.489 (4.489)   Data 4.069 (4.069)   Loss 0.0977 (0.0977)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 23:59:06]
  Epoch: [071][200/391]   Time 0.315 (0.401)   Data 0.000 (0.021)   Loss 0.1221 (0.0779)   Prec@1 95.312 (97.458)   Prec@5 100.000 (99.996)   [2019-08-31 00:00:22]
  **Test** Prec@1 90.980 Prec@5 99.680 Error@1 9.020

==>>[2019-08-31 00:01:59] [Epoch=072/100] [Need: 01:19:27] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [072][000/391]   Time 4.118 (4.118)   Data 3.631 (3.631)   Loss 0.0816 (0.0816)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 00:02:03]
  Epoch: [072][200/391]   Time 0.426 (0.382)   Data 0.000 (0.018)   Loss 0.0630 (0.0777)   Prec@1 99.219 (97.442)   Prec@5 100.000 (99.984)   [2019-08-31 00:03:15]
  **Test** Prec@1 90.990 Prec@5 99.730 Error@1 9.010

==>>[2019-08-31 00:04:49] [Epoch=073/100] [Need: 01:16:37] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [073][000/391]   Time 4.107 (4.107)   Data 3.738 (3.738)   Loss 0.0654 (0.0654)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 00:04:53]
  Epoch: [073][200/391]   Time 0.200 (0.388)   Data 0.000 (0.019)   Loss 0.1118 (0.0783)   Prec@1 96.094 (97.384)   Prec@5 100.000 (99.981)   [2019-08-31 00:06:07]
  **Test** Prec@1 90.990 Prec@5 99.700 Error@1 9.010

==>>[2019-08-31 00:07:40] [Epoch=074/100] [Need: 01:13:47] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [074][000/391]   Time 3.905 (3.905)   Data 3.386 (3.386)   Loss 0.0459 (0.0459)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:07:44]
  Epoch: [074][200/391]   Time 0.560 (0.386)   Data 0.000 (0.017)   Loss 0.1131 (0.0784)   Prec@1 96.875 (97.404)   Prec@5 100.000 (99.996)   [2019-08-31 00:08:57]
  **Test** Prec@1 90.970 Prec@5 99.680 Error@1 9.030

==>>[2019-08-31 00:10:31] [Epoch=075/100] [Need: 01:10:57] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [075][000/391]   Time 4.100 (4.100)   Data 3.603 (3.603)   Loss 0.0480 (0.0480)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 00:10:35]
  Epoch: [075][200/391]   Time 0.307 (0.382)   Data 0.000 (0.018)   Loss 0.0361 (0.0773)   Prec@1 99.219 (97.404)   Prec@5 100.000 (99.992)   [2019-08-31 00:11:48]
  **Test** Prec@1 91.100 Prec@5 99.760 Error@1 8.900

==>>[2019-08-31 00:13:21] [Epoch=076/100] [Need: 01:08:07] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [076][000/391]   Time 4.318 (4.318)   Data 3.962 (3.962)   Loss 0.0654 (0.0654)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:13:26]
  Epoch: [076][200/391]   Time 0.402 (0.397)   Data 0.000 (0.020)   Loss 0.0823 (0.0748)   Prec@1 97.656 (97.505)   Prec@5 100.000 (99.988)   [2019-08-31 00:14:41]
  **Test** Prec@1 90.980 Prec@5 99.730 Error@1 9.020

==>>[2019-08-31 00:16:14] [Epoch=077/100] [Need: 01:05:17] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [077][000/391]   Time 5.047 (5.047)   Data 4.520 (4.520)   Loss 0.0609 (0.0609)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:16:19]
  Epoch: [077][200/391]   Time 0.512 (0.403)   Data 0.000 (0.023)   Loss 0.0830 (0.0804)   Prec@1 97.656 (97.236)   Prec@5 100.000 (99.992)   [2019-08-31 00:17:35]
  **Test** Prec@1 90.670 Prec@5 99.660 Error@1 9.330

==>>[2019-08-31 00:19:08] [Epoch=078/100] [Need: 01:02:28] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [078][000/391]   Time 3.972 (3.972)   Data 3.560 (3.560)   Loss 0.0640 (0.0640)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:19:12]
  Epoch: [078][200/391]   Time 0.359 (0.389)   Data 0.000 (0.018)   Loss 0.0327 (0.0798)   Prec@1 100.000 (97.256)   Prec@5 100.000 (99.992)   [2019-08-31 00:20:26]
  **Test** Prec@1 90.750 Prec@5 99.730 Error@1 9.250

==>>[2019-08-31 00:22:02] [Epoch=079/100] [Need: 00:59:39] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [079][000/391]   Time 3.160 (3.160)   Data 2.680 (2.680)   Loss 0.0647 (0.0647)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 00:22:06]
  Epoch: [079][200/391]   Time 0.589 (0.382)   Data 0.000 (0.014)   Loss 0.0569 (0.0768)   Prec@1 98.438 (97.512)   Prec@5 100.000 (99.984)   [2019-08-31 00:23:19]
  **Test** Prec@1 90.950 Prec@5 99.700 Error@1 9.050

==>>[2019-08-31 00:24:57] [Epoch=080/100] [Need: 00:56:49] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [080][000/391]   Time 4.182 (4.182)   Data 3.638 (3.638)   Loss 0.1030 (0.1030)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 00:25:01]
  Epoch: [080][200/391]   Time 0.523 (0.383)   Data 0.000 (0.018)   Loss 0.0733 (0.0815)   Prec@1 97.656 (97.306)   Prec@5 100.000 (99.996)   [2019-08-31 00:26:13]
  **Test** Prec@1 90.630 Prec@5 99.660 Error@1 9.370

==>>[2019-08-31 00:27:45] [Epoch=081/100] [Need: 00:53:58] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [081][000/391]   Time 3.706 (3.706)   Data 3.159 (3.159)   Loss 0.0527 (0.0527)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:27:49]
  Epoch: [081][200/391]   Time 0.441 (0.384)   Data 0.000 (0.016)   Loss 0.0654 (0.0776)   Prec@1 98.438 (97.493)   Prec@5 100.000 (99.988)   [2019-08-31 00:29:03]
  **Test** Prec@1 90.700 Prec@5 99.690 Error@1 9.300

==>>[2019-08-31 00:30:38] [Epoch=082/100] [Need: 00:51:08] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [082][000/391]   Time 3.945 (3.945)   Data 3.655 (3.655)   Loss 0.0642 (0.0642)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:30:42]
  Epoch: [082][200/391]   Time 0.645 (0.389)   Data 0.000 (0.018)   Loss 0.0620 (0.0736)   Prec@1 97.656 (97.617)   Prec@5 100.000 (99.988)   [2019-08-31 00:31:56]
  **Test** Prec@1 90.840 Prec@5 99.690 Error@1 9.160

==>>[2019-08-31 00:33:28] [Epoch=083/100] [Need: 00:48:18] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [083][000/391]   Time 3.909 (3.909)   Data 3.508 (3.508)   Loss 0.0867 (0.0867)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 00:33:32]
  Epoch: [083][200/391]   Time 0.430 (0.388)   Data 0.000 (0.018)   Loss 0.0381 (0.0783)   Prec@1 100.000 (97.326)   Prec@5 100.000 (99.969)   [2019-08-31 00:34:46]
  **Test** Prec@1 90.210 Prec@5 99.620 Error@1 9.790

==>>[2019-08-31 00:36:20] [Epoch=084/100] [Need: 00:45:27] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [084][000/391]   Time 3.801 (3.801)   Data 3.348 (3.348)   Loss 0.0361 (0.0361)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:36:24]
  Epoch: [084][200/391]   Time 0.327 (0.394)   Data 0.000 (0.017)   Loss 0.0554 (0.0769)   Prec@1 98.438 (97.396)   Prec@5 100.000 (99.992)   [2019-08-31 00:37:39]
  **Test** Prec@1 90.770 Prec@5 99.720 Error@1 9.230

==>>[2019-08-31 00:39:16] [Epoch=085/100] [Need: 00:42:38] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [085][000/391]   Time 3.409 (3.409)   Data 3.000 (3.000)   Loss 0.1141 (0.1141)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-31 00:39:19]
  Epoch: [085][200/391]   Time 0.340 (0.391)   Data 0.000 (0.015)   Loss 0.0389 (0.0771)   Prec@1 99.219 (97.520)   Prec@5 100.000 (99.988)   [2019-08-31 00:40:34]
  **Test** Prec@1 90.910 Prec@5 99.710 Error@1 9.090

==>>[2019-08-31 00:42:11] [Epoch=086/100] [Need: 00:39:48] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [086][000/391]   Time 2.075 (2.075)   Data 1.678 (1.678)   Loss 0.1052 (0.1052)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 00:42:13]
  Epoch: [086][200/391]   Time 0.253 (0.373)   Data 0.000 (0.009)   Loss 0.1296 (0.0780)   Prec@1 94.531 (97.454)   Prec@5 100.000 (99.981)   [2019-08-31 00:43:26]
  **Test** Prec@1 91.290 Prec@5 99.640 Error@1 8.710

==>>[2019-08-31 00:45:01] [Epoch=087/100] [Need: 00:36:57] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [087][000/391]   Time 3.346 (3.346)   Data 2.894 (2.894)   Loss 0.0505 (0.0505)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:45:04]
  Epoch: [087][200/391]   Time 0.314 (0.389)   Data 0.000 (0.015)   Loss 0.0553 (0.0799)   Prec@1 98.438 (97.423)   Prec@5 100.000 (99.988)   [2019-08-31 00:46:19]
  **Test** Prec@1 91.280 Prec@5 99.710 Error@1 8.720

==>>[2019-08-31 00:47:52] [Epoch=088/100] [Need: 00:34:07] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [088][000/391]   Time 3.377 (3.377)   Data 3.057 (3.057)   Loss 0.0617 (0.0617)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:47:55]
  Epoch: [088][200/391]   Time 0.495 (0.397)   Data 0.000 (0.016)   Loss 0.0587 (0.0786)   Prec@1 97.656 (97.396)   Prec@5 100.000 (99.992)   [2019-08-31 00:49:12]
  **Test** Prec@1 91.100 Prec@5 99.740 Error@1 8.900

==>>[2019-08-31 00:50:44] [Epoch=089/100] [Need: 00:31:16] [learning_rate=0.0001] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [089][000/391]   Time 3.481 (3.481)   Data 3.115 (3.115)   Loss 0.0798 (0.0798)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 00:50:48]
  Epoch: [089][200/391]   Time 0.367 (0.397)   Data 0.000 (0.016)   Loss 0.0916 (0.0794)   Prec@1 96.875 (97.338)   Prec@5 100.000 (100.000)   [2019-08-31 00:52:04]
  **Test** Prec@1 90.960 Prec@5 99.680 Error@1 9.040

==>>[2019-08-31 00:53:38] [Epoch=090/100] [Need: 00:28:26] [learning_rate=0.0000] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [090][000/391]   Time 3.673 (3.673)   Data 3.249 (3.249)   Loss 0.0388 (0.0388)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2019-08-31 00:53:42]
  Epoch: [090][200/391]   Time 0.230 (0.399)   Data 0.000 (0.017)   Loss 0.0558 (0.0797)   Prec@1 98.438 (97.279)   Prec@5 100.000 (99.988)   [2019-08-31 00:54:58]
  **Test** Prec@1 91.030 Prec@5 99.690 Error@1 8.970

==>>[2019-08-31 00:56:31] [Epoch=091/100] [Need: 00:25:36] [learning_rate=0.0000] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [091][000/391]   Time 3.161 (3.161)   Data 2.793 (2.793)   Loss 0.0399 (0.0399)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:56:34]
  Epoch: [091][200/391]   Time 0.423 (0.390)   Data 0.000 (0.014)   Loss 0.0325 (0.0752)   Prec@1 99.219 (97.450)   Prec@5 100.000 (99.992)   [2019-08-31 00:57:49]
  **Test** Prec@1 91.250 Prec@5 99.660 Error@1 8.750

==>>[2019-08-31 00:59:22] [Epoch=092/100] [Need: 00:22:45] [learning_rate=0.0000] [Best : Accuracy=91.43, Error=8.57]
  Epoch: [092][000/391]   Time 3.731 (3.731)   Data 3.252 (3.252)   Loss 0.0537 (0.0537)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:59:26]
  Epoch: [092][200/391]   Time 0.215 (0.387)   Data 0.000 (0.016)   Loss 0.0705 (0.0723)   Prec@1 98.438 (97.645)   Prec@5 100.000 (99.977)   [2019-08-31 01:00:40]
  **Test** Prec@1 91.570 Prec@5 99.690 Error@1 8.430

==>>[2019-08-31 01:02:15] [Epoch=093/100] [Need: 00:19:55] [learning_rate=0.0000] [Best : Accuracy=91.57, Error=8.43]
  Epoch: [093][000/391]   Time 3.995 (3.995)   Data 3.434 (3.434)   Loss 0.0396 (0.0396)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 01:02:19]
  Epoch: [093][200/391]   Time 0.495 (0.396)   Data 0.000 (0.017)   Loss 0.0340 (0.0749)   Prec@1 98.438 (97.435)   Prec@5 100.000 (99.992)   [2019-08-31 01:03:35]
  **Test** Prec@1 91.280 Prec@5 99.700 Error@1 8.720

==>>[2019-08-31 01:05:02] [Epoch=094/100] [Need: 00:17:04] [learning_rate=0.0000] [Best : Accuracy=91.57, Error=8.43]
  Epoch: [094][000/391]   Time 3.364 (3.364)   Data 2.987 (2.987)   Loss 0.0825 (0.0825)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 01:05:05]
  Epoch: [094][200/391]   Time 0.368 (0.360)   Data 0.000 (0.015)   Loss 0.0532 (0.0711)   Prec@1 99.219 (97.746)   Prec@5 100.000 (99.984)   [2019-08-31 01:06:14]
  **Test** Prec@1 91.120 Prec@5 99.700 Error@1 8.880

==>>[2019-08-31 01:07:41] [Epoch=095/100] [Need: 00:14:12] [learning_rate=0.0000] [Best : Accuracy=91.57, Error=8.43]
  Epoch: [095][000/391]   Time 4.381 (4.381)   Data 3.813 (3.813)   Loss 0.0406 (0.0406)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 01:07:45]
  Epoch: [095][200/391]   Time 0.386 (0.375)   Data 0.000 (0.019)   Loss 0.1155 (0.0770)   Prec@1 95.312 (97.442)   Prec@5 100.000 (99.988)   [2019-08-31 01:08:56]
  **Test** Prec@1 91.050 Prec@5 99.700 Error@1 8.950

==>>[2019-08-31 01:10:20] [Epoch=096/100] [Need: 00:11:21] [learning_rate=0.0000] [Best : Accuracy=91.57, Error=8.43]
  Epoch: [096][000/391]   Time 4.162 (4.162)   Data 3.680 (3.680)   Loss 0.0788 (0.0788)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 01:10:25]
  Epoch: [096][200/391]   Time 0.307 (0.361)   Data 0.000 (0.019)   Loss 0.0402 (0.0737)   Prec@1 99.219 (97.520)   Prec@5 100.000 (99.996)   [2019-08-31 01:11:33]
  **Test** Prec@1 91.300 Prec@5 99.710 Error@1 8.700

==>>[2019-08-31 01:12:58] [Epoch=097/100] [Need: 00:08:30] [learning_rate=0.0000] [Best : Accuracy=91.57, Error=8.43]
  Epoch: [097][000/391]   Time 3.657 (3.657)   Data 3.074 (3.074)   Loss 0.0608 (0.0608)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 01:13:01]
  Epoch: [097][200/391]   Time 0.390 (0.360)   Data 0.000 (0.016)   Loss 0.0750 (0.0716)   Prec@1 98.438 (97.637)   Prec@5 100.000 (99.988)   [2019-08-31 01:14:10]
  **Test** Prec@1 91.300 Prec@5 99.710 Error@1 8.700

==>>[2019-08-31 01:15:38] [Epoch=098/100] [Need: 00:05:40] [learning_rate=0.0000] [Best : Accuracy=91.57, Error=8.43]
  Epoch: [098][000/391]   Time 4.220 (4.220)   Data 3.676 (3.676)   Loss 0.0682 (0.0682)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:15:42]
  Epoch: [098][200/391]   Time 0.451 (0.337)   Data 0.000 (0.019)   Loss 0.0718 (0.0723)   Prec@1 97.656 (97.695)   Prec@5 100.000 (99.992)   [2019-08-31 01:16:46]
  **Test** Prec@1 91.160 Prec@5 99.740 Error@1 8.840

==>>[2019-08-31 01:18:04] [Epoch=099/100] [Need: 00:02:49] [learning_rate=0.0000] [Best : Accuracy=91.57, Error=8.43]
  Epoch: [099][000/391]   Time 3.167 (3.167)   Data 2.713 (2.713)   Loss 0.0608 (0.0608)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:18:08]
  Epoch: [099][200/391]   Time 0.349 (0.299)   Data 0.000 (0.014)   Loss 0.0787 (0.0729)   Prec@1 96.875 (97.613)   Prec@5 100.000 (99.981)   [2019-08-31 01:19:05]
  **Test** Prec@1 91.260 Prec@5 99.710 Error@1 8.740
