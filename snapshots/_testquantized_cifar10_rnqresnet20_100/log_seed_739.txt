save path : ./snapshots/_testquantized_cifar10_rnqresnet20_100
{'aq_bits': None, 'aq_type': 'rule', 'arch': 'rnqresnet20', 'batch_size': 128, 'data_path': '../../datasets/CIFAR10/cifar', 'dataset': 'cifar10', 'decay': 0.0001, 'epochs': 100, 'evaluate': False, 'gammas': [0.1, 0.1, 0.1], 'learning_rate': 0.1, 'manualSeed': 739, 'momentum': 0.9, 'ngpu': 4, 'pretrained': '', 'print_freq': 200, 'resume': 'snapshots/quantized_cifar10_rnqresnet20_100/checkpoint.pth.tar', 'save_path': './snapshots/_testquantized_cifar10_rnqresnet20_100', 'schedule': [30, 60, 90], 'start_epoch': 0, 'use_cuda': True, 'workers': 16}
Random Seed: 739
python version : 3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]
torch  version : 1.1.0
cudnn  version : 7501
=> creating model 'rnqresnet20'
=> network :
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): QuantLinear(in_features=64, out_features=10, bias=True, method=KMeans, nbits=4)
)
=> loading checkpoint 'snapshots/quantized_cifar10_rnqresnet20_100/checkpoint.pth.tar'
=> loaded checkpoint 'snapshots/quantized_cifar10_rnqresnet20_100/checkpoint.pth.tar' (epoch 100)

==>>[2019-08-30 22:14:50] [Epoch=000/100] [Need: 00:00:00] [learning_rate=0.1000] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/391]   Time 49.692 (49.692)   Data 2.494 (2.494)   Loss 0.1764 (0.1764)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 22:15:39]
  Epoch: [000][200/391]   Time 0.375 (0.665)   Data 0.000 (0.013)   Loss 0.4555 (0.5278)   Prec@1 78.906 (81.782)   Prec@5 98.438 (99.133)   [2019-08-30 22:17:03]
  **Test** Prec@1 72.780 Prec@5 96.880 Error@1 27.220

==>>[2019-08-30 22:18:56] [Epoch=001/100] [Need: 06:44:15] [learning_rate=0.1000] [Best : Accuracy=72.78, Error=27.22]
  Epoch: [001][000/391]   Time 5.403 (5.403)   Data 4.810 (4.810)   Loss 0.5885 (0.5885)   Prec@1 80.469 (80.469)   Prec@5 99.219 (99.219)   [2019-08-30 22:19:01]
  Epoch: [001][200/391]   Time 0.384 (0.443)   Data 0.000 (0.024)   Loss 0.4217 (0.4316)   Prec@1 85.938 (85.075)   Prec@5 100.000 (99.425)   [2019-08-30 22:20:25]
  **Test** Prec@1 68.030 Prec@5 97.260 Error@1 31.970

==>>[2019-08-30 22:22:17] [Epoch=002/100] [Need: 06:04:47] [learning_rate=0.1000] [Best : Accuracy=72.78, Error=27.22]
  Epoch: [002][000/391]   Time 6.460 (6.460)   Data 5.756 (5.756)   Loss 0.3536 (0.3536)   Prec@1 89.062 (89.062)   Prec@5 100.000 (100.000)   [2019-08-30 22:22:24]
  Epoch: [002][200/391]   Time 0.383 (0.484)   Data 0.000 (0.029)   Loss 0.4524 (0.4064)   Prec@1 83.594 (85.969)   Prec@5 99.219 (99.405)   [2019-08-30 22:23:54]
  **Test** Prec@1 69.230 Prec@5 96.560 Error@1 30.770

==>>[2019-08-30 22:25:46] [Epoch=003/100] [Need: 05:52:43] [learning_rate=0.1000] [Best : Accuracy=72.78, Error=27.22]
  Epoch: [003][000/391]   Time 5.839 (5.839)   Data 5.368 (5.368)   Loss 0.3464 (0.3464)   Prec@1 89.844 (89.844)   Prec@5 100.000 (100.000)   [2019-08-30 22:25:51]
  Epoch: [003][200/391]   Time 0.479 (0.474)   Data 0.000 (0.027)   Loss 0.4611 (0.4002)   Prec@1 82.031 (86.186)   Prec@5 100.000 (99.514)   [2019-08-30 22:27:21]
  **Test** Prec@1 72.190 Prec@5 97.700 Error@1 27.810

==>>[2019-08-30 22:29:13] [Epoch=004/100] [Need: 05:44:53] [learning_rate=0.1000] [Best : Accuracy=72.78, Error=27.22]
  Epoch: [004][000/391]   Time 5.524 (5.524)   Data 4.995 (4.995)   Loss 0.3961 (0.3961)   Prec@1 85.156 (85.156)   Prec@5 99.219 (99.219)   [2019-08-30 22:29:18]
  Epoch: [004][200/391]   Time 0.536 (0.474)   Data 0.000 (0.025)   Loss 0.5095 (0.3838)   Prec@1 82.812 (86.688)   Prec@5 98.438 (99.475)   [2019-08-30 22:30:48]
  **Test** Prec@1 81.370 Prec@5 99.100 Error@1 18.630

==>>[2019-08-30 22:32:42] [Epoch=005/100] [Need: 05:39:09] [learning_rate=0.1000] [Best : Accuracy=81.37, Error=18.63]
  Epoch: [005][000/391]   Time 4.307 (4.307)   Data 3.856 (3.856)   Loss 0.3702 (0.3702)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2019-08-30 22:32:46]
  Epoch: [005][200/391]   Time 0.351 (0.454)   Data 0.000 (0.019)   Loss 0.2858 (0.3557)   Prec@1 89.844 (87.655)   Prec@5 100.000 (99.565)   [2019-08-30 22:34:13]
  **Test** Prec@1 78.960 Prec@5 98.800 Error@1 21.040

==>>[2019-08-30 22:36:10] [Epoch=006/100] [Need: 05:34:03] [learning_rate=0.1000] [Best : Accuracy=81.37, Error=18.63]
  Epoch: [006][000/391]   Time 6.234 (6.234)   Data 5.875 (5.875)   Loss 0.2871 (0.2871)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 22:36:16]
  Epoch: [006][200/391]   Time 0.378 (0.473)   Data 0.000 (0.030)   Loss 0.3969 (0.3432)   Prec@1 87.500 (88.099)   Prec@5 100.000 (99.530)   [2019-08-30 22:37:45]
  **Test** Prec@1 78.450 Prec@5 97.760 Error@1 21.550

==>>[2019-08-30 22:39:37] [Epoch=007/100] [Need: 05:28:57] [learning_rate=0.1000] [Best : Accuracy=81.37, Error=18.63]
  Epoch: [007][000/391]   Time 5.336 (5.336)   Data 4.778 (4.778)   Loss 0.3045 (0.3045)   Prec@1 88.281 (88.281)   Prec@5 99.219 (99.219)   [2019-08-30 22:39:42]
  Epoch: [007][200/391]   Time 0.634 (0.474)   Data 0.000 (0.024)   Loss 0.2480 (0.3332)   Prec@1 92.188 (88.619)   Prec@5 99.219 (99.642)   [2019-08-30 22:41:12]
  **Test** Prec@1 82.370 Prec@5 99.130 Error@1 17.630

==>>[2019-08-30 22:43:03] [Epoch=008/100] [Need: 05:24:25] [learning_rate=0.1000] [Best : Accuracy=82.37, Error=17.63]
  Epoch: [008][000/391]   Time 4.772 (4.772)   Data 4.114 (4.114)   Loss 0.3563 (0.3563)   Prec@1 84.375 (84.375)   Prec@5 99.219 (99.219)   [2019-08-30 22:43:08]
  Epoch: [008][200/391]   Time 0.391 (0.475)   Data 0.000 (0.021)   Loss 0.3962 (0.3404)   Prec@1 84.375 (88.102)   Prec@5 99.219 (99.611)   [2019-08-30 22:44:39]
  **Test** Prec@1 82.060 Prec@5 99.070 Error@1 17.940

==>>[2019-08-30 22:46:27] [Epoch=009/100] [Need: 05:19:26] [learning_rate=0.1000] [Best : Accuracy=82.37, Error=17.63]
  Epoch: [009][000/391]   Time 6.054 (6.054)   Data 5.358 (5.358)   Loss 0.2921 (0.2921)   Prec@1 87.500 (87.500)   Prec@5 98.438 (98.438)   [2019-08-30 22:46:33]
  Epoch: [009][200/391]   Time 0.564 (0.475)   Data 0.000 (0.027)   Loss 0.3261 (0.3284)   Prec@1 87.500 (88.553)   Prec@5 98.438 (99.576)   [2019-08-30 22:48:02]
  **Test** Prec@1 74.230 Prec@5 96.850 Error@1 25.770

==>>[2019-08-30 22:49:52] [Epoch=010/100] [Need: 05:15:17] [learning_rate=0.1000] [Best : Accuracy=82.37, Error=17.63]
  Epoch: [010][000/391]   Time 4.507 (4.507)   Data 3.941 (3.941)   Loss 0.4219 (0.4219)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2019-08-30 22:49:57]
  Epoch: [010][200/391]   Time 0.445 (0.456)   Data 0.000 (0.020)   Loss 0.3716 (0.3153)   Prec@1 85.938 (89.167)   Prec@5 100.000 (99.677)   [2019-08-30 22:51:24]
  **Test** Prec@1 83.260 Prec@5 99.110 Error@1 16.740

==>>[2019-08-30 22:53:15] [Epoch=011/100] [Need: 05:10:40] [learning_rate=0.1000] [Best : Accuracy=83.26, Error=16.74]
  Epoch: [011][000/391]   Time 5.239 (5.239)   Data 4.627 (4.627)   Loss 0.2892 (0.2892)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 22:53:20]
  Epoch: [011][200/391]   Time 0.518 (0.457)   Data 0.000 (0.023)   Loss 0.2375 (0.3145)   Prec@1 90.625 (88.985)   Prec@5 100.000 (99.666)   [2019-08-30 22:54:47]
  **Test** Prec@1 82.600 Prec@5 99.100 Error@1 17.400

==>>[2019-08-30 22:56:41] [Epoch=012/100] [Need: 05:06:45] [learning_rate=0.1000] [Best : Accuracy=83.26, Error=16.74]
  Epoch: [012][000/391]   Time 5.332 (5.332)   Data 4.841 (4.841)   Loss 0.3111 (0.3111)   Prec@1 89.844 (89.844)   Prec@5 99.219 (99.219)   [2019-08-30 22:56:46]
  Epoch: [012][200/391]   Time 0.370 (0.464)   Data 0.000 (0.024)   Loss 0.3528 (0.3220)   Prec@1 88.281 (88.880)   Prec@5 100.000 (99.631)   [2019-08-30 22:58:14]
  **Test** Prec@1 79.460 Prec@5 98.440 Error@1 20.540

==>>[2019-08-30 23:00:05] [Epoch=013/100] [Need: 05:02:42] [learning_rate=0.1000] [Best : Accuracy=83.26, Error=16.74]
  Epoch: [013][000/391]   Time 4.437 (4.437)   Data 4.046 (4.046)   Loss 0.3969 (0.3969)   Prec@1 86.719 (86.719)   Prec@5 100.000 (100.000)   [2019-08-30 23:00:10]
  Epoch: [013][200/391]   Time 0.311 (0.461)   Data 0.000 (0.020)   Loss 0.2532 (0.3098)   Prec@1 91.406 (89.152)   Prec@5 100.000 (99.674)   [2019-08-30 23:01:38]
  **Test** Prec@1 83.800 Prec@5 99.190 Error@1 16.200

==>>[2019-08-30 23:03:30] [Epoch=014/100] [Need: 04:58:55] [learning_rate=0.1000] [Best : Accuracy=83.80, Error=16.20]
  Epoch: [014][000/391]   Time 6.677 (6.677)   Data 6.142 (6.142)   Loss 0.4212 (0.4212)   Prec@1 86.719 (86.719)   Prec@5 100.000 (100.000)   [2019-08-30 23:03:37]
  Epoch: [014][200/391]   Time 0.548 (0.471)   Data 0.000 (0.031)   Loss 0.4456 (0.3177)   Prec@1 87.500 (88.880)   Prec@5 100.000 (99.701)   [2019-08-30 23:05:05]
  **Test** Prec@1 79.960 Prec@5 98.690 Error@1 20.040

==>>[2019-08-30 23:06:58] [Epoch=015/100] [Need: 04:55:19] [learning_rate=0.1000] [Best : Accuracy=83.80, Error=16.20]
  Epoch: [015][000/391]   Time 5.182 (5.182)   Data 4.607 (4.607)   Loss 0.2122 (0.2122)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 23:07:03]
  Epoch: [015][200/391]   Time 0.379 (0.443)   Data 0.000 (0.023)   Loss 0.3044 (0.3115)   Prec@1 89.062 (89.195)   Prec@5 100.000 (99.674)   [2019-08-30 23:08:27]
  **Test** Prec@1 83.130 Prec@5 98.960 Error@1 16.870

==>>[2019-08-30 23:10:20] [Epoch=016/100] [Need: 04:51:18] [learning_rate=0.1000] [Best : Accuracy=83.80, Error=16.20]
  Epoch: [016][000/391]   Time 6.320 (6.320)   Data 5.759 (5.759)   Loss 0.3186 (0.3186)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2019-08-30 23:10:26]
  Epoch: [016][200/391]   Time 0.377 (0.450)   Data 0.000 (0.029)   Loss 0.2950 (0.3049)   Prec@1 89.062 (89.517)   Prec@5 100.000 (99.670)   [2019-08-30 23:11:50]
  **Test** Prec@1 78.990 Prec@5 98.900 Error@1 21.010

==>>[2019-08-30 23:13:45] [Epoch=017/100] [Need: 04:47:36] [learning_rate=0.1000] [Best : Accuracy=83.80, Error=16.20]
  Epoch: [017][000/391]   Time 6.607 (6.607)   Data 6.087 (6.087)   Loss 0.3021 (0.3021)   Prec@1 85.938 (85.938)   Prec@5 100.000 (100.000)   [2019-08-30 23:13:52]
  Epoch: [017][200/391]   Time 0.299 (0.467)   Data 0.000 (0.031)   Loss 0.1874 (0.3117)   Prec@1 90.625 (89.152)   Prec@5 100.000 (99.743)   [2019-08-30 23:15:19]
  **Test** Prec@1 77.110 Prec@5 99.070 Error@1 22.890

==>>[2019-08-30 23:17:11] [Epoch=018/100] [Need: 04:43:57] [learning_rate=0.1000] [Best : Accuracy=83.80, Error=16.20]
  Epoch: [018][000/391]   Time 6.637 (6.637)   Data 6.138 (6.138)   Loss 0.3248 (0.3248)   Prec@1 88.281 (88.281)   Prec@5 100.000 (100.000)   [2019-08-30 23:17:17]
  Epoch: [018][200/391]   Time 0.403 (0.482)   Data 0.000 (0.031)   Loss 0.2947 (0.3179)   Prec@1 90.625 (89.078)   Prec@5 100.000 (99.674)   [2019-08-30 23:18:48]
  **Test** Prec@1 74.280 Prec@5 98.610 Error@1 25.720

==>>[2019-08-30 23:20:42] [Epoch=019/100] [Need: 04:40:43] [learning_rate=0.1000] [Best : Accuracy=83.80, Error=16.20]
  Epoch: [019][000/391]   Time 5.550 (5.550)   Data 4.908 (4.908)   Loss 0.2364 (0.2364)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-30 23:20:47]
  Epoch: [019][200/391]   Time 0.504 (0.474)   Data 0.000 (0.025)   Loss 0.1999 (0.3034)   Prec@1 92.969 (89.459)   Prec@5 99.219 (99.685)   [2019-08-30 23:22:17]
  **Test** Prec@1 78.330 Prec@5 99.000 Error@1 21.670

==>>[2019-08-30 23:24:08] [Epoch=020/100] [Need: 04:37:06] [learning_rate=0.1000] [Best : Accuracy=83.80, Error=16.20]
  Epoch: [020][000/391]   Time 5.492 (5.492)   Data 4.804 (4.804)   Loss 0.3409 (0.3409)   Prec@1 89.844 (89.844)   Prec@5 99.219 (99.219)   [2019-08-30 23:24:13]
  Epoch: [020][200/391]   Time 0.371 (0.476)   Data 0.000 (0.024)   Loss 0.3883 (0.3063)   Prec@1 86.719 (89.370)   Prec@5 100.000 (99.720)   [2019-08-30 23:25:44]
  **Test** Prec@1 63.370 Prec@5 96.240 Error@1 36.630

==>>[2019-08-30 23:27:38] [Epoch=021/100] [Need: 04:33:49] [learning_rate=0.1000] [Best : Accuracy=83.80, Error=16.20]
  Epoch: [021][000/391]   Time 5.939 (5.939)   Data 5.396 (5.396)   Loss 0.3440 (0.3440)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2019-08-30 23:27:44]
  Epoch: [021][200/391]   Time 0.456 (0.450)   Data 0.000 (0.027)   Loss 0.3145 (0.3099)   Prec@1 87.500 (89.160)   Prec@5 100.000 (99.670)   [2019-08-30 23:29:09]
  **Test** Prec@1 84.290 Prec@5 99.310 Error@1 15.710

==>>[2019-08-30 23:31:02] [Epoch=022/100] [Need: 04:30:08] [learning_rate=0.1000] [Best : Accuracy=84.29, Error=15.71]
  Epoch: [022][000/391]   Time 4.989 (4.989)   Data 4.317 (4.317)   Loss 0.3494 (0.3494)   Prec@1 87.500 (87.500)   Prec@5 99.219 (99.219)   [2019-08-30 23:31:07]
  Epoch: [022][200/391]   Time 0.351 (0.470)   Data 0.000 (0.022)   Loss 0.3009 (0.3040)   Prec@1 86.719 (89.381)   Prec@5 100.000 (99.689)   [2019-08-30 23:32:37]
  **Test** Prec@1 76.010 Prec@5 98.260 Error@1 23.990

==>>[2019-08-30 23:34:28] [Epoch=023/100] [Need: 04:26:32] [learning_rate=0.1000] [Best : Accuracy=84.29, Error=15.71]
  Epoch: [023][000/391]   Time 5.978 (5.978)   Data 5.140 (5.140)   Loss 0.2647 (0.2647)   Prec@1 89.062 (89.062)   Prec@5 100.000 (100.000)   [2019-08-30 23:34:34]
  Epoch: [023][200/391]   Time 0.445 (0.482)   Data 0.000 (0.026)   Loss 0.2888 (0.3108)   Prec@1 92.188 (89.156)   Prec@5 100.000 (99.654)   [2019-08-30 23:36:05]
  **Test** Prec@1 80.370 Prec@5 98.890 Error@1 19.630

==>>[2019-08-30 23:37:58] [Epoch=024/100] [Need: 04:23:13] [learning_rate=0.1000] [Best : Accuracy=84.29, Error=15.71]
  Epoch: [024][000/391]   Time 5.644 (5.644)   Data 5.158 (5.158)   Loss 0.2220 (0.2220)   Prec@1 94.531 (94.531)   Prec@5 99.219 (99.219)   [2019-08-30 23:38:04]
  Epoch: [024][200/391]   Time 0.462 (0.480)   Data 0.000 (0.026)   Loss 0.2557 (0.3155)   Prec@1 88.281 (88.798)   Prec@5 100.000 (99.674)   [2019-08-30 23:39:35]
  **Test** Prec@1 75.250 Prec@5 97.780 Error@1 24.750

==>>[2019-08-30 23:41:27] [Epoch=025/100] [Need: 04:19:46] [learning_rate=0.1000] [Best : Accuracy=84.29, Error=15.71]
  Epoch: [025][000/391]   Time 4.888 (4.888)   Data 4.431 (4.431)   Loss 0.3483 (0.3483)   Prec@1 85.156 (85.156)   Prec@5 99.219 (99.219)   [2019-08-30 23:41:31]
  Epoch: [025][200/391]   Time 0.335 (0.464)   Data 0.000 (0.022)   Loss 0.2882 (0.3034)   Prec@1 90.625 (89.385)   Prec@5 100.000 (99.662)   [2019-08-30 23:43:00]
  **Test** Prec@1 79.480 Prec@5 98.890 Error@1 20.520

==>>[2019-08-30 23:44:55] [Epoch=026/100] [Need: 04:16:21] [learning_rate=0.1000] [Best : Accuracy=84.29, Error=15.71]
  Epoch: [026][000/391]   Time 5.421 (5.421)   Data 4.926 (4.926)   Loss 0.2141 (0.2141)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 23:45:01]
  Epoch: [026][200/391]   Time 0.323 (0.463)   Data 0.000 (0.025)   Loss 0.2987 (0.3037)   Prec@1 86.719 (89.696)   Prec@5 100.000 (99.639)   [2019-08-30 23:46:28]
  **Test** Prec@1 80.190 Prec@5 98.760 Error@1 19.810

==>>[2019-08-30 23:48:20] [Epoch=027/100] [Need: 04:12:45] [learning_rate=0.1000] [Best : Accuracy=84.29, Error=15.71]
  Epoch: [027][000/391]   Time 4.660 (4.660)   Data 4.043 (4.043)   Loss 0.2995 (0.2995)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 23:48:24]
  Epoch: [027][200/391]   Time 0.366 (0.474)   Data 0.000 (0.020)   Loss 0.2872 (0.3077)   Prec@1 90.625 (89.346)   Prec@5 100.000 (99.720)   [2019-08-30 23:49:55]
  **Test** Prec@1 79.850 Prec@5 98.750 Error@1 20.150

==>>[2019-08-30 23:51:45] [Epoch=028/100] [Need: 04:09:09] [learning_rate=0.1000] [Best : Accuracy=84.29, Error=15.71]
  Epoch: [028][000/391]   Time 5.242 (5.242)   Data 4.634 (4.634)   Loss 0.2564 (0.2564)   Prec@1 92.188 (92.188)   Prec@5 99.219 (99.219)   [2019-08-30 23:51:50]
  Epoch: [028][200/391]   Time 0.400 (0.474)   Data 0.000 (0.023)   Loss 0.2788 (0.3060)   Prec@1 91.406 (89.486)   Prec@5 100.000 (99.751)   [2019-08-30 23:53:21]
  **Test** Prec@1 80.400 Prec@5 98.450 Error@1 19.600

==>>[2019-08-30 23:55:12] [Epoch=029/100] [Need: 04:05:39] [learning_rate=0.1000] [Best : Accuracy=84.29, Error=15.71]
  Epoch: [029][000/391]   Time 5.343 (5.343)   Data 4.812 (4.812)   Loss 0.2558 (0.2558)   Prec@1 92.188 (92.188)   Prec@5 99.219 (99.219)   [2019-08-30 23:55:17]
  Epoch: [029][200/391]   Time 0.323 (0.465)   Data 0.000 (0.024)   Loss 0.2620 (0.2979)   Prec@1 87.500 (89.743)   Prec@5 100.000 (99.685)   [2019-08-30 23:56:45]
  **Test** Prec@1 75.180 Prec@5 98.320 Error@1 24.820

==>>[2019-08-30 23:58:34] [Epoch=030/100] [Need: 04:02:01] [learning_rate=0.0100] [Best : Accuracy=84.29, Error=15.71]
  Epoch: [030][000/391]   Time 5.229 (5.229)   Data 4.619 (4.619)   Loss 0.2622 (0.2622)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2019-08-30 23:58:40]
  Epoch: [030][200/391]   Time 0.333 (0.467)   Data 0.000 (0.023)   Loss 0.2747 (0.2324)   Prec@1 90.625 (91.857)   Prec@5 100.000 (99.786)   [2019-08-31 00:00:08]
  **Test** Prec@1 89.880 Prec@5 99.730 Error@1 10.120

==>>[2019-08-31 00:02:04] [Epoch=031/100] [Need: 03:58:38] [learning_rate=0.0100] [Best : Accuracy=89.88, Error=10.12]
  Epoch: [031][000/391]   Time 5.796 (5.796)   Data 5.220 (5.220)   Loss 0.2207 (0.2207)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-31 00:02:10]
  Epoch: [031][200/391]   Time 0.260 (0.470)   Data 0.000 (0.026)   Loss 0.1474 (0.1877)   Prec@1 94.531 (93.614)   Prec@5 100.000 (99.891)   [2019-08-31 00:03:39]
  **Test** Prec@1 89.690 Prec@5 99.740 Error@1 10.310

==>>[2019-08-31 00:05:29] [Epoch=032/100] [Need: 03:55:06] [learning_rate=0.0100] [Best : Accuracy=89.88, Error=10.12]
  Epoch: [032][000/391]   Time 4.914 (4.914)   Data 4.444 (4.444)   Loss 0.1822 (0.1822)   Prec@1 94.531 (94.531)   Prec@5 99.219 (99.219)   [2019-08-31 00:05:34]
  Epoch: [032][200/391]   Time 0.573 (0.478)   Data 0.000 (0.022)   Loss 0.2522 (0.1839)   Prec@1 90.625 (93.641)   Prec@5 100.000 (99.872)   [2019-08-31 00:07:05]
  **Test** Prec@1 89.970 Prec@5 99.750 Error@1 10.030

==>>[2019-08-31 00:08:55] [Epoch=033/100] [Need: 03:51:36] [learning_rate=0.0100] [Best : Accuracy=89.97, Error=10.03]
  Epoch: [033][000/391]   Time 6.448 (6.448)   Data 5.774 (5.774)   Loss 0.1135 (0.1135)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 00:09:02]
  Epoch: [033][200/391]   Time 0.447 (0.471)   Data 0.000 (0.029)   Loss 0.1677 (0.1723)   Prec@1 92.969 (94.100)   Prec@5 100.000 (99.918)   [2019-08-31 00:10:30]
  **Test** Prec@1 90.100 Prec@5 99.640 Error@1 9.900

==>>[2019-08-31 00:12:22] [Epoch=034/100] [Need: 03:48:08] [learning_rate=0.0100] [Best : Accuracy=90.10, Error=9.90]
  Epoch: [034][000/391]   Time 4.910 (4.910)   Data 4.256 (4.256)   Loss 0.1026 (0.1026)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 00:12:27]
  Epoch: [034][200/391]   Time 0.355 (0.460)   Data 0.000 (0.021)   Loss 0.2019 (0.1717)   Prec@1 95.312 (94.061)   Prec@5 99.219 (99.891)   [2019-08-31 00:13:55]
  **Test** Prec@1 89.580 Prec@5 99.740 Error@1 10.420

==>>[2019-08-31 00:15:49] [Epoch=035/100] [Need: 03:44:39] [learning_rate=0.0100] [Best : Accuracy=90.10, Error=9.90]
  Epoch: [035][000/391]   Time 6.460 (6.460)   Data 5.770 (5.770)   Loss 0.1280 (0.1280)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 00:15:55]
  Epoch: [035][200/391]   Time 0.429 (0.472)   Data 0.001 (0.029)   Loss 0.1671 (0.1645)   Prec@1 92.969 (94.232)   Prec@5 100.000 (99.887)   [2019-08-31 00:17:24]
  **Test** Prec@1 89.740 Prec@5 99.720 Error@1 10.260

==>>[2019-08-31 00:19:17] [Epoch=036/100] [Need: 03:41:13] [learning_rate=0.0100] [Best : Accuracy=90.10, Error=9.90]
  Epoch: [036][000/391]   Time 5.265 (5.265)   Data 4.613 (4.613)   Loss 0.0822 (0.0822)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:19:23]
  Epoch: [036][200/391]   Time 0.510 (0.468)   Data 0.000 (0.023)   Loss 0.0694 (0.1611)   Prec@1 98.438 (94.551)   Prec@5 100.000 (99.942)   [2019-08-31 00:20:52]
  **Test** Prec@1 90.070 Prec@5 99.700 Error@1 9.930

==>>[2019-08-31 00:22:43] [Epoch=037/100] [Need: 03:37:42] [learning_rate=0.0100] [Best : Accuracy=90.10, Error=9.90]
  Epoch: [037][000/391]   Time 4.694 (4.694)   Data 4.122 (4.122)   Loss 0.1227 (0.1227)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 00:22:48]
  Epoch: [037][200/391]   Time 0.434 (0.459)   Data 0.000 (0.021)   Loss 0.1534 (0.1581)   Prec@1 96.875 (94.566)   Prec@5 100.000 (99.918)   [2019-08-31 00:24:15]
  **Test** Prec@1 90.340 Prec@5 99.670 Error@1 9.660

==>>[2019-08-31 00:26:08] [Epoch=038/100] [Need: 03:34:11] [learning_rate=0.0100] [Best : Accuracy=90.34, Error=9.66]
  Epoch: [038][000/391]   Time 6.823 (6.823)   Data 6.260 (6.260)   Loss 0.1176 (0.1176)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 00:26:15]
  Epoch: [038][200/391]   Time 0.585 (0.487)   Data 0.000 (0.031)   Loss 0.1937 (0.1565)   Prec@1 92.188 (94.644)   Prec@5 100.000 (99.949)   [2019-08-31 00:27:46]
  **Test** Prec@1 90.170 Prec@5 99.750 Error@1 9.830

==>>[2019-08-31 00:29:37] [Epoch=039/100] [Need: 03:30:47] [learning_rate=0.0100] [Best : Accuracy=90.34, Error=9.66]
  Epoch: [039][000/391]   Time 5.182 (5.182)   Data 4.667 (4.667)   Loss 0.1561 (0.1561)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-31 00:29:42]
  Epoch: [039][200/391]   Time 0.413 (0.461)   Data 0.000 (0.024)   Loss 0.1445 (0.1538)   Prec@1 96.094 (94.741)   Prec@5 99.219 (99.918)   [2019-08-31 00:31:09]
  **Test** Prec@1 89.600 Prec@5 99.750 Error@1 10.400

==>>[2019-08-31 00:33:01] [Epoch=040/100] [Need: 03:27:15] [learning_rate=0.0100] [Best : Accuracy=90.34, Error=9.66]
  Epoch: [040][000/391]   Time 5.208 (5.208)   Data 4.821 (4.821)   Loss 0.2080 (0.2080)   Prec@1 92.188 (92.188)   Prec@5 98.438 (98.438)   [2019-08-31 00:33:06]
  Epoch: [040][200/391]   Time 0.668 (0.470)   Data 0.000 (0.024)   Loss 0.2513 (0.1531)   Prec@1 89.844 (94.745)   Prec@5 100.000 (99.949)   [2019-08-31 00:34:36]
  **Test** Prec@1 90.350 Prec@5 99.820 Error@1 9.650

==>>[2019-08-31 00:36:29] [Epoch=041/100] [Need: 03:23:49] [learning_rate=0.0100] [Best : Accuracy=90.35, Error=9.65]
  Epoch: [041][000/391]   Time 4.816 (4.816)   Data 4.400 (4.400)   Loss 0.1263 (0.1263)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-31 00:36:34]
  Epoch: [041][200/391]   Time 0.448 (0.467)   Data 0.000 (0.022)   Loss 0.1867 (0.1499)   Prec@1 92.188 (94.935)   Prec@5 100.000 (99.887)   [2019-08-31 00:38:03]
  **Test** Prec@1 90.010 Prec@5 99.690 Error@1 9.990

==>>[2019-08-31 00:39:52] [Epoch=042/100] [Need: 03:20:16] [learning_rate=0.0100] [Best : Accuracy=90.35, Error=9.65]
  Epoch: [042][000/391]   Time 5.846 (5.846)   Data 5.149 (5.149)   Loss 0.2445 (0.2445)   Prec@1 91.406 (91.406)   Prec@5 99.219 (99.219)   [2019-08-31 00:39:58]
  Epoch: [042][200/391]   Time 0.498 (0.470)   Data 0.000 (0.026)   Loss 0.1204 (0.1521)   Prec@1 95.312 (94.660)   Prec@5 100.000 (99.934)   [2019-08-31 00:41:27]
  **Test** Prec@1 88.970 Prec@5 99.640 Error@1 11.030

==>>[2019-08-31 00:43:22] [Epoch=043/100] [Need: 03:16:52] [learning_rate=0.0100] [Best : Accuracy=90.35, Error=9.65]
  Epoch: [043][000/391]   Time 6.335 (6.335)   Data 5.798 (5.798)   Loss 0.1413 (0.1413)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-31 00:43:28]
  Epoch: [043][200/391]   Time 0.489 (0.474)   Data 0.000 (0.029)   Loss 0.1209 (0.1516)   Prec@1 95.312 (94.675)   Prec@5 100.000 (99.953)   [2019-08-31 00:44:57]
  **Test** Prec@1 90.080 Prec@5 99.690 Error@1 9.920

==>>[2019-08-31 00:46:49] [Epoch=044/100] [Need: 03:13:24] [learning_rate=0.0100] [Best : Accuracy=90.35, Error=9.65]
  Epoch: [044][000/391]   Time 5.549 (5.549)   Data 5.064 (5.064)   Loss 0.1045 (0.1045)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 00:46:54]
  Epoch: [044][200/391]   Time 0.335 (0.466)   Data 0.000 (0.025)   Loss 0.1670 (0.1490)   Prec@1 93.750 (94.819)   Prec@5 100.000 (99.926)   [2019-08-31 00:48:23]
  **Test** Prec@1 89.920 Prec@5 99.530 Error@1 10.080

==>>[2019-08-31 00:50:16] [Epoch=045/100] [Need: 03:09:57] [learning_rate=0.0100] [Best : Accuracy=90.35, Error=9.65]
  Epoch: [045][000/391]   Time 7.120 (7.120)   Data 6.583 (6.583)   Loss 0.1773 (0.1773)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-31 00:50:23]
  Epoch: [045][200/391]   Time 0.510 (0.466)   Data 0.000 (0.033)   Loss 0.1290 (0.1458)   Prec@1 96.094 (94.881)   Prec@5 100.000 (99.918)   [2019-08-31 00:51:49]
  **Test** Prec@1 90.450 Prec@5 99.750 Error@1 9.550

==>>[2019-08-31 00:53:41] [Epoch=046/100] [Need: 03:06:27] [learning_rate=0.0100] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [046][000/391]   Time 6.321 (6.321)   Data 5.701 (5.701)   Loss 0.1126 (0.1126)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 00:53:47]
  Epoch: [046][200/391]   Time 0.296 (0.474)   Data 0.000 (0.029)   Loss 0.2132 (0.1451)   Prec@1 92.188 (94.970)   Prec@5 99.219 (99.953)   [2019-08-31 00:55:16]
  **Test** Prec@1 89.710 Prec@5 99.690 Error@1 10.290

==>>[2019-08-31 00:57:04] [Epoch=047/100] [Need: 03:02:55] [learning_rate=0.0100] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [047][000/391]   Time 7.117 (7.117)   Data 6.419 (6.419)   Loss 0.2078 (0.2078)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-31 00:57:11]
  Epoch: [047][200/391]   Time 0.574 (0.473)   Data 0.000 (0.032)   Loss 0.1704 (0.1462)   Prec@1 92.969 (94.897)   Prec@5 100.000 (99.918)   [2019-08-31 00:58:39]
  **Test** Prec@1 90.330 Prec@5 99.660 Error@1 9.670

==>>[2019-08-31 01:00:29] [Epoch=048/100] [Need: 02:59:25] [learning_rate=0.0100] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [048][000/391]   Time 6.578 (6.578)   Data 5.977 (5.977)   Loss 0.1365 (0.1365)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-31 01:00:35]
  Epoch: [048][200/391]   Time 0.493 (0.478)   Data 0.000 (0.030)   Loss 0.2331 (0.1455)   Prec@1 92.188 (94.846)   Prec@5 99.219 (99.938)   [2019-08-31 01:02:05]
  **Test** Prec@1 89.610 Prec@5 99.720 Error@1 10.390

==>>[2019-08-31 01:03:57] [Epoch=049/100] [Need: 02:56:00] [learning_rate=0.0100] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [049][000/391]   Time 4.853 (4.853)   Data 4.308 (4.308)   Loss 0.0710 (0.0710)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 01:04:02]
  Epoch: [049][200/391]   Time 0.337 (0.431)   Data 0.000 (0.022)   Loss 0.0774 (0.1407)   Prec@1 96.875 (95.153)   Prec@5 100.000 (99.961)   [2019-08-31 01:05:24]
  **Test** Prec@1 90.150 Prec@5 99.690 Error@1 9.850

==>>[2019-08-31 01:07:10] [Epoch=050/100] [Need: 02:52:18] [learning_rate=0.0100] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [050][000/391]   Time 6.210 (6.210)   Data 5.678 (5.678)   Loss 0.0784 (0.0784)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:07:16]
  Epoch: [050][200/391]   Time 0.337 (0.421)   Data 0.000 (0.029)   Loss 0.1603 (0.1466)   Prec@1 92.969 (94.862)   Prec@5 100.000 (99.934)   [2019-08-31 01:08:35]
  **Test** Prec@1 89.080 Prec@5 99.660 Error@1 10.920

==>>[2019-08-31 01:10:16] [Epoch=051/100] [Need: 02:48:32] [learning_rate=0.0100] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [051][000/391]   Time 3.551 (3.551)   Data 3.047 (3.047)   Loss 0.1479 (0.1479)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-31 01:10:20]
  Epoch: [051][200/391]   Time 0.480 (0.428)   Data 0.000 (0.015)   Loss 0.1199 (0.1438)   Prec@1 96.094 (94.959)   Prec@5 100.000 (99.949)   [2019-08-31 01:11:42]
  **Test** Prec@1 89.370 Prec@5 99.660 Error@1 10.630

==>>[2019-08-31 01:13:24] [Epoch=052/100] [Need: 02:44:49] [learning_rate=0.0100] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [052][000/391]   Time 5.596 (5.596)   Data 5.059 (5.059)   Loss 0.1371 (0.1371)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-31 01:13:30]
  Epoch: [052][200/391]   Time 0.359 (0.446)   Data 0.000 (0.025)   Loss 0.1233 (0.1453)   Prec@1 96.094 (94.889)   Prec@5 100.000 (99.961)   [2019-08-31 01:14:54]
  **Test** Prec@1 89.840 Prec@5 99.760 Error@1 10.160

==>>[2019-08-31 01:16:32] [Epoch=053/100] [Need: 02:41:07] [learning_rate=0.0100] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [053][000/391]   Time 5.151 (5.151)   Data 4.714 (4.714)   Loss 0.1766 (0.1766)   Prec@1 93.750 (93.750)   Prec@5 99.219 (99.219)   [2019-08-31 01:16:37]
  Epoch: [053][200/391]   Time 0.359 (0.395)   Data 0.000 (0.024)   Loss 0.1232 (0.1441)   Prec@1 97.656 (95.103)   Prec@5 100.000 (99.938)   [2019-08-31 01:17:51]
  **Test** Prec@1 90.220 Prec@5 99.700 Error@1 9.780

==>>[2019-08-31 01:19:18] [Epoch=054/100] [Need: 02:37:07] [learning_rate=0.0100] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [054][000/391]   Time 4.420 (4.420)   Data 3.973 (3.973)   Loss 0.2001 (0.2001)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-31 01:19:23]
  Epoch: [054][200/391]   Time 0.306 (0.337)   Data 0.000 (0.020)   Loss 0.1935 (0.1420)   Prec@1 93.750 (95.126)   Prec@5 100.000 (99.953)   [2019-08-31 01:20:26]
  **Test** Prec@1 90.190 Prec@5 99.710 Error@1 9.810

==>>[2019-08-31 01:21:27] [Epoch=055/100] [Need: 02:32:41] [learning_rate=0.0100] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [055][000/391]   Time 2.926 (2.926)   Data 2.604 (2.604)   Loss 0.0810 (0.0810)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 01:21:30]
  Epoch: [055][200/391]   Time 0.208 (0.261)   Data 0.000 (0.013)   Loss 0.1662 (0.1439)   Prec@1 92.969 (95.002)   Prec@5 100.000 (99.965)   [2019-08-31 01:22:20]
  **Test** Prec@1 90.000 Prec@5 99.710 Error@1 10.000

==>>[2019-08-31 01:23:08] [Epoch=056/100] [Need: 02:27:56] [learning_rate=0.0100] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [056][000/391]   Time 1.949 (1.949)   Data 1.642 (1.642)   Loss 0.1712 (0.1712)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-31 01:23:10]
  Epoch: [056][200/391]   Time 0.185 (0.216)   Data 0.000 (0.008)   Loss 0.1181 (0.1432)   Prec@1 96.875 (95.052)   Prec@5 100.000 (99.934)   [2019-08-31 01:23:51]
  **Test** Prec@1 90.290 Prec@5 99.680 Error@1 9.710

==>>[2019-08-31 01:24:41] [Epoch=057/100] [Need: 02:23:13] [learning_rate=0.0100] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [057][000/391]   Time 2.119 (2.119)   Data 1.855 (1.855)   Loss 0.1260 (0.1260)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 01:24:43]
  Epoch: [057][200/391]   Time 0.229 (0.213)   Data 0.000 (0.009)   Loss 0.1349 (0.1450)   Prec@1 95.312 (94.939)   Prec@5 100.000 (99.942)   [2019-08-31 01:25:24]
  **Test** Prec@1 89.790 Prec@5 99.640 Error@1 10.210

==>>[2019-08-31 01:26:13] [Epoch=058/100] [Need: 02:18:35] [learning_rate=0.0100] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [058][000/391]   Time 2.031 (2.031)   Data 1.721 (1.721)   Loss 0.1061 (0.1061)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 01:26:15]
  Epoch: [058][200/391]   Time 0.203 (0.208)   Data 0.000 (0.009)   Loss 0.1476 (0.1450)   Prec@1 96.094 (94.904)   Prec@5 100.000 (99.949)   [2019-08-31 01:26:55]
  **Test** Prec@1 89.690 Prec@5 99.690 Error@1 10.310

==>>[2019-08-31 01:27:46] [Epoch=059/100] [Need: 02:14:04] [learning_rate=0.0100] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [059][000/391]   Time 2.193 (2.193)   Data 1.880 (1.880)   Loss 0.1170 (0.1170)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-31 01:27:48]
  Epoch: [059][200/391]   Time 0.160 (0.185)   Data 0.000 (0.010)   Loss 0.1701 (0.1465)   Prec@1 92.969 (94.768)   Prec@5 100.000 (99.946)   [2019-08-31 01:28:23]
  **Test** Prec@1 88.920 Prec@5 99.550 Error@1 11.080

==>>[2019-08-31 01:29:04] [Epoch=060/100] [Need: 02:09:29] [learning_rate=0.0010] [Best : Accuracy=90.45, Error=9.55]
  Epoch: [060][000/391]   Time 1.773 (1.773)   Data 1.555 (1.555)   Loss 0.1619 (0.1619)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-31 01:29:06]
  Epoch: [060][200/391]   Time 0.163 (0.180)   Data 0.000 (0.008)   Loss 0.1880 (0.1293)   Prec@1 92.188 (95.612)   Prec@5 100.000 (99.961)   [2019-08-31 01:29:40]
  **Test** Prec@1 91.110 Prec@5 99.660 Error@1 8.890

==>>[2019-08-31 01:30:21] [Epoch=061/100] [Need: 02:05:00] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [061][000/391]   Time 1.792 (1.792)   Data 1.538 (1.538)   Loss 0.0914 (0.0914)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:30:23]
  Epoch: [061][200/391]   Time 0.168 (0.182)   Data 0.000 (0.008)   Loss 0.1477 (0.1150)   Prec@1 96.094 (96.098)   Prec@5 100.000 (99.973)   [2019-08-31 01:30:58]
  **Test** Prec@1 91.030 Prec@5 99.670 Error@1 8.970

==>>[2019-08-31 01:31:38] [Epoch=062/100] [Need: 02:00:36] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [062][000/391]   Time 1.781 (1.781)   Data 1.571 (1.571)   Loss 0.1003 (0.1003)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 01:31:39]
  Epoch: [062][200/391]   Time 0.192 (0.182)   Data 0.000 (0.008)   Loss 0.1063 (0.1185)   Prec@1 96.875 (95.853)   Prec@5 100.000 (99.969)   [2019-08-31 01:32:14]
  **Test** Prec@1 90.980 Prec@5 99.650 Error@1 9.020

==>>[2019-08-31 01:32:54] [Epoch=063/100] [Need: 01:56:19] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [063][000/391]   Time 1.717 (1.717)   Data 1.508 (1.508)   Loss 0.0850 (0.0850)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:32:56]
  Epoch: [063][200/391]   Time 0.148 (0.182)   Data 0.000 (0.008)   Loss 0.1122 (0.1076)   Prec@1 96.875 (96.335)   Prec@5 100.000 (99.973)   [2019-08-31 01:33:30]
  **Test** Prec@1 91.060 Prec@5 99.710 Error@1 8.940

==>>[2019-08-31 01:34:11] [Epoch=064/100] [Need: 01:52:07] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [064][000/391]   Time 1.834 (1.834)   Data 1.580 (1.580)   Loss 0.1006 (0.1006)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 01:34:12]
  Epoch: [064][200/391]   Time 0.178 (0.181)   Data 0.000 (0.008)   Loss 0.0694 (0.1079)   Prec@1 98.438 (96.358)   Prec@5 100.000 (99.984)   [2019-08-31 01:34:47]
  **Test** Prec@1 91.000 Prec@5 99.580 Error@1 9.000

==>>[2019-08-31 01:35:27] [Epoch=065/100] [Need: 01:48:01] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [065][000/391]   Time 1.753 (1.753)   Data 1.525 (1.525)   Loss 0.1373 (0.1373)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-31 01:35:29]
  Epoch: [065][200/391]   Time 0.163 (0.181)   Data 0.000 (0.008)   Loss 0.0976 (0.1093)   Prec@1 96.875 (96.276)   Prec@5 100.000 (99.969)   [2019-08-31 01:36:03]
  **Test** Prec@1 90.640 Prec@5 99.670 Error@1 9.360

==>>[2019-08-31 01:36:44] [Epoch=066/100] [Need: 01:44:00] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [066][000/391]   Time 1.865 (1.865)   Data 1.632 (1.632)   Loss 0.1329 (0.1329)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-31 01:36:46]
  Epoch: [066][200/391]   Time 0.170 (0.182)   Data 0.000 (0.008)   Loss 0.1666 (0.1056)   Prec@1 95.312 (96.424)   Prec@5 100.000 (99.957)   [2019-08-31 01:37:20]
  **Test** Prec@1 90.990 Prec@5 99.710 Error@1 9.010

==>>[2019-08-31 01:38:01] [Epoch=067/100] [Need: 01:40:04] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [067][000/391]   Time 1.836 (1.836)   Data 1.605 (1.605)   Loss 0.1108 (0.1108)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 01:38:03]
  Epoch: [067][200/391]   Time 0.166 (0.182)   Data 0.000 (0.008)   Loss 0.1093 (0.1085)   Prec@1 96.094 (96.269)   Prec@5 100.000 (99.953)   [2019-08-31 01:38:37]
  **Test** Prec@1 91.080 Prec@5 99.690 Error@1 8.920

==>>[2019-08-31 01:39:18] [Epoch=068/100] [Need: 01:36:12] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [068][000/391]   Time 1.825 (1.825)   Data 1.594 (1.594)   Loss 0.1484 (0.1484)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-31 01:39:19]
  Epoch: [068][200/391]   Time 0.183 (0.181)   Data 0.000 (0.008)   Loss 0.0990 (0.1081)   Prec@1 96.094 (96.284)   Prec@5 100.000 (99.969)   [2019-08-31 01:39:54]
  **Test** Prec@1 90.690 Prec@5 99.750 Error@1 9.310

==>>[2019-08-31 01:40:34] [Epoch=069/100] [Need: 01:32:26] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [069][000/391]   Time 1.825 (1.825)   Data 1.592 (1.592)   Loss 0.1208 (0.1208)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-31 01:40:36]
  Epoch: [069][200/391]   Time 0.178 (0.182)   Data 0.000 (0.008)   Loss 0.1634 (0.1037)   Prec@1 95.312 (96.490)   Prec@5 100.000 (99.981)   [2019-08-31 01:41:11]
  **Test** Prec@1 90.760 Prec@5 99.690 Error@1 9.240

==>>[2019-08-31 01:41:51] [Epoch=070/100] [Need: 01:28:43] [learning_rate=0.0010] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [070][000/391]   Time 1.796 (1.796)   Data 1.551 (1.551)   Loss 0.0488 (0.0488)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2019-08-31 01:41:53]
  Epoch: [070][200/391]   Time 0.195 (0.182)   Data 0.000 (0.008)   Loss 0.1337 (0.1067)   Prec@1 93.750 (96.346)   Prec@5 100.000 (99.973)   [2019-08-31 01:42:28]
  **Test** Prec@1 91.290 Prec@5 99.720 Error@1 8.710

==>>[2019-08-31 01:43:09] [Epoch=071/100] [Need: 01:25:05] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [071][000/391]   Time 1.782 (1.782)   Data 1.525 (1.525)   Loss 0.0568 (0.0568)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 01:43:10]
  Epoch: [071][200/391]   Time 0.165 (0.182)   Data 0.000 (0.008)   Loss 0.0839 (0.1072)   Prec@1 96.094 (96.296)   Prec@5 100.000 (99.965)   [2019-08-31 01:43:45]
  **Test** Prec@1 90.990 Prec@5 99.690 Error@1 9.010

==>>[2019-08-31 01:44:25] [Epoch=072/100] [Need: 01:21:30] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [072][000/391]   Time 1.759 (1.759)   Data 1.560 (1.560)   Loss 0.0629 (0.0629)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2019-08-31 01:44:27]
  Epoch: [072][200/391]   Time 0.160 (0.181)   Data 0.000 (0.008)   Loss 0.0540 (0.1008)   Prec@1 98.438 (96.692)   Prec@5 100.000 (99.992)   [2019-08-31 01:45:02]
  **Test** Prec@1 90.630 Prec@5 99.680 Error@1 9.370

==>>[2019-08-31 01:45:42] [Epoch=073/100] [Need: 01:17:59] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [073][000/391]   Time 1.780 (1.780)   Data 1.558 (1.558)   Loss 0.1677 (0.1677)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-08-31 01:45:44]
  Epoch: [073][200/391]   Time 0.152 (0.180)   Data 0.000 (0.008)   Loss 0.1048 (0.1010)   Prec@1 95.312 (96.502)   Prec@5 100.000 (99.981)   [2019-08-31 01:46:18]
  **Test** Prec@1 91.240 Prec@5 99.740 Error@1 8.760

==>>[2019-08-31 01:46:59] [Epoch=074/100] [Need: 01:14:32] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [074][000/391]   Time 1.850 (1.850)   Data 1.612 (1.612)   Loss 0.0833 (0.0833)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:47:01]
  Epoch: [074][200/391]   Time 0.158 (0.181)   Data 0.000 (0.008)   Loss 0.0633 (0.1032)   Prec@1 97.656 (96.502)   Prec@5 100.000 (99.977)   [2019-08-31 01:47:36]
  **Test** Prec@1 91.010 Prec@5 99.760 Error@1 8.990

==>>[2019-08-31 01:48:16] [Epoch=075/100] [Need: 01:11:08] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [075][000/391]   Time 1.827 (1.827)   Data 1.574 (1.574)   Loss 0.0863 (0.0863)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:48:18]
  Epoch: [075][200/391]   Time 0.169 (0.185)   Data 0.000 (0.008)   Loss 0.0723 (0.1028)   Prec@1 97.656 (96.482)   Prec@5 100.000 (99.988)   [2019-08-31 01:48:53]
  **Test** Prec@1 90.670 Prec@5 99.690 Error@1 9.330

==>>[2019-08-31 01:49:34] [Epoch=076/100] [Need: 01:07:48] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [076][000/391]   Time 1.808 (1.808)   Data 1.570 (1.570)   Loss 0.1163 (0.1163)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:49:36]
  Epoch: [076][200/391]   Time 0.171 (0.181)   Data 0.000 (0.008)   Loss 0.0765 (0.1029)   Prec@1 96.875 (96.498)   Prec@5 100.000 (99.965)   [2019-08-31 01:50:10]
  **Test** Prec@1 90.640 Prec@5 99.750 Error@1 9.360

==>>[2019-08-31 01:50:50] [Epoch=077/100] [Need: 01:04:31] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [077][000/391]   Time 1.820 (1.820)   Data 1.562 (1.562)   Loss 0.0965 (0.0965)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 01:50:52]
  Epoch: [077][200/391]   Time 0.163 (0.184)   Data 0.000 (0.008)   Loss 0.0879 (0.0979)   Prec@1 98.438 (96.712)   Prec@5 100.000 (99.957)   [2019-08-31 01:51:27]
  **Test** Prec@1 90.830 Prec@5 99.670 Error@1 9.170

==>>[2019-08-31 01:52:08] [Epoch=078/100] [Need: 01:01:17] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [078][000/391]   Time 1.745 (1.745)   Data 1.507 (1.507)   Loss 0.1121 (0.1121)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:52:09]
  Epoch: [078][200/391]   Time 0.162 (0.182)   Data 0.000 (0.008)   Loss 0.0602 (0.1009)   Prec@1 98.438 (96.432)   Prec@5 100.000 (99.977)   [2019-08-31 01:52:44]
  **Test** Prec@1 90.410 Prec@5 99.770 Error@1 9.590

==>>[2019-08-31 01:53:25] [Epoch=079/100] [Need: 00:58:06] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [079][000/391]   Time 1.788 (1.788)   Data 1.555 (1.555)   Loss 0.1178 (0.1178)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 01:53:27]
  Epoch: [079][200/391]   Time 0.208 (0.181)   Data 0.000 (0.008)   Loss 0.1180 (0.1010)   Prec@1 94.531 (96.502)   Prec@5 100.000 (99.977)   [2019-08-31 01:54:01]
  **Test** Prec@1 90.610 Prec@5 99.750 Error@1 9.390

==>>[2019-08-31 01:54:41] [Epoch=080/100] [Need: 00:54:57] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [080][000/391]   Time 1.787 (1.787)   Data 1.541 (1.541)   Loss 0.0931 (0.0931)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 01:54:43]
  Epoch: [080][200/391]   Time 0.173 (0.182)   Data 0.000 (0.008)   Loss 0.1573 (0.1033)   Prec@1 96.094 (96.517)   Prec@5 100.000 (99.926)   [2019-08-31 01:55:18]
  **Test** Prec@1 90.570 Prec@5 99.730 Error@1 9.430

==>>[2019-08-31 01:55:58] [Epoch=081/100] [Need: 00:51:52] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [081][000/391]   Time 1.744 (1.744)   Data 1.513 (1.513)   Loss 0.0625 (0.0625)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 01:55:59]
  Epoch: [081][200/391]   Time 0.167 (0.180)   Data 0.000 (0.008)   Loss 0.0800 (0.1024)   Prec@1 96.094 (96.463)   Prec@5 100.000 (99.961)   [2019-08-31 01:56:34]
  **Test** Prec@1 90.760 Prec@5 99.710 Error@1 9.240

==>>[2019-08-31 01:57:14] [Epoch=082/100] [Need: 00:48:49] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [082][000/391]   Time 1.769 (1.769)   Data 1.534 (1.534)   Loss 0.1293 (0.1293)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-31 01:57:16]
  Epoch: [082][200/391]   Time 0.199 (0.181)   Data 0.000 (0.008)   Loss 0.0523 (0.1026)   Prec@1 97.656 (96.459)   Prec@5 100.000 (99.981)   [2019-08-31 01:57:50]
  **Test** Prec@1 90.780 Prec@5 99.700 Error@1 9.220

==>>[2019-08-31 01:58:31] [Epoch=083/100] [Need: 00:45:48] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [083][000/391]   Time 1.736 (1.736)   Data 1.520 (1.520)   Loss 0.1088 (0.1088)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 01:58:33]
  Epoch: [083][200/391]   Time 0.194 (0.182)   Data 0.000 (0.008)   Loss 0.0857 (0.0996)   Prec@1 96.094 (96.583)   Prec@5 100.000 (99.965)   [2019-08-31 01:59:08]
  **Test** Prec@1 90.690 Prec@5 99.780 Error@1 9.310

==>>[2019-08-31 01:59:49] [Epoch=084/100] [Need: 00:42:51] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [084][000/391]   Time 1.773 (1.773)   Data 1.520 (1.520)   Loss 0.0949 (0.0949)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 01:59:50]
  Epoch: [084][200/391]   Time 0.167 (0.179)   Data 0.000 (0.008)   Loss 0.1616 (0.1037)   Prec@1 92.969 (96.393)   Prec@5 100.000 (99.969)   [2019-08-31 02:00:25]
  **Test** Prec@1 90.770 Prec@5 99.730 Error@1 9.230

==>>[2019-08-31 02:01:05] [Epoch=085/100] [Need: 00:39:55] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [085][000/391]   Time 1.807 (1.807)   Data 1.563 (1.563)   Loss 0.0721 (0.0721)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 02:01:07]
  Epoch: [085][200/391]   Time 0.180 (0.179)   Data 0.000 (0.008)   Loss 0.0734 (0.0966)   Prec@1 97.656 (96.716)   Prec@5 100.000 (99.977)   [2019-08-31 02:01:42]
  **Test** Prec@1 90.100 Prec@5 99.740 Error@1 9.900

==>>[2019-08-31 02:02:22] [Epoch=086/100] [Need: 00:37:02] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [086][000/391]   Time 1.783 (1.783)   Data 1.563 (1.563)   Loss 0.1801 (0.1801)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-31 02:02:24]
  Epoch: [086][200/391]   Time 0.196 (0.182)   Data 0.000 (0.008)   Loss 0.1609 (0.0985)   Prec@1 92.969 (96.591)   Prec@5 100.000 (99.977)   [2019-08-31 02:02:59]
  **Test** Prec@1 90.780 Prec@5 99.790 Error@1 9.220

==>>[2019-08-31 02:03:39] [Epoch=087/100] [Need: 00:34:11] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [087][000/391]   Time 1.762 (1.762)   Data 1.565 (1.565)   Loss 0.0780 (0.0780)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 02:03:41]
  Epoch: [087][200/391]   Time 0.212 (0.181)   Data 0.000 (0.008)   Loss 0.1284 (0.1008)   Prec@1 94.531 (96.607)   Prec@5 100.000 (99.961)   [2019-08-31 02:04:16]
  **Test** Prec@1 90.890 Prec@5 99.710 Error@1 9.110

==>>[2019-08-31 02:04:56] [Epoch=088/100] [Need: 00:31:22] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [088][000/391]   Time 1.769 (1.769)   Data 1.543 (1.543)   Loss 0.1604 (0.1604)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 02:04:58]
  Epoch: [088][200/391]   Time 0.180 (0.182)   Data 0.000 (0.008)   Loss 0.1941 (0.1062)   Prec@1 92.188 (96.358)   Prec@5 100.000 (99.973)   [2019-08-31 02:05:33]
  **Test** Prec@1 90.550 Prec@5 99.640 Error@1 9.450

==>>[2019-08-31 02:06:13] [Epoch=089/100] [Need: 00:28:35] [learning_rate=0.0010] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [089][000/391]   Time 1.764 (1.764)   Data 1.548 (1.548)   Loss 0.1497 (0.1497)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-31 02:06:15]
  Epoch: [089][200/391]   Time 0.164 (0.182)   Data 0.000 (0.008)   Loss 0.1185 (0.1046)   Prec@1 94.531 (96.350)   Prec@5 100.000 (99.949)   [2019-08-31 02:06:49]
  **Test** Prec@1 90.020 Prec@5 99.700 Error@1 9.980

==>>[2019-08-31 02:07:30] [Epoch=090/100] [Need: 00:25:51] [learning_rate=0.0001] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [090][000/391]   Time 1.762 (1.762)   Data 1.557 (1.557)   Loss 0.0646 (0.0646)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 02:07:32]
  Epoch: [090][200/391]   Time 0.167 (0.178)   Data 0.000 (0.008)   Loss 0.0900 (0.0959)   Prec@1 96.094 (96.727)   Prec@5 100.000 (99.965)   [2019-08-31 02:08:06]
  **Test** Prec@1 91.140 Prec@5 99.720 Error@1 8.860

==>>[2019-08-31 02:08:46] [Epoch=091/100] [Need: 00:23:08] [learning_rate=0.0001] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [091][000/391]   Time 1.842 (1.842)   Data 1.610 (1.610)   Loss 0.0628 (0.0628)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 02:08:48]
  Epoch: [091][200/391]   Time 0.165 (0.182)   Data 0.000 (0.008)   Loss 0.0726 (0.0980)   Prec@1 97.656 (96.603)   Prec@5 100.000 (99.973)   [2019-08-31 02:09:23]
  **Test** Prec@1 91.170 Prec@5 99.750 Error@1 8.830

==>>[2019-08-31 02:10:03] [Epoch=092/100] [Need: 00:20:27] [learning_rate=0.0001] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [092][000/391]   Time 1.803 (1.803)   Data 1.581 (1.581)   Loss 0.0734 (0.0734)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 02:10:05]
  Epoch: [092][200/391]   Time 0.132 (0.182)   Data 0.000 (0.008)   Loss 0.1175 (0.0910)   Prec@1 95.312 (96.929)   Prec@5 100.000 (99.977)   [2019-08-31 02:10:40]
  **Test** Prec@1 91.110 Prec@5 99.700 Error@1 8.890

==>>[2019-08-31 02:11:20] [Epoch=093/100] [Need: 00:17:48] [learning_rate=0.0001] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [093][000/391]   Time 1.813 (1.813)   Data 1.597 (1.597)   Loss 0.0262 (0.0262)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2019-08-31 02:11:22]
  Epoch: [093][200/391]   Time 0.182 (0.183)   Data 0.000 (0.008)   Loss 0.1629 (0.0925)   Prec@1 92.969 (96.824)   Prec@5 100.000 (99.977)   [2019-08-31 02:11:57]
  **Test** Prec@1 91.110 Prec@5 99.710 Error@1 8.890

==>>[2019-08-31 02:12:37] [Epoch=094/100] [Need: 00:15:10] [learning_rate=0.0001] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [094][000/391]   Time 1.863 (1.863)   Data 1.624 (1.624)   Loss 0.1027 (0.1027)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 02:12:39]
  Epoch: [094][200/391]   Time 0.180 (0.184)   Data 0.000 (0.008)   Loss 0.0724 (0.0916)   Prec@1 98.438 (96.961)   Prec@5 99.219 (99.977)   [2019-08-31 02:13:14]
  **Test** Prec@1 91.280 Prec@5 99.680 Error@1 8.720

==>>[2019-08-31 02:13:54] [Epoch=095/100] [Need: 00:12:34] [learning_rate=0.0001] [Best : Accuracy=91.29, Error=8.71]
  Epoch: [095][000/391]   Time 1.833 (1.833)   Data 1.610 (1.610)   Loss 0.0697 (0.0697)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 02:13:56]
  Epoch: [095][200/391]   Time 0.174 (0.182)   Data 0.000 (0.008)   Loss 0.1006 (0.0921)   Prec@1 97.656 (96.914)   Prec@5 100.000 (99.973)   [2019-08-31 02:14:31]
  **Test** Prec@1 91.450 Prec@5 99.750 Error@1 8.550

==>>[2019-08-31 02:15:11] [Epoch=096/100] [Need: 00:10:00] [learning_rate=0.0001] [Best : Accuracy=91.45, Error=8.55]
  Epoch: [096][000/391]   Time 1.849 (1.849)   Data 1.600 (1.600)   Loss 0.1498 (0.1498)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-31 02:15:13]
  Epoch: [096][200/391]   Time 0.172 (0.180)   Data 0.000 (0.008)   Loss 0.1358 (0.0925)   Prec@1 96.094 (96.863)   Prec@5 100.000 (99.977)   [2019-08-31 02:15:47]
  **Test** Prec@1 91.430 Prec@5 99.710 Error@1 8.570

==>>[2019-08-31 02:16:27] [Epoch=097/100] [Need: 00:07:28] [learning_rate=0.0001] [Best : Accuracy=91.45, Error=8.55]
  Epoch: [097][000/391]   Time 1.844 (1.844)   Data 1.601 (1.601)   Loss 0.1483 (0.1483)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 02:16:29]
  Epoch: [097][200/391]   Time 0.150 (0.181)   Data 0.000 (0.008)   Loss 0.0664 (0.0912)   Prec@1 98.438 (96.883)   Prec@5 100.000 (99.981)   [2019-08-31 02:17:03]
  **Test** Prec@1 91.240 Prec@5 99.740 Error@1 8.760

==>>[2019-08-31 02:17:43] [Epoch=098/100] [Need: 00:04:57] [learning_rate=0.0001] [Best : Accuracy=91.45, Error=8.55]
  Epoch: [098][000/391]   Time 1.834 (1.834)   Data 1.595 (1.595)   Loss 0.0437 (0.0437)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 02:17:45]
  Epoch: [098][200/391]   Time 0.216 (0.180)   Data 0.000 (0.008)   Loss 0.1138 (0.0886)   Prec@1 96.875 (96.980)   Prec@5 100.000 (99.981)   [2019-08-31 02:18:19]
  **Test** Prec@1 91.170 Prec@5 99.760 Error@1 8.830

==>>[2019-08-31 02:18:59] [Epoch=099/100] [Need: 00:02:27] [learning_rate=0.0001] [Best : Accuracy=91.45, Error=8.55]
  Epoch: [099][000/391]   Time 1.806 (1.806)   Data 1.586 (1.586)   Loss 0.0897 (0.0897)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 02:19:01]
  Epoch: [099][200/391]   Time 0.181 (0.182)   Data 0.000 (0.008)   Loss 0.0815 (0.0894)   Prec@1 96.875 (96.898)   Prec@5 100.000 (99.988)   [2019-08-31 02:19:36]
  **Test** Prec@1 91.250 Prec@5 99.730 Error@1 8.750
