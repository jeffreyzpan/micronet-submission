save path : ./snapshots/uniform_8_quantized_cifar10_rnqresnet20_100
{'aq_bits': 8, 'aq_type': 'uniform', 'arch': 'rnqresnet20', 'batch_size': 128, 'data_path': '../../datasets/CIFAR10/cifar', 'dataset': 'cifar10', 'decay': 0.0001, 'epochs': 100, 'evaluate': False, 'gammas': [0.1, 0.1, 0.1], 'learning_rate': 0.01, 'manualSeed': 4522, 'momentum': 0.9, 'ngpu': 4, 'pretrained': '', 'print_freq': 200, 'resume': 'snapshots/finetune_cifar10_resnet20_300/model_best.pth.tar', 'save_path': './snapshots/uniform_8_quantized_cifar10_rnqresnet20_100', 'schedule': [30, 60, 90], 'start_epoch': 0, 'use_cuda': True, 'workers': 16}
Random Seed: 4522
python version : 3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]
torch  version : 1.1.0
cudnn  version : 7501
=> creating model 'rnqresnet20'
=> network :
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=16, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): RNQConv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=32, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): RNQConv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (routing): Sequential(
          (0): Linear(in_features=64, out_features=16, bias=True)
          (1): ReLU()
          (2): Linear(in_features=16, out_features=1, bias=True)
          (3): Sigmoid()
        )
      )
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): QuantLinear(in_features=64, out_features=10, bias=True, method=KMeans, nbits=4)
)
=> loading checkpoint 'snapshots/finetune_cifar10_resnet20_300/model_best.pth.tar'
=> loaded checkpoint 'snapshots/finetune_cifar10_resnet20_300/model_best.pth.tar' (epoch 291)

==>>[2019-08-30 20:50:38] [Epoch=000/100] [Need: 00:00:00] [learning_rate=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/391]   Time 47.671 (47.671)   Data 1.306 (1.306)   Loss 0.0694 (0.0694)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 20:51:25]
  Epoch: [000][200/391]   Time 0.492 (0.595)   Data 0.000 (0.007)   Loss 0.1772 (0.1699)   Prec@1 93.750 (94.053)   Prec@5 99.219 (99.922)   [2019-08-30 20:52:37]
  **Test** Prec@1 87.870 Prec@5 99.650 Error@1 12.130

==>>[2019-08-30 20:54:13] [Epoch=001/100] [Need: 05:52:04] [learning_rate=0.0100] [Best : Accuracy=87.87, Error=12.13]
  Epoch: [001][000/391]   Time 4.687 (4.687)   Data 4.143 (4.143)   Loss 0.2296 (0.2296)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2019-08-30 20:54:17]
  Epoch: [001][200/391]   Time 0.363 (0.389)   Data 0.000 (0.021)   Loss 0.1381 (0.1549)   Prec@1 93.750 (94.578)   Prec@5 100.000 (99.942)   [2019-08-30 20:55:31]
  **Test** Prec@1 87.050 Prec@5 99.380 Error@1 12.950

==>>[2019-08-30 20:57:08] [Epoch=002/100] [Need: 05:17:38] [learning_rate=0.0100] [Best : Accuracy=87.87, Error=12.13]
  Epoch: [002][000/391]   Time 3.906 (3.906)   Data 3.649 (3.649)   Loss 0.1340 (0.1340)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 20:57:12]
  Epoch: [002][200/391]   Time 0.252 (0.386)   Data 0.000 (0.018)   Loss 0.1762 (0.1594)   Prec@1 94.531 (94.185)   Prec@5 100.000 (99.918)   [2019-08-30 20:58:26]
  **Test** Prec@1 87.690 Prec@5 99.520 Error@1 12.310

==>>[2019-08-30 21:00:00] [Epoch=003/100] [Need: 05:02:18] [learning_rate=0.0100] [Best : Accuracy=87.87, Error=12.13]
  Epoch: [003][000/391]   Time 3.816 (3.816)   Data 3.331 (3.331)   Loss 0.1423 (0.1423)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 21:00:04]
  Epoch: [003][200/391]   Time 0.442 (0.386)   Data 0.000 (0.017)   Loss 0.1299 (0.1543)   Prec@1 97.656 (94.566)   Prec@5 100.000 (99.942)   [2019-08-30 21:01:18]
  **Test** Prec@1 87.100 Prec@5 99.510 Error@1 12.900

==>>[2019-08-30 21:02:50] [Epoch=004/100] [Need: 04:52:30] [learning_rate=0.0100] [Best : Accuracy=87.87, Error=12.13]
  Epoch: [004][000/391]   Time 4.872 (4.872)   Data 4.517 (4.517)   Loss 0.1223 (0.1223)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 21:02:55]
  Epoch: [004][200/391]   Time 0.327 (0.393)   Data 0.000 (0.023)   Loss 0.2309 (0.1498)   Prec@1 91.406 (94.772)   Prec@5 100.000 (99.961)   [2019-08-30 21:04:09]
  **Test** Prec@1 86.050 Prec@5 99.450 Error@1 13.950

==>>[2019-08-30 21:05:44] [Epoch=005/100] [Need: 04:46:46] [learning_rate=0.0100] [Best : Accuracy=87.87, Error=12.13]
  Epoch: [005][000/391]   Time 3.712 (3.712)   Data 3.217 (3.217)   Loss 0.1309 (0.1309)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 21:05:48]
  Epoch: [005][200/391]   Time 0.399 (0.394)   Data 0.000 (0.016)   Loss 0.1281 (0.1491)   Prec@1 94.531 (94.683)   Prec@5 100.000 (99.926)   [2019-08-30 21:07:04]
  **Test** Prec@1 88.500 Prec@5 99.480 Error@1 11.500

==>>[2019-08-30 21:08:37] [Epoch=006/100] [Need: 04:41:28] [learning_rate=0.0100] [Best : Accuracy=88.50, Error=11.50]
  Epoch: [006][000/391]   Time 4.094 (4.094)   Data 3.605 (3.605)   Loss 0.1040 (0.1040)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:08:41]
  Epoch: [006][200/391]   Time 0.312 (0.386)   Data 0.000 (0.018)   Loss 0.1883 (0.1596)   Prec@1 90.625 (94.224)   Prec@5 100.000 (99.926)   [2019-08-30 21:09:54]
  **Test** Prec@1 87.950 Prec@5 99.580 Error@1 12.050

==>>[2019-08-30 21:11:28] [Epoch=007/100] [Need: 04:36:34] [learning_rate=0.0100] [Best : Accuracy=88.50, Error=11.50]
  Epoch: [007][000/391]   Time 5.215 (5.215)   Data 4.704 (4.704)   Loss 0.2274 (0.2274)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-08-30 21:11:33]
  Epoch: [007][200/391]   Time 0.329 (0.386)   Data 0.000 (0.024)   Loss 0.1879 (0.1522)   Prec@1 95.312 (94.683)   Prec@5 100.000 (99.953)   [2019-08-30 21:12:45]
  **Test** Prec@1 87.940 Prec@5 99.480 Error@1 12.060

==>>[2019-08-30 21:14:21] [Epoch=008/100] [Need: 04:32:39] [learning_rate=0.0100] [Best : Accuracy=88.50, Error=11.50]
  Epoch: [008][000/391]   Time 4.482 (4.482)   Data 4.063 (4.063)   Loss 0.1175 (0.1175)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:14:26]
  Epoch: [008][200/391]   Time 0.268 (0.398)   Data 0.000 (0.021)   Loss 0.1543 (0.1551)   Prec@1 93.750 (94.481)   Prec@5 100.000 (99.903)   [2019-08-30 21:15:41]
  **Test** Prec@1 83.670 Prec@5 99.330 Error@1 16.330

==>>[2019-08-30 21:17:13] [Epoch=009/100] [Need: 04:28:34] [learning_rate=0.0100] [Best : Accuracy=88.50, Error=11.50]
  Epoch: [009][000/391]   Time 4.338 (4.338)   Data 4.014 (4.014)   Loss 0.1678 (0.1678)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 21:17:17]
  Epoch: [009][200/391]   Time 0.467 (0.391)   Data 0.003 (0.022)   Loss 0.2205 (0.1560)   Prec@1 92.969 (94.387)   Prec@5 100.000 (99.946)   [2019-08-30 21:18:32]
  **Test** Prec@1 86.880 Prec@5 99.590 Error@1 13.120

==>>[2019-08-30 21:20:05] [Epoch=010/100] [Need: 04:24:51] [learning_rate=0.0100] [Best : Accuracy=88.50, Error=11.50]
  Epoch: [010][000/391]   Time 4.052 (4.052)   Data 3.454 (3.454)   Loss 0.1299 (0.1299)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:20:09]
  Epoch: [010][200/391]   Time 0.461 (0.388)   Data 0.000 (0.017)   Loss 0.1995 (0.1508)   Prec@1 94.531 (94.815)   Prec@5 100.000 (99.938)   [2019-08-30 21:21:23]
  **Test** Prec@1 86.180 Prec@5 99.410 Error@1 13.820

==>>[2019-08-30 21:22:59] [Epoch=011/100] [Need: 04:21:38] [learning_rate=0.0100] [Best : Accuracy=88.50, Error=11.50]
  Epoch: [011][000/391]   Time 4.183 (4.183)   Data 3.815 (3.815)   Loss 0.1285 (0.1285)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:23:03]
  Epoch: [011][200/391]   Time 0.464 (0.384)   Data 0.000 (0.020)   Loss 0.1362 (0.1527)   Prec@1 94.531 (94.644)   Prec@5 100.000 (99.938)   [2019-08-30 21:24:16]
  **Test** Prec@1 89.260 Prec@5 99.690 Error@1 10.740

==>>[2019-08-30 21:25:51] [Epoch=012/100] [Need: 04:18:06] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [012][000/391]   Time 4.296 (4.296)   Data 3.944 (3.944)   Loss 0.1899 (0.1899)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:25:55]
  Epoch: [012][200/391]   Time 0.321 (0.390)   Data 0.000 (0.020)   Loss 0.2500 (0.1515)   Prec@1 92.969 (94.838)   Prec@5 100.000 (99.926)   [2019-08-30 21:27:09]
  **Test** Prec@1 85.220 Prec@5 99.350 Error@1 14.780

==>>[2019-08-30 21:28:43] [Epoch=013/100] [Need: 04:14:48] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [013][000/391]   Time 4.898 (4.898)   Data 4.379 (4.379)   Loss 0.1669 (0.1669)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2019-08-30 21:28:48]
  Epoch: [013][200/391]   Time 0.357 (0.390)   Data 0.000 (0.022)   Loss 0.1120 (0.1582)   Prec@1 96.094 (94.387)   Prec@5 100.000 (99.953)   [2019-08-30 21:30:02]
  **Test** Prec@1 88.850 Prec@5 99.600 Error@1 11.150

==>>[2019-08-30 21:31:36] [Epoch=014/100] [Need: 04:11:37] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [014][000/391]   Time 3.100 (3.100)   Data 2.621 (2.621)   Loss 0.0893 (0.0893)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:31:40]
  Epoch: [014][200/391]   Time 0.344 (0.383)   Data 0.000 (0.013)   Loss 0.2146 (0.1502)   Prec@1 91.406 (94.605)   Prec@5 100.000 (99.965)   [2019-08-30 21:32:53]
  **Test** Prec@1 86.470 Prec@5 99.430 Error@1 13.530

==>>[2019-08-30 21:34:28] [Epoch=015/100] [Need: 04:08:20] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [015][000/391]   Time 4.863 (4.863)   Data 4.197 (4.197)   Loss 0.1321 (0.1321)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:34:33]
  Epoch: [015][200/391]   Time 0.396 (0.392)   Data 0.000 (0.021)   Loss 0.2235 (0.1519)   Prec@1 93.750 (94.710)   Prec@5 100.000 (99.946)   [2019-08-30 21:35:47]
  **Test** Prec@1 87.000 Prec@5 99.420 Error@1 13.000

==>>[2019-08-30 21:37:23] [Epoch=016/100] [Need: 04:05:19] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [016][000/391]   Time 6.495 (6.495)   Data 5.968 (5.968)   Loss 0.2077 (0.2077)   Prec@1 92.188 (92.188)   Prec@5 99.219 (99.219)   [2019-08-30 21:37:29]
  Epoch: [016][200/391]   Time 0.474 (0.405)   Data 0.000 (0.030)   Loss 0.2031 (0.1576)   Prec@1 93.750 (94.391)   Prec@5 100.000 (99.938)   [2019-08-30 21:38:44]
  **Test** Prec@1 88.620 Prec@5 99.640 Error@1 11.380

==>>[2019-08-30 21:40:17] [Epoch=017/100] [Need: 04:02:19] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [017][000/391]   Time 5.077 (5.077)   Data 4.315 (4.315)   Loss 0.1295 (0.1295)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 21:40:22]
  Epoch: [017][200/391]   Time 0.617 (0.401)   Data 0.000 (0.022)   Loss 0.1425 (0.1522)   Prec@1 92.969 (94.729)   Prec@5 100.000 (99.965)   [2019-08-30 21:41:38]
  **Test** Prec@1 87.450 Prec@5 99.540 Error@1 12.550

==>>[2019-08-30 21:43:12] [Epoch=018/100] [Need: 03:59:22] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [018][000/391]   Time 4.096 (4.096)   Data 3.728 (3.728)   Loss 0.1470 (0.1470)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 21:43:16]
  Epoch: [018][200/391]   Time 0.639 (0.393)   Data 0.000 (0.019)   Loss 0.1305 (0.1563)   Prec@1 94.531 (94.352)   Prec@5 100.000 (99.895)   [2019-08-30 21:44:31]
  **Test** Prec@1 86.380 Prec@5 99.550 Error@1 13.620

==>>[2019-08-30 21:46:07] [Epoch=019/100] [Need: 03:56:24] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [019][000/391]   Time 4.983 (4.983)   Data 4.592 (4.592)   Loss 0.1224 (0.1224)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:46:12]
  Epoch: [019][200/391]   Time 0.428 (0.388)   Data 0.000 (0.023)   Loss 0.2399 (0.1473)   Prec@1 92.188 (94.935)   Prec@5 100.000 (99.911)   [2019-08-30 21:47:25]
  **Test** Prec@1 88.460 Prec@5 99.540 Error@1 11.540

==>>[2019-08-30 21:48:59] [Epoch=020/100] [Need: 03:53:22] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [020][000/391]   Time 3.935 (3.935)   Data 3.488 (3.488)   Loss 0.1375 (0.1375)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 21:49:03]
  Epoch: [020][200/391]   Time 0.546 (0.388)   Data 0.000 (0.018)   Loss 0.2035 (0.1547)   Prec@1 93.750 (94.640)   Prec@5 100.000 (99.973)   [2019-08-30 21:50:17]
  **Test** Prec@1 86.510 Prec@5 99.520 Error@1 13.490

==>>[2019-08-30 21:51:51] [Epoch=021/100] [Need: 03:50:14] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [021][000/391]   Time 4.214 (4.214)   Data 3.841 (3.841)   Loss 0.1288 (0.1288)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:51:55]
  Epoch: [021][200/391]   Time 0.510 (0.401)   Data 0.000 (0.019)   Loss 0.1670 (0.1603)   Prec@1 92.969 (94.306)   Prec@5 100.000 (99.899)   [2019-08-30 21:53:12]
  **Test** Prec@1 89.010 Prec@5 99.560 Error@1 10.990

==>>[2019-08-30 21:54:44] [Epoch=022/100] [Need: 03:47:13] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [022][000/391]   Time 4.760 (4.760)   Data 4.303 (4.303)   Loss 0.1135 (0.1135)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 21:54:49]
  Epoch: [022][200/391]   Time 0.430 (0.409)   Data 0.000 (0.022)   Loss 0.1213 (0.1521)   Prec@1 96.094 (94.628)   Prec@5 100.000 (99.926)   [2019-08-30 21:56:06]
  **Test** Prec@1 88.280 Prec@5 99.470 Error@1 11.720

==>>[2019-08-30 21:57:40] [Epoch=023/100] [Need: 03:44:23] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [023][000/391]   Time 4.096 (4.096)   Data 3.753 (3.753)   Loss 0.1463 (0.1463)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 21:57:45]
  Epoch: [023][200/391]   Time 0.361 (0.397)   Data 0.000 (0.019)   Loss 0.1751 (0.1551)   Prec@1 93.750 (94.430)   Prec@5 100.000 (99.926)   [2019-08-30 21:59:00]
  **Test** Prec@1 86.790 Prec@5 99.280 Error@1 13.210

==>>[2019-08-30 22:00:33] [Epoch=024/100] [Need: 03:41:20] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [024][000/391]   Time 4.877 (4.877)   Data 4.231 (4.231)   Loss 0.1704 (0.1704)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 22:00:38]
  Epoch: [024][200/391]   Time 0.347 (0.403)   Data 0.000 (0.021)   Loss 0.1862 (0.1557)   Prec@1 92.188 (94.461)   Prec@5 99.219 (99.934)   [2019-08-30 22:01:54]
  **Test** Prec@1 88.030 Prec@5 99.580 Error@1 11.970

==>>[2019-08-30 22:03:24] [Epoch=025/100] [Need: 03:38:14] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [025][000/391]   Time 3.690 (3.690)   Data 3.208 (3.208)   Loss 0.0829 (0.0829)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 22:03:27]
  Epoch: [025][200/391]   Time 0.540 (0.368)   Data 0.000 (0.016)   Loss 0.2487 (0.1568)   Prec@1 90.625 (94.523)   Prec@5 100.000 (99.930)   [2019-08-30 22:04:37]
  **Test** Prec@1 88.300 Prec@5 99.630 Error@1 11.700

==>>[2019-08-30 22:06:03] [Epoch=026/100] [Need: 03:34:37] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [026][000/391]   Time 3.884 (3.884)   Data 3.421 (3.421)   Loss 0.1268 (0.1268)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 22:06:07]
  Epoch: [026][200/391]   Time 0.367 (0.371)   Data 0.000 (0.017)   Loss 0.2169 (0.1587)   Prec@1 92.969 (94.446)   Prec@5 100.000 (99.942)   [2019-08-30 22:07:17]
  **Test** Prec@1 85.500 Prec@5 99.290 Error@1 14.500

==>>[2019-08-30 22:08:43] [Epoch=027/100] [Need: 03:31:04] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [027][000/391]   Time 3.408 (3.408)   Data 3.022 (3.022)   Loss 0.0819 (0.0819)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 22:08:46]
  Epoch: [027][200/391]   Time 0.358 (0.372)   Data 0.000 (0.015)   Loss 0.1283 (0.1485)   Prec@1 95.312 (94.722)   Prec@5 100.000 (99.949)   [2019-08-30 22:09:58]
  **Test** Prec@1 85.320 Prec@5 98.940 Error@1 14.680

==>>[2019-08-30 22:11:26] [Epoch=028/100] [Need: 03:27:43] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [028][000/391]   Time 4.342 (4.342)   Data 3.707 (3.707)   Loss 0.0993 (0.0993)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 22:11:30]
  Epoch: [028][200/391]   Time 0.419 (0.377)   Data 0.000 (0.019)   Loss 0.2541 (0.1531)   Prec@1 92.188 (94.504)   Prec@5 99.219 (99.934)   [2019-08-30 22:12:41]
  **Test** Prec@1 88.080 Prec@5 99.530 Error@1 11.920

==>>[2019-08-30 22:14:05] [Epoch=029/100] [Need: 03:24:16] [learning_rate=0.0100] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [029][000/391]   Time 2.790 (2.790)   Data 2.394 (2.394)   Loss 0.1843 (0.1843)   Prec@1 92.969 (92.969)   Prec@5 99.219 (99.219)   [2019-08-30 22:14:08]
  Epoch: [029][200/391]   Time 0.210 (0.366)   Data 0.000 (0.012)   Loss 0.1147 (0.1523)   Prec@1 95.312 (94.609)   Prec@5 100.000 (99.911)   [2019-08-30 22:15:18]
  **Test** Prec@1 88.250 Prec@5 99.550 Error@1 11.750

==>>[2019-08-30 22:16:49] [Epoch=030/100] [Need: 03:21:04] [learning_rate=0.0010] [Best : Accuracy=89.26, Error=10.74]
  Epoch: [030][000/391]   Time 4.136 (4.136)   Data 3.637 (3.637)   Loss 0.1198 (0.1198)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 22:16:53]
  Epoch: [030][200/391]   Time 0.350 (0.403)   Data 0.000 (0.018)   Loss 0.1694 (0.1247)   Prec@1 95.312 (95.585)   Prec@5 100.000 (99.965)   [2019-08-30 22:18:10]
  **Test** Prec@1 90.480 Prec@5 99.700 Error@1 9.520

==>>[2019-08-30 22:19:43] [Epoch=031/100] [Need: 03:18:16] [learning_rate=0.0010] [Best : Accuracy=90.48, Error=9.52]
  Epoch: [031][000/391]   Time 4.594 (4.594)   Data 4.081 (4.081)   Loss 0.0841 (0.0841)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 22:19:48]
  Epoch: [031][200/391]   Time 0.408 (0.418)   Data 0.000 (0.021)   Loss 0.0777 (0.1039)   Prec@1 96.875 (96.444)   Prec@5 100.000 (99.981)   [2019-08-30 22:21:07]
  **Test** Prec@1 90.620 Prec@5 99.690 Error@1 9.380

==>>[2019-08-30 22:22:40] [Epoch=032/100] [Need: 03:15:32] [learning_rate=0.0010] [Best : Accuracy=90.62, Error=9.38]
  Epoch: [032][000/391]   Time 3.141 (3.141)   Data 2.852 (2.852)   Loss 0.0863 (0.0863)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 22:22:43]
  Epoch: [032][200/391]   Time 0.284 (0.397)   Data 0.000 (0.015)   Loss 0.1435 (0.0990)   Prec@1 95.312 (96.766)   Prec@5 100.000 (99.996)   [2019-08-30 22:24:00]
  **Test** Prec@1 90.570 Prec@5 99.710 Error@1 9.430

==>>[2019-08-30 22:25:31] [Epoch=033/100] [Need: 03:12:36] [learning_rate=0.0010] [Best : Accuracy=90.62, Error=9.38]
  Epoch: [033][000/391]   Time 3.647 (3.647)   Data 3.338 (3.338)   Loss 0.0795 (0.0795)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 22:25:35]
  Epoch: [033][200/391]   Time 0.546 (0.406)   Data 0.000 (0.017)   Loss 0.0859 (0.0986)   Prec@1 97.656 (96.731)   Prec@5 100.000 (99.981)   [2019-08-30 22:26:53]
  **Test** Prec@1 90.590 Prec@5 99.640 Error@1 9.410

==>>[2019-08-30 22:28:25] [Epoch=034/100] [Need: 03:09:48] [learning_rate=0.0010] [Best : Accuracy=90.62, Error=9.38]
  Epoch: [034][000/391]   Time 3.337 (3.337)   Data 2.855 (2.855)   Loss 0.1593 (0.1593)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2019-08-30 22:28:29]
  Epoch: [034][200/391]   Time 0.417 (0.396)   Data 0.000 (0.014)   Loss 0.0928 (0.0933)   Prec@1 97.656 (96.891)   Prec@5 100.000 (99.977)   [2019-08-30 22:29:45]
  **Test** Prec@1 90.340 Prec@5 99.710 Error@1 9.660

==>>[2019-08-30 22:31:15] [Epoch=035/100] [Need: 03:06:50] [learning_rate=0.0010] [Best : Accuracy=90.62, Error=9.38]
  Epoch: [035][000/391]   Time 3.447 (3.447)   Data 2.928 (2.928)   Loss 0.0655 (0.0655)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 22:31:19]
  Epoch: [035][200/391]   Time 0.344 (0.408)   Data 0.000 (0.015)   Loss 0.0796 (0.0938)   Prec@1 98.438 (96.875)   Prec@5 100.000 (99.977)   [2019-08-30 22:32:37]
  **Test** Prec@1 91.140 Prec@5 99.710 Error@1 8.860

==>>[2019-08-30 22:34:08] [Epoch=036/100] [Need: 03:03:59] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [036][000/391]   Time 3.877 (3.877)   Data 3.473 (3.473)   Loss 0.1082 (0.1082)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 22:34:12]
  Epoch: [036][200/391]   Time 0.504 (0.406)   Data 0.000 (0.018)   Loss 0.1051 (0.0921)   Prec@1 96.875 (96.824)   Prec@5 100.000 (99.977)   [2019-08-30 22:35:30]
  **Test** Prec@1 90.820 Prec@5 99.740 Error@1 9.180

==>>[2019-08-30 22:37:00] [Epoch=037/100] [Need: 03:01:05] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [037][000/391]   Time 4.276 (4.276)   Data 3.776 (3.776)   Loss 0.0659 (0.0659)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 22:37:05]
  Epoch: [037][200/391]   Time 0.524 (0.409)   Data 0.000 (0.019)   Loss 0.0798 (0.0899)   Prec@1 97.656 (97.007)   Prec@5 100.000 (99.984)   [2019-08-30 22:38:23]
  **Test** Prec@1 90.750 Prec@5 99.720 Error@1 9.250

==>>[2019-08-30 22:39:53] [Epoch=038/100] [Need: 02:58:13] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [038][000/391]   Time 4.510 (4.510)   Data 4.031 (4.031)   Loss 0.1362 (0.1362)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 22:39:58]
  Epoch: [038][200/391]   Time 0.401 (0.405)   Data 0.000 (0.020)   Loss 0.0590 (0.0902)   Prec@1 98.438 (97.011)   Prec@5 100.000 (99.996)   [2019-08-30 22:41:15]
  **Test** Prec@1 90.670 Prec@5 99.640 Error@1 9.330

==>>[2019-08-30 22:42:46] [Epoch=039/100] [Need: 02:55:22] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [039][000/391]   Time 5.692 (5.692)   Data 5.225 (5.225)   Loss 0.0538 (0.0538)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 22:42:52]
  Epoch: [039][200/391]   Time 0.261 (0.425)   Data 0.000 (0.026)   Loss 0.1191 (0.0926)   Prec@1 98.438 (96.949)   Prec@5 100.000 (99.981)   [2019-08-30 22:44:12]
  **Test** Prec@1 90.770 Prec@5 99.680 Error@1 9.230

==>>[2019-08-30 22:45:42] [Epoch=040/100] [Need: 02:52:34] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [040][000/391]   Time 4.990 (4.990)   Data 4.547 (4.547)   Loss 0.0742 (0.0742)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 22:45:47]
  Epoch: [040][200/391]   Time 0.492 (0.411)   Data 0.000 (0.023)   Loss 0.0968 (0.0903)   Prec@1 96.875 (96.984)   Prec@5 100.000 (99.977)   [2019-08-30 22:47:05]
  **Test** Prec@1 90.480 Prec@5 99.700 Error@1 9.520

==>>[2019-08-30 22:48:36] [Epoch=041/100] [Need: 02:49:43] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [041][000/391]   Time 5.563 (5.563)   Data 5.126 (5.126)   Loss 0.0963 (0.0963)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 22:48:41]
  Epoch: [041][200/391]   Time 0.536 (0.406)   Data 0.000 (0.026)   Loss 0.1379 (0.0871)   Prec@1 95.312 (97.097)   Prec@5 99.219 (99.992)   [2019-08-30 22:49:57]
  **Test** Prec@1 90.530 Prec@5 99.600 Error@1 9.470

==>>[2019-08-30 22:51:28] [Epoch=042/100] [Need: 02:46:51] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [042][000/391]   Time 5.751 (5.751)   Data 5.321 (5.321)   Loss 0.0731 (0.0731)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 22:51:34]
  Epoch: [042][200/391]   Time 0.417 (0.412)   Data 0.000 (0.027)   Loss 0.0778 (0.0889)   Prec@1 96.094 (97.003)   Prec@5 100.000 (99.984)   [2019-08-30 22:52:51]
  **Test** Prec@1 90.330 Prec@5 99.730 Error@1 9.670

==>>[2019-08-30 22:54:27] [Epoch=043/100] [Need: 02:44:06] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [043][000/391]   Time 3.523 (3.523)   Data 3.226 (3.226)   Loss 0.0960 (0.0960)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 22:54:30]
  Epoch: [043][200/391]   Time 0.476 (0.412)   Data 0.000 (0.016)   Loss 0.0831 (0.0855)   Prec@1 94.531 (97.073)   Prec@5 100.000 (99.992)   [2019-08-30 22:55:50]
  **Test** Prec@1 90.920 Prec@5 99.730 Error@1 9.080

==>>[2019-08-30 22:57:20] [Epoch=044/100] [Need: 02:41:14] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [044][000/391]   Time 4.402 (4.402)   Data 3.943 (3.943)   Loss 0.0713 (0.0713)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 22:57:24]
  Epoch: [044][200/391]   Time 0.284 (0.412)   Data 0.000 (0.020)   Loss 0.0923 (0.0846)   Prec@1 96.875 (97.132)   Prec@5 100.000 (99.981)   [2019-08-30 22:58:43]
  **Test** Prec@1 90.720 Prec@5 99.730 Error@1 9.280

==>>[2019-08-30 23:00:13] [Epoch=045/100] [Need: 02:38:22] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [045][000/391]   Time 3.159 (3.159)   Data 2.760 (2.760)   Loss 0.0996 (0.0996)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 23:00:16]
  Epoch: [045][200/391]   Time 0.221 (0.392)   Data 0.000 (0.014)   Loss 0.1089 (0.0883)   Prec@1 96.875 (97.046)   Prec@5 100.000 (99.988)   [2019-08-30 23:01:32]
  **Test** Prec@1 90.600 Prec@5 99.700 Error@1 9.400

==>>[2019-08-30 23:03:02] [Epoch=046/100] [Need: 02:35:25] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [046][000/391]   Time 4.233 (4.233)   Data 3.955 (3.955)   Loss 0.0859 (0.0859)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2019-08-30 23:03:07]
  Epoch: [046][200/391]   Time 0.532 (0.402)   Data 0.000 (0.020)   Loss 0.0856 (0.0853)   Prec@1 96.875 (96.999)   Prec@5 100.000 (99.977)   [2019-08-30 23:04:23]
  **Test** Prec@1 90.620 Prec@5 99.690 Error@1 9.380

==>>[2019-08-30 23:05:55] [Epoch=047/100] [Need: 02:32:32] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [047][000/391]   Time 3.395 (3.395)   Data 2.851 (2.851)   Loss 0.0752 (0.0752)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 23:05:58]
  Epoch: [047][200/391]   Time 0.289 (0.391)   Data 0.000 (0.014)   Loss 0.1172 (0.0828)   Prec@1 95.312 (97.256)   Prec@5 100.000 (99.992)   [2019-08-30 23:07:13]
  **Test** Prec@1 90.320 Prec@5 99.670 Error@1 9.680

==>>[2019-08-30 23:08:48] [Epoch=048/100] [Need: 02:29:40] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [048][000/391]   Time 3.449 (3.449)   Data 3.000 (3.000)   Loss 0.0875 (0.0875)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 23:08:52]
  Epoch: [048][200/391]   Time 0.452 (0.405)   Data 0.000 (0.015)   Loss 0.0823 (0.0840)   Prec@1 97.656 (97.170)   Prec@5 100.000 (99.996)   [2019-08-30 23:10:10]
  **Test** Prec@1 90.530 Prec@5 99.730 Error@1 9.470

==>>[2019-08-30 23:11:41] [Epoch=049/100] [Need: 02:26:48] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [049][000/391]   Time 4.526 (4.526)   Data 3.935 (3.935)   Loss 0.0570 (0.0570)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 23:11:46]
  Epoch: [049][200/391]   Time 0.401 (0.403)   Data 0.000 (0.020)   Loss 0.0564 (0.0885)   Prec@1 98.438 (96.988)   Prec@5 100.000 (99.984)   [2019-08-30 23:13:02]
  **Test** Prec@1 90.430 Prec@5 99.730 Error@1 9.570

==>>[2019-08-30 23:14:33] [Epoch=050/100] [Need: 02:23:54] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [050][000/391]   Time 5.717 (5.717)   Data 4.876 (4.876)   Loss 0.0850 (0.0850)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-30 23:14:39]
  Epoch: [050][200/391]   Time 0.474 (0.417)   Data 0.000 (0.024)   Loss 0.0457 (0.0861)   Prec@1 98.438 (96.999)   Prec@5 100.000 (99.984)   [2019-08-30 23:15:57]
  **Test** Prec@1 90.020 Prec@5 99.600 Error@1 9.980

==>>[2019-08-30 23:17:30] [Epoch=051/100] [Need: 02:21:05] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [051][000/391]   Time 3.618 (3.618)   Data 3.231 (3.231)   Loss 0.0534 (0.0534)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 23:17:33]
  Epoch: [051][200/391]   Time 0.392 (0.411)   Data 0.000 (0.016)   Loss 0.1105 (0.0883)   Prec@1 95.312 (97.015)   Prec@5 100.000 (99.973)   [2019-08-30 23:18:52]
  **Test** Prec@1 91.040 Prec@5 99.680 Error@1 8.960

==>>[2019-08-30 23:20:25] [Epoch=052/100] [Need: 02:18:14] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [052][000/391]   Time 4.792 (4.792)   Data 4.377 (4.377)   Loss 0.0830 (0.0830)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:20:29]
  Epoch: [052][200/391]   Time 0.322 (0.406)   Data 0.000 (0.022)   Loss 0.0683 (0.0856)   Prec@1 98.438 (97.058)   Prec@5 100.000 (99.984)   [2019-08-30 23:21:46]
  **Test** Prec@1 90.250 Prec@5 99.670 Error@1 9.750

==>>[2019-08-30 23:23:18] [Epoch=053/100] [Need: 02:15:22] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [053][000/391]   Time 4.583 (4.583)   Data 4.016 (4.016)   Loss 0.0621 (0.0621)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:23:23]
  Epoch: [053][200/391]   Time 0.337 (0.393)   Data 0.000 (0.020)   Loss 0.1011 (0.0859)   Prec@1 96.875 (97.046)   Prec@5 100.000 (99.973)   [2019-08-30 23:24:37]
  **Test** Prec@1 90.650 Prec@5 99.700 Error@1 9.350

==>>[2019-08-30 23:26:12] [Epoch=054/100] [Need: 02:12:30] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [054][000/391]   Time 5.544 (5.544)   Data 5.088 (5.088)   Loss 0.0594 (0.0594)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 23:26:18]
  Epoch: [054][200/391]   Time 0.416 (0.404)   Data 0.000 (0.026)   Loss 0.0626 (0.0870)   Prec@1 97.656 (97.015)   Prec@5 100.000 (99.984)   [2019-08-30 23:27:33]
  **Test** Prec@1 90.580 Prec@5 99.680 Error@1 9.420

==>>[2019-08-30 23:29:05] [Epoch=055/100] [Need: 02:09:37] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [055][000/391]   Time 4.401 (4.401)   Data 3.946 (3.946)   Loss 0.0842 (0.0842)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:29:09]
  Epoch: [055][200/391]   Time 0.275 (0.404)   Data 0.000 (0.020)   Loss 0.0308 (0.0881)   Prec@1 99.219 (96.902)   Prec@5 100.000 (99.992)   [2019-08-30 23:30:26]
  **Test** Prec@1 90.570 Prec@5 99.700 Error@1 9.430

==>>[2019-08-30 23:31:59] [Epoch=056/100] [Need: 02:06:45] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [056][000/391]   Time 3.664 (3.664)   Data 3.197 (3.197)   Loss 0.0700 (0.0700)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 23:32:03]
  Epoch: [056][200/391]   Time 0.262 (0.405)   Data 0.000 (0.016)   Loss 0.0248 (0.0822)   Prec@1 99.219 (97.314)   Prec@5 100.000 (99.988)   [2019-08-30 23:33:20]
  **Test** Prec@1 90.760 Prec@5 99.720 Error@1 9.240

==>>[2019-08-30 23:34:53] [Epoch=057/100] [Need: 02:03:53] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [057][000/391]   Time 4.226 (4.226)   Data 3.626 (3.626)   Loss 0.1270 (0.1270)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-08-30 23:34:57]
  Epoch: [057][200/391]   Time 0.512 (0.416)   Data 0.000 (0.018)   Loss 0.1075 (0.0870)   Prec@1 98.438 (97.030)   Prec@5 100.000 (99.996)   [2019-08-30 23:36:17]
  **Test** Prec@1 90.900 Prec@5 99.720 Error@1 9.100

==>>[2019-08-30 23:37:48] [Epoch=058/100] [Need: 02:01:02] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [058][000/391]   Time 4.461 (4.461)   Data 3.915 (3.915)   Loss 0.1108 (0.1108)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:37:52]
  Epoch: [058][200/391]   Time 0.517 (0.414)   Data 0.000 (0.020)   Loss 0.0663 (0.0852)   Prec@1 96.875 (97.077)   Prec@5 100.000 (99.992)   [2019-08-30 23:39:11]
  **Test** Prec@1 90.320 Prec@5 99.740 Error@1 9.680

==>>[2019-08-30 23:40:42] [Epoch=059/100] [Need: 01:58:10] [learning_rate=0.0010] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [059][000/391]   Time 4.935 (4.935)   Data 4.473 (4.473)   Loss 0.0485 (0.0485)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 23:40:47]
  Epoch: [059][200/391]   Time 0.352 (0.381)   Data 0.000 (0.023)   Loss 0.1419 (0.0836)   Prec@1 96.094 (97.159)   Prec@5 100.000 (99.977)   [2019-08-30 23:41:58]
  **Test** Prec@1 90.590 Prec@5 99.730 Error@1 9.410

==>>[2019-08-30 23:43:31] [Epoch=060/100] [Need: 01:55:14] [learning_rate=0.0001] [Best : Accuracy=91.14, Error=8.86]
  Epoch: [060][000/391]   Time 5.082 (5.082)   Data 4.400 (4.400)   Loss 0.0487 (0.0487)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 23:43:36]
  Epoch: [060][200/391]   Time 0.407 (0.407)   Data 0.000 (0.022)   Loss 0.0671 (0.0786)   Prec@1 97.656 (97.435)   Prec@5 100.000 (99.988)   [2019-08-30 23:44:53]
  **Test** Prec@1 91.200 Prec@5 99.750 Error@1 8.800

==>>[2019-08-30 23:46:28] [Epoch=061/100] [Need: 01:52:24] [learning_rate=0.0001] [Best : Accuracy=91.20, Error=8.80]
  Epoch: [061][000/391]   Time 5.018 (5.018)   Data 4.618 (4.618)   Loss 0.0507 (0.0507)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-30 23:46:33]
  Epoch: [061][200/391]   Time 0.306 (0.399)   Data 0.000 (0.023)   Loss 0.0767 (0.0761)   Prec@1 95.312 (97.450)   Prec@5 100.000 (99.992)   [2019-08-30 23:47:48]
  **Test** Prec@1 91.350 Prec@5 99.780 Error@1 8.650

==>>[2019-08-30 23:49:18] [Epoch=062/100] [Need: 01:49:29] [learning_rate=0.0001] [Best : Accuracy=91.35, Error=8.65]
  Epoch: [062][000/391]   Time 3.972 (3.972)   Data 3.556 (3.556)   Loss 0.0657 (0.0657)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:49:22]
  Epoch: [062][200/391]   Time 0.321 (0.398)   Data 0.000 (0.018)   Loss 0.1039 (0.0706)   Prec@1 97.656 (97.594)   Prec@5 100.000 (99.981)   [2019-08-30 23:50:38]
  **Test** Prec@1 91.190 Prec@5 99.700 Error@1 8.810

==>>[2019-08-30 23:52:12] [Epoch=063/100] [Need: 01:46:37] [learning_rate=0.0001] [Best : Accuracy=91.35, Error=8.65]
  Epoch: [063][000/391]   Time 4.747 (4.747)   Data 4.128 (4.128)   Loss 0.0978 (0.0978)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-30 23:52:17]
  Epoch: [063][200/391]   Time 0.408 (0.408)   Data 0.000 (0.021)   Loss 0.0591 (0.0721)   Prec@1 98.438 (97.746)   Prec@5 100.000 (99.996)   [2019-08-30 23:53:34]
  **Test** Prec@1 90.980 Prec@5 99.720 Error@1 9.020

==>>[2019-08-30 23:55:07] [Epoch=064/100] [Need: 01:43:45] [learning_rate=0.0001] [Best : Accuracy=91.35, Error=8.65]
  Epoch: [064][000/391]   Time 3.514 (3.514)   Data 3.050 (3.050)   Loss 0.0230 (0.0230)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-30 23:55:10]
  Epoch: [064][200/391]   Time 0.427 (0.404)   Data 0.000 (0.015)   Loss 0.1368 (0.0674)   Prec@1 95.312 (97.839)   Prec@5 100.000 (99.988)   [2019-08-30 23:56:28]
  **Test** Prec@1 91.350 Prec@5 99.690 Error@1 8.650

==>>[2019-08-30 23:58:03] [Epoch=065/100] [Need: 01:40:54] [learning_rate=0.0001] [Best : Accuracy=91.35, Error=8.65]
  Epoch: [065][000/391]   Time 3.381 (3.381)   Data 3.041 (3.041)   Loss 0.0645 (0.0645)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-30 23:58:07]
  Epoch: [065][200/391]   Time 0.470 (0.389)   Data 0.000 (0.015)   Loss 0.0265 (0.0719)   Prec@1 99.219 (97.676)   Prec@5 100.000 (99.988)   [2019-08-30 23:59:21]
  **Test** Prec@1 91.210 Prec@5 99.720 Error@1 8.790

==>>[2019-08-31 00:00:58] [Epoch=066/100] [Need: 01:38:02] [learning_rate=0.0001] [Best : Accuracy=91.35, Error=8.65]
  Epoch: [066][000/391]   Time 4.543 (4.543)   Data 4.043 (4.043)   Loss 0.0562 (0.0562)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:01:03]
  Epoch: [066][200/391]   Time 0.433 (0.397)   Data 0.000 (0.020)   Loss 0.1757 (0.0715)   Prec@1 92.969 (97.660)   Prec@5 100.000 (99.988)   [2019-08-31 00:02:18]
  **Test** Prec@1 91.050 Prec@5 99.720 Error@1 8.950

==>>[2019-08-31 00:03:54] [Epoch=067/100] [Need: 01:35:10] [learning_rate=0.0001] [Best : Accuracy=91.35, Error=8.65]
  Epoch: [067][000/391]   Time 4.449 (4.449)   Data 3.979 (3.979)   Loss 0.0758 (0.0758)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 00:03:58]
  Epoch: [067][200/391]   Time 0.268 (0.389)   Data 0.000 (0.020)   Loss 0.0367 (0.0694)   Prec@1 98.438 (97.699)   Prec@5 100.000 (99.992)   [2019-08-31 00:05:12]
  **Test** Prec@1 90.970 Prec@5 99.660 Error@1 9.030

==>>[2019-08-31 00:06:46] [Epoch=068/100] [Need: 01:32:17] [learning_rate=0.0001] [Best : Accuracy=91.35, Error=8.65]
  Epoch: [068][000/391]   Time 5.428 (5.428)   Data 4.939 (4.939)   Loss 0.0270 (0.0270)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2019-08-31 00:06:51]
  Epoch: [068][200/391]   Time 0.408 (0.409)   Data 0.000 (0.025)   Loss 0.0702 (0.0694)   Prec@1 96.875 (97.738)   Prec@5 100.000 (99.984)   [2019-08-31 00:08:08]
  **Test** Prec@1 91.380 Prec@5 99.620 Error@1 8.620

==>>[2019-08-31 00:09:41] [Epoch=069/100] [Need: 01:29:25] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [069][000/391]   Time 5.178 (5.178)   Data 4.753 (4.753)   Loss 0.0571 (0.0571)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:09:46]
  Epoch: [069][200/391]   Time 0.343 (0.401)   Data 0.000 (0.024)   Loss 0.1015 (0.0730)   Prec@1 96.875 (97.571)   Prec@5 100.000 (99.988)   [2019-08-31 00:11:02]
  **Test** Prec@1 91.350 Prec@5 99.640 Error@1 8.650

==>>[2019-08-31 00:12:37] [Epoch=070/100] [Need: 01:26:33] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [070][000/391]   Time 4.581 (4.581)   Data 4.018 (4.018)   Loss 0.0669 (0.0669)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 00:12:42]
  Epoch: [070][200/391]   Time 0.327 (0.393)   Data 0.000 (0.020)   Loss 0.0565 (0.0677)   Prec@1 97.656 (97.796)   Prec@5 100.000 (100.000)   [2019-08-31 00:13:56]
  **Test** Prec@1 91.020 Prec@5 99.720 Error@1 8.980

==>>[2019-08-31 00:15:30] [Epoch=071/100] [Need: 01:23:39] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [071][000/391]   Time 5.855 (5.855)   Data 5.406 (5.406)   Loss 0.0636 (0.0636)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:15:35]
  Epoch: [071][200/391]   Time 0.344 (0.401)   Data 0.000 (0.029)   Loss 0.0953 (0.0720)   Prec@1 96.875 (97.512)   Prec@5 100.000 (99.988)   [2019-08-31 00:16:50]
  **Test** Prec@1 91.050 Prec@5 99.680 Error@1 8.950

==>>[2019-08-31 00:18:23] [Epoch=072/100] [Need: 01:20:46] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [072][000/391]   Time 4.725 (4.725)   Data 4.317 (4.317)   Loss 0.0558 (0.0558)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:18:27]
  Epoch: [072][200/391]   Time 0.342 (0.398)   Data 0.000 (0.022)   Loss 0.0949 (0.0674)   Prec@1 96.875 (97.858)   Prec@5 100.000 (99.996)   [2019-08-31 00:19:42]
  **Test** Prec@1 91.090 Prec@5 99.720 Error@1 8.910

==>>[2019-08-31 00:21:16] [Epoch=073/100] [Need: 01:17:54] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [073][000/391]   Time 4.423 (4.423)   Data 3.852 (3.852)   Loss 0.0384 (0.0384)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:21:21]
  Epoch: [073][200/391]   Time 0.396 (0.398)   Data 0.000 (0.019)   Loss 0.0646 (0.0669)   Prec@1 97.656 (97.847)   Prec@5 100.000 (99.988)   [2019-08-31 00:22:36]
  **Test** Prec@1 91.160 Prec@5 99.700 Error@1 8.840

==>>[2019-08-31 00:24:12] [Epoch=074/100] [Need: 01:15:01] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [074][000/391]   Time 3.911 (3.911)   Data 3.485 (3.485)   Loss 0.0388 (0.0388)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:24:16]
  Epoch: [074][200/391]   Time 0.399 (0.391)   Data 0.000 (0.018)   Loss 0.0791 (0.0694)   Prec@1 97.656 (97.757)   Prec@5 100.000 (99.996)   [2019-08-31 00:25:31]
  **Test** Prec@1 91.080 Prec@5 99.650 Error@1 8.920

==>>[2019-08-31 00:27:05] [Epoch=075/100] [Need: 01:12:08] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [075][000/391]   Time 4.551 (4.551)   Data 4.052 (4.052)   Loss 0.1088 (0.1088)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 00:27:10]
  Epoch: [075][200/391]   Time 0.349 (0.386)   Data 0.000 (0.020)   Loss 0.0535 (0.0673)   Prec@1 98.438 (97.897)   Prec@5 100.000 (99.996)   [2019-08-31 00:28:23]
  **Test** Prec@1 91.040 Prec@5 99.660 Error@1 8.960

==>>[2019-08-31 00:29:58] [Epoch=076/100] [Need: 01:09:15] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [076][000/391]   Time 4.791 (4.791)   Data 4.422 (4.422)   Loss 0.0456 (0.0456)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:30:03]
  Epoch: [076][200/391]   Time 0.284 (0.394)   Data 0.000 (0.023)   Loss 0.0833 (0.0673)   Prec@1 97.656 (97.866)   Prec@5 100.000 (99.992)   [2019-08-31 00:31:17]
  **Test** Prec@1 91.340 Prec@5 99.610 Error@1 8.660

==>>[2019-08-31 00:32:54] [Epoch=077/100] [Need: 01:06:23] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [077][000/391]   Time 3.399 (3.399)   Data 3.106 (3.106)   Loss 0.1247 (0.1247)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 00:32:58]
  Epoch: [077][200/391]   Time 0.435 (0.388)   Data 0.000 (0.016)   Loss 0.0325 (0.0701)   Prec@1 99.219 (97.715)   Prec@5 100.000 (99.992)   [2019-08-31 00:34:12]
  **Test** Prec@1 90.970 Prec@5 99.660 Error@1 9.030

==>>[2019-08-31 00:35:50] [Epoch=078/100] [Need: 01:03:30] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [078][000/391]   Time 4.126 (4.126)   Data 3.640 (3.640)   Loss 0.0659 (0.0659)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 00:35:54]
  Epoch: [078][200/391]   Time 0.274 (0.394)   Data 0.000 (0.018)   Loss 0.0824 (0.0713)   Prec@1 96.094 (97.613)   Prec@5 100.000 (100.000)   [2019-08-31 00:37:09]
  **Test** Prec@1 91.160 Prec@5 99.700 Error@1 8.840

==>>[2019-08-31 00:38:43] [Epoch=079/100] [Need: 01:00:37] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [079][000/391]   Time 3.256 (3.256)   Data 2.738 (2.738)   Loss 0.0607 (0.0607)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 00:38:47]
  Epoch: [079][200/391]   Time 0.478 (0.374)   Data 0.000 (0.014)   Loss 0.0480 (0.0711)   Prec@1 99.219 (97.718)   Prec@5 100.000 (99.996)   [2019-08-31 00:39:59]
  **Test** Prec@1 91.070 Prec@5 99.720 Error@1 8.930

==>>[2019-08-31 00:41:32] [Epoch=080/100] [Need: 00:57:43] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [080][000/391]   Time 3.887 (3.887)   Data 3.573 (3.573)   Loss 0.0558 (0.0558)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 00:41:36]
  Epoch: [080][200/391]   Time 0.304 (0.388)   Data 0.000 (0.018)   Loss 0.0872 (0.0727)   Prec@1 96.094 (97.648)   Prec@5 100.000 (100.000)   [2019-08-31 00:42:50]
  **Test** Prec@1 90.980 Prec@5 99.660 Error@1 9.020

==>>[2019-08-31 00:44:27] [Epoch=081/100] [Need: 00:54:50] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [081][000/391]   Time 3.639 (3.639)   Data 3.116 (3.116)   Loss 0.0446 (0.0446)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:44:31]
  Epoch: [081][200/391]   Time 0.234 (0.396)   Data 0.000 (0.016)   Loss 0.0501 (0.0683)   Prec@1 98.438 (97.730)   Prec@5 100.000 (99.996)   [2019-08-31 00:45:46]
  **Test** Prec@1 91.210 Prec@5 99.660 Error@1 8.790

==>>[2019-08-31 00:47:22] [Epoch=082/100] [Need: 00:51:57] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [082][000/391]   Time 5.036 (5.036)   Data 4.475 (4.475)   Loss 0.0509 (0.0509)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 00:47:27]
  Epoch: [082][200/391]   Time 0.627 (0.402)   Data 0.000 (0.023)   Loss 0.0698 (0.0697)   Prec@1 99.219 (97.687)   Prec@5 100.000 (99.992)   [2019-08-31 00:48:43]
  **Test** Prec@1 91.190 Prec@5 99.680 Error@1 8.810

==>>[2019-08-31 00:50:17] [Epoch=083/100] [Need: 00:49:04] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [083][000/391]   Time 3.234 (3.234)   Data 2.754 (2.754)   Loss 0.0607 (0.0607)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:50:20]
  Epoch: [083][200/391]   Time 0.351 (0.384)   Data 0.000 (0.014)   Loss 0.0978 (0.0681)   Prec@1 95.312 (97.843)   Prec@5 100.000 (99.992)   [2019-08-31 00:51:34]
  **Test** Prec@1 91.030 Prec@5 99.690 Error@1 8.970

==>>[2019-08-31 00:53:10] [Epoch=084/100] [Need: 00:46:11] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [084][000/391]   Time 3.664 (3.664)   Data 3.173 (3.173)   Loss 0.0776 (0.0776)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 00:53:13]
  Epoch: [084][200/391]   Time 0.295 (0.383)   Data 0.000 (0.016)   Loss 0.0436 (0.0660)   Prec@1 98.438 (97.854)   Prec@5 100.000 (99.992)   [2019-08-31 00:54:27]
  **Test** Prec@1 91.210 Prec@5 99.750 Error@1 8.790

==>>[2019-08-31 00:56:04] [Epoch=085/100] [Need: 00:43:18] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [085][000/391]   Time 4.424 (4.424)   Data 3.768 (3.768)   Loss 0.0843 (0.0843)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 00:56:08]
  Epoch: [085][200/391]   Time 0.417 (0.378)   Data 0.000 (0.019)   Loss 0.0254 (0.0689)   Prec@1 100.000 (97.648)   Prec@5 100.000 (100.000)   [2019-08-31 00:57:19]
  **Test** Prec@1 90.920 Prec@5 99.720 Error@1 9.080

==>>[2019-08-31 00:58:56] [Epoch=086/100] [Need: 00:40:25] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [086][000/391]   Time 5.418 (5.418)   Data 4.802 (4.802)   Loss 0.0573 (0.0573)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 00:59:01]
  Epoch: [086][200/391]   Time 0.512 (0.396)   Data 0.000 (0.024)   Loss 0.0935 (0.0697)   Prec@1 96.875 (97.742)   Prec@5 100.000 (99.992)   [2019-08-31 01:00:16]
  **Test** Prec@1 91.030 Prec@5 99.670 Error@1 8.970

==>>[2019-08-31 01:01:50] [Epoch=087/100] [Need: 00:37:32] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [087][000/391]   Time 3.969 (3.969)   Data 3.518 (3.518)   Loss 0.0894 (0.0894)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:01:54]
  Epoch: [087][200/391]   Time 0.398 (0.396)   Data 0.000 (0.018)   Loss 0.0261 (0.0689)   Prec@1 99.219 (97.808)   Prec@5 100.000 (99.996)   [2019-08-31 01:03:10]
  **Test** Prec@1 90.870 Prec@5 99.710 Error@1 9.130

==>>[2019-08-31 01:04:42] [Epoch=088/100] [Need: 00:34:38] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [088][000/391]   Time 4.485 (4.485)   Data 3.960 (3.960)   Loss 0.0776 (0.0776)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:04:47]
  Epoch: [088][200/391]   Time 0.338 (0.358)   Data 0.000 (0.020)   Loss 0.0860 (0.0709)   Prec@1 97.656 (97.617)   Prec@5 100.000 (99.988)   [2019-08-31 01:05:54]
  **Test** Prec@1 91.140 Prec@5 99.640 Error@1 8.860

==>>[2019-08-31 01:07:22] [Epoch=089/100] [Need: 00:31:43] [learning_rate=0.0001] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [089][000/391]   Time 5.407 (5.407)   Data 4.907 (4.907)   Loss 0.0545 (0.0545)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 01:07:28]
  Epoch: [089][200/391]   Time 0.207 (0.362)   Data 0.000 (0.025)   Loss 0.0574 (0.0680)   Prec@1 98.438 (97.874)   Prec@5 100.000 (99.996)   [2019-08-31 01:08:35]
  **Test** Prec@1 90.840 Prec@5 99.750 Error@1 9.160

==>>[2019-08-31 01:10:03] [Epoch=090/100] [Need: 00:28:49] [learning_rate=0.0000] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [090][000/391]   Time 4.522 (4.522)   Data 4.033 (4.033)   Loss 0.0305 (0.0305)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 01:10:08]
  Epoch: [090][200/391]   Time 0.343 (0.344)   Data 0.000 (0.020)   Loss 0.0524 (0.0644)   Prec@1 97.656 (97.963)   Prec@5 100.000 (99.996)   [2019-08-31 01:11:13]
  **Test** Prec@1 91.160 Prec@5 99.730 Error@1 8.840

==>>[2019-08-31 01:12:42] [Epoch=091/100] [Need: 00:25:55] [learning_rate=0.0000] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [091][000/391]   Time 4.380 (4.380)   Data 4.004 (4.004)   Loss 0.0510 (0.0510)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:12:46]
  Epoch: [091][200/391]   Time 0.392 (0.347)   Data 0.000 (0.020)   Loss 0.0573 (0.0630)   Prec@1 99.219 (98.076)   Prec@5 100.000 (99.988)   [2019-08-31 01:13:52]
  **Test** Prec@1 91.280 Prec@5 99.750 Error@1 8.720

==>>[2019-08-31 01:15:18] [Epoch=092/100] [Need: 00:23:00] [learning_rate=0.0000] [Best : Accuracy=91.38, Error=8.62]
  Epoch: [092][000/391]   Time 3.680 (3.680)   Data 3.060 (3.060)   Loss 0.0532 (0.0532)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2019-08-31 01:15:22]
  Epoch: [092][200/391]   Time 0.286 (0.334)   Data 0.000 (0.016)   Loss 0.0422 (0.0661)   Prec@1 98.438 (97.905)   Prec@5 100.000 (99.992)   [2019-08-31 01:16:25]
  **Test** Prec@1 91.530 Prec@5 99.710 Error@1 8.470

==>>[2019-08-31 01:17:44] [Epoch=093/100] [Need: 00:20:06] [learning_rate=0.0000] [Best : Accuracy=91.53, Error=8.47]
  Epoch: [093][000/391]   Time 4.893 (4.893)   Data 4.491 (4.491)   Loss 0.0666 (0.0666)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2019-08-31 01:17:49]
  Epoch: [093][200/391]   Time 0.331 (0.305)   Data 0.000 (0.023)   Loss 0.0691 (0.0624)   Prec@1 98.438 (98.053)   Prec@5 100.000 (99.992)   [2019-08-31 01:18:45]
  **Test** Prec@1 91.500 Prec@5 99.750 Error@1 8.500

==>>[2019-08-31 01:19:54] [Epoch=094/100] [Need: 00:17:11] [learning_rate=0.0000] [Best : Accuracy=91.53, Error=8.47]
  Epoch: [094][000/391]   Time 2.820 (2.820)   Data 2.478 (2.478)   Loss 0.0870 (0.0870)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 01:19:57]
  Epoch: [094][200/391]   Time 0.204 (0.259)   Data 0.000 (0.013)   Loss 0.0621 (0.0634)   Prec@1 97.656 (97.963)   Prec@5 100.000 (99.988)   [2019-08-31 01:20:46]
  **Test** Prec@1 91.380 Prec@5 99.710 Error@1 8.620

==>>[2019-08-31 01:21:33] [Epoch=095/100] [Need: 00:14:15] [learning_rate=0.0000] [Best : Accuracy=91.53, Error=8.47]
  Epoch: [095][000/391]   Time 2.094 (2.094)   Data 1.813 (1.813)   Loss 0.0510 (0.0510)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 01:21:35]
  Epoch: [095][200/391]   Time 0.188 (0.224)   Data 0.000 (0.009)   Loss 0.1216 (0.0622)   Prec@1 95.312 (97.932)   Prec@5 100.000 (99.996)   [2019-08-31 01:22:18]
  **Test** Prec@1 90.800 Prec@5 99.720 Error@1 9.200

==>>[2019-08-31 01:22:57] [Epoch=096/100] [Need: 00:11:20] [learning_rate=0.0000] [Best : Accuracy=91.53, Error=8.47]
  Epoch: [096][000/391]   Time 1.663 (1.663)   Data 1.346 (1.346)   Loss 0.0943 (0.0943)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 01:22:59]
  Epoch: [096][200/391]   Time 0.213 (0.170)   Data 0.000 (0.007)   Loss 0.0493 (0.0633)   Prec@1 99.219 (98.022)   Prec@5 100.000 (99.988)   [2019-08-31 01:23:32]
  **Test** Prec@1 91.080 Prec@5 99.710 Error@1 8.920

==>>[2019-08-31 01:24:13] [Epoch=097/100] [Need: 00:08:27] [learning_rate=0.0000] [Best : Accuracy=91.53, Error=8.47]
  Epoch: [097][000/391]   Time 1.587 (1.587)   Data 1.265 (1.265)   Loss 0.0423 (0.0423)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2019-08-31 01:24:14]
  Epoch: [097][200/391]   Time 0.144 (0.169)   Data 0.000 (0.007)   Loss 0.0625 (0.0651)   Prec@1 97.656 (97.862)   Prec@5 100.000 (99.996)   [2019-08-31 01:24:47]
  **Test** Prec@1 91.120 Prec@5 99.740 Error@1 8.880

==>>[2019-08-31 01:25:27] [Epoch=098/100] [Need: 00:05:36] [learning_rate=0.0000] [Best : Accuracy=91.53, Error=8.47]
  Epoch: [098][000/391]   Time 1.640 (1.640)   Data 1.332 (1.332)   Loss 0.0934 (0.0934)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2019-08-31 01:25:28]
  Epoch: [098][200/391]   Time 0.146 (0.171)   Data 0.000 (0.007)   Loss 0.0701 (0.0637)   Prec@1 96.875 (97.921)   Prec@5 100.000 (99.992)   [2019-08-31 01:26:01]
  **Test** Prec@1 91.230 Prec@5 99.700 Error@1 8.770

==>>[2019-08-31 01:26:39] [Epoch=099/100] [Need: 00:02:47] [learning_rate=0.0000] [Best : Accuracy=91.53, Error=8.47]
  Epoch: [099][000/391]   Time 1.662 (1.662)   Data 1.389 (1.389)   Loss 0.0953 (0.0953)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2019-08-31 01:26:41]
  Epoch: [099][200/391]   Time 0.139 (0.178)   Data 0.000 (0.007)   Loss 0.0992 (0.0640)   Prec@1 95.312 (97.785)   Prec@5 100.000 (99.996)   [2019-08-31 01:27:15]
  **Test** Prec@1 91.030 Prec@5 99.690 Error@1 8.970
